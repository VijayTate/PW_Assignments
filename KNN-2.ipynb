{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a897151-fead-4a04-b4b2-9947f6086d2c",
   "metadata": {},
   "source": [
    "#### Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc2d0b-d7c6-407f-b258-25472d6d3b97",
   "metadata": {},
   "source": [
    "The main difference between the Euclidean distance metric and the Manhattan distance metric in K-Nearest Neighbors (KNN) is the way they measure the distance between data points in feature space. This difference can have a significant impact on the performance of a KNN classifier or regressor, depending on the dataset and problem. Here's a comparison of the two distance metrics and their potential effects:\n",
    "\n",
    "**Euclidean Distance:**\n",
    "- Definition: Euclidean distance, also known as L2 distance, calculates the straight-line distance (as the crow flies) between two points in Euclidean space.\n",
    "- Formula: For two points (x1, y1) and (x2, y2) in two dimensions, the Euclidean distance is calculated as:\n",
    "  Euclidean Distance = √((x2 - x1)² + (y2 - y1)²)\n",
    "- Characteristics:\n",
    "  - Gives more weight to diagonal movement in the feature space.\n",
    "  - Assumes that distances along all axes are equally important.\n",
    "  - Emphasizes the global structure of the data.\n",
    "- Effect on KNN:\n",
    "  - Euclidean distance can perform well when the underlying data distribution has a spherical or isotropic shape.\n",
    "  - Suitable for problems where the relationships between features are approximately equal in all directions.\n",
    "\n",
    "**Manhattan Distance:**\n",
    "- Definition: Manhattan distance, also known as L1 distance or city block distance, calculates the distance by summing the absolute differences between corresponding coordinates of two points.\n",
    "- Formula: For two points (x1, y1) and (x2, y2) in two dimensions, the Manhattan distance is calculated as:\n",
    "  Manhattan Distance = |x2 - x1| + |y2 - y1|\n",
    "- Characteristics:\n",
    "  - Emphasizes movement along gridlines, as in navigating city blocks.\n",
    "  - Suitable for situations where diagonal movement in the feature space is less meaningful.\n",
    "  - Robust to outliers since it considers absolute differences.\n",
    "- Effect on KNN:\n",
    "  - Manhattan distance can perform well when data relationships are more aligned with the axes (e.g., tabular data with categorical features).\n",
    "  - Suitable for problems where certain features contribute significantly along specific dimensions.\n",
    "\n",
    "**Effect on KNN Performance:**\n",
    "- Choice of Distance Metric: The choice between Euclidean and Manhattan distance should be guided by the characteristics of the data. Using the wrong metric can lead to suboptimal results.\n",
    "- Impact on Neighbor Selection: The choice of distance metric affects which data points are identified as the K nearest neighbors. This can influence the decision-making process in KNN.\n",
    "- Data Distribution: If the data distribution is more aligned with one distance metric over the other, using the appropriate metric can lead to better model performance.\n",
    "- Outliers: Manhattan distance is often more robust to outliers since it considers absolute differences, making it suitable for datasets with outliers.\n",
    "- Scaling: Feature scaling can also influence the performance of distance metrics. It's important to scale features appropriately based on the chosen distance metric.\n",
    "\n",
    "In practice, it's common to experiment with both distance metrics and evaluate their impact on KNN's performance using techniques such as cross-validation. The choice of distance metric should align with the specific characteristics of the dataset and the problem's requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a0a1b-7303-4324-8499-10d63a226382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e08d7-b5fb-4fe2-9ee0-815966d03c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c62ec095-6853-499c-b33d-03e4238f34ef",
   "metadata": {},
   "source": [
    "#### Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508d026-889a-4dcf-8035-229d49532d3d",
   "metadata": {},
   "source": [
    "Choosing the optimal value of K (the number of neighbors) is a critical hyperparameter tuning step when using the K-Nearest Neighbors (KNN) algorithm for classification or regression. The choice of K can significantly impact the model's performance. Several techniques can be used to determine the optimal K value:\n",
    "\n",
    "**1. Grid Search:**\n",
    "   - Grid search is a systematic approach where you evaluate the model's performance for a range of K values, typically specified as a list or range. You train and validate the model for each K and select the K that results in the best performance based on a chosen evaluation metric (e.g., accuracy, F1 score, MSE).\n",
    "   - Grid search is effective but can be computationally expensive, especially for large datasets and high-dimensional feature spaces.\n",
    "\n",
    "**2. Cross-Validation:**\n",
    "   - Cross-validation (e.g., k-fold cross-validation) allows you to estimate the model's performance for different K values while efficiently using the available data. You divide the data into multiple subsets (folds), train the model on K-1 folds, and validate it on the remaining fold. This process is repeated for different K values.\n",
    "   - Cross-validation provides more reliable estimates of model performance and helps prevent overfitting. You can choose the K with the best cross-validated performance.\n",
    "\n",
    "**3. Elbow Method:**\n",
    "   - The elbow method involves plotting the model's performance (e.g., error rate or loss) as a function of K. As K increases, the error typically decreases. The idea is to look for the \"elbow point\" on the plot, which represents the point at which increasing K no longer significantly improves performance.\n",
    "   - The elbow method provides an intuitive way to choose K, but it may not always yield a clear elbow, especially in complex datasets.\n",
    "\n",
    "**4. Leave-One-Out Cross-Validation (LOOCV):**\n",
    "   - LOOCV is a variation of cross-validation where each data point is treated as a test point while the remaining data is used for training. This process is repeated for each data point.\n",
    "   - LOOCV provides an estimate of the model's performance for each K value, and the K with the best overall performance is selected. LOOCV can be computationally expensive but provides an unbiased estimate.\n",
    "\n",
    "**5. Domain Knowledge and Problem Context:**\n",
    "   - Consider the specific problem and dataset characteristics when choosing K. For example, if you know that the problem typically involves many similar instances, a smaller K may be appropriate. If the problem is noisy, a larger K might be more robust.\n",
    "\n",
    "**6. Plotting Accuracy vs. K:**\n",
    "   - Plotting the model's accuracy or performance metric against K can provide visual insights into how the choice of K affects the model's behavior. You can observe trends and make informed decisions based on the plot.\n",
    "\n",
    "**7. Automated Hyperparameter Tuning:**\n",
    "   - Automated hyperparameter optimization techniques, such as Bayesian optimization or random search, can be used to search for the optimal K value in a more efficient manner.\n",
    "\n",
    "It's essential to consider the trade-offs when choosing K. Smaller K values may lead to overfitting, while larger K values may lead to underfitting. The choice of K should balance bias and variance, with the aim of achieving good generalization performance on unseen data. Experimentation, cross-validation, and domain knowledge are often necessary to make an informed choice for K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42730647-dc4e-4041-b273-22ed807965a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1a5ed2-9ec1-4d44-bc51-fba6ce165506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a6f8ba-ff74-419d-9b88-c669a8981bc4",
   "metadata": {},
   "source": [
    "#### Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f36e66-e6aa-4a48-8ab6-b55c7f5373e9",
   "metadata": {},
   "source": [
    "The choice of distance metric in a K-Nearest Neighbors (KNN) classifier or regressor significantly affects the performance of the model because it determines how similarity between data points is calculated. Different distance metrics can lead to different results based on the characteristics of the data and the problem at hand. Here's how the choice of distance metric impacts KNN performance and when you might choose one metric over the other:\n",
    "\n",
    "**Euclidean Distance:**\n",
    "- **Characteristics:** Euclidean distance calculates the straight-line distance between two points in Euclidean space. It considers both horizontal and vertical differences.\n",
    "- **Use Cases:**\n",
    "   - Choose Euclidean distance when you assume that the relationships between features are approximately equal in all directions.\n",
    "   - Suitable for problems where the underlying data distribution has a spherical or isotropic shape.\n",
    "   - Works well when the data is evenly distributed, and distances along all axes are equally important.\n",
    "- **Considerations:** Euclidean distance may not perform well in high-dimensional spaces (curse of dimensionality) or when features have different scales.\n",
    "\n",
    "**Manhattan Distance:**\n",
    "- **Characteristics:** Manhattan distance, also known as L1 distance or city block distance, calculates the distance by summing the absolute differences between corresponding coordinates of two points. It emphasizes movement along gridlines.\n",
    "- **Use Cases:**\n",
    "   - Choose Manhattan distance when you want to emphasize movement along gridlines, as in cases where diagonal movement in the feature space is less meaningful.\n",
    "   - Suitable for data with categorical features, tabular data, or problems where certain features contribute significantly along specific dimensions.\n",
    "   - Robust to outliers since it considers absolute differences.\n",
    "- **Considerations:** Manhattan distance may perform poorly when the underlying data distribution does not align well with gridlines.\n",
    "\n",
    "**Choice of Distance Metric:**\n",
    "- **Data Characteristics:** The choice between Euclidean and Manhattan distance should align with the characteristics of the data. Consider the distribution of data points and the relationships between features. Experiment with both distance metrics and evaluate their impact on model performance.\n",
    "- **Feature Scaling:** Feature scaling is crucial when using distance-based metrics. Ensure that features are scaled appropriately, especially if they have different units or scales. Scaling can mitigate the impact of features with large magnitude values.\n",
    "- **Outliers:** The choice of distance metric can influence the model's sensitivity to outliers. Manhattan distance is often more robust to outliers due to its use of absolute differences.\n",
    "- **Dimensionality:** High-dimensional spaces can be challenging for Euclidean distance due to the curse of dimensionality. In such cases, Manhattan distance or other specialized distance metrics may be more appropriate.\n",
    "\n",
    "Ultimately, the choice of distance metric should be guided by a deep understanding of the data and the problem you are trying to solve. It's often advisable to experiment with both metrics and evaluate their performance using techniques such as cross-validation to determine which one works best for your specific use case. Additionally, domain knowledge and insights about the data can help inform the choice of distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f77c2-bdd1-4c60-9611-067c36c85e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c765ea17-4f9b-45e0-bf52-1c056c32f3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48678545-b84e-497d-abbd-0a45ca392b21",
   "metadata": {},
   "source": [
    "#### Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949edab8-b7c4-43ff-8629-e1a070ff1d3b",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) classifiers and regressors have several hyperparameters that can significantly impact the model's performance. Understanding these hyperparameters and how they affect the model is essential for fine-tuning KNN models. Here are some common hyperparameters in KNN and their effects:\n",
    "\n",
    "**1. Number of Neighbors (K):**\n",
    "   - **Hyperparameter:** The number of nearest neighbors to consider when making predictions.\n",
    "   - **Effect:** \n",
    "     - Smaller values of K (e.g., K=1 or K=3) can lead to more complex and potentially noisy decision boundaries. They may be prone to overfitting.\n",
    "     - Larger values of K (e.g., K=10 or K=20) can result in smoother decision boundaries, but they may underfit the data.\n",
    "   - **Tuning:** Use techniques such as grid search, cross-validation, or the elbow method to find an optimal value of K that balances bias and variance for your specific dataset.\n",
    "\n",
    "**2. Distance Metric:**\n",
    "   - **Hyperparameter:** The choice of distance metric, such as Euclidean or Manhattan.\n",
    "   - **Effect:** The distance metric defines how similarity between data points is calculated and can impact the shape of decision boundaries.\n",
    "   - **Tuning:** Experiment with both Euclidean and Manhattan distances based on the characteristics of the data and the problem. Cross-validation can help identify the better-performing metric.\n",
    "\n",
    "**3. Weighted or Uniform Voting:**\n",
    "   - **Hyperparameter:** In KNN classifiers, you can choose between weighted voting (where closer neighbors have more influence) or uniform voting (all neighbors have equal influence).\n",
    "   - **Effect:** Weighted voting gives more importance to closer neighbors, potentially improving model performance when some neighbors are more relevant than others.\n",
    "   - **Tuning:** Test both weighted and uniform voting to see which one works better for your specific problem. Grid search can be used to tune this hyperparameter.\n",
    "\n",
    "**4. Distance Weights (Optional):**\n",
    "   - **Hyperparameter:** Weights assigned to each feature dimension when calculating distances (e.g., feature-specific weights).\n",
    "   - **Effect:** Allows you to assign different importance to individual features. Useful when some features are more informative than others.\n",
    "   - **Tuning:** Experiment with different feature-specific weights based on domain knowledge or feature importance analysis.\n",
    "\n",
    "**5. Algorithm for Nearest Neighbors Search:**\n",
    "   - **Hyperparameter:** The algorithm used to find the nearest neighbors (e.g., brute-force, KD-tree, Ball tree).\n",
    "   - **Effect:** Different algorithms have varying computational complexities and may perform differently on datasets of different sizes and dimensions.\n",
    "   - **Tuning:** Depending on the dataset size and dimensionality, choose an appropriate nearest neighbor search algorithm. This can help optimize model training time.\n",
    "\n",
    "**6. Leaf Size (for Tree-Based Algorithms):**\n",
    "   - **Hyperparameter:** The maximum number of data points in a leaf node for tree-based algorithms like KD-tree and Ball tree.\n",
    "   - **Effect:** Smaller leaf sizes can lead to more fine-grained trees, which may be computationally expensive but can improve accuracy. Larger leaf sizes result in coarser trees.\n",
    "   - **Tuning:** Experiment with different leaf sizes based on the dataset size and dimensionality. Smaller leaf sizes may be preferred for high-dimensional data.\n",
    "\n",
    "**7. Preprocessing (Scaling and Feature Selection):**\n",
    "   - **Effect:** Proper feature scaling (e.g., min-max scaling or standardization) and feature selection (choosing relevant features) can significantly impact KNN performance.\n",
    "   - **Tuning:** Apply appropriate preprocessing techniques to ensure that the data is in a suitable format for KNN.\n",
    "\n",
    "To tune these hyperparameters effectively, you can use techniques such as grid search, random search, or Bayesian optimization. Cross-validation is crucial for evaluating the model's performance with different hyperparameter configurations. Additionally, consider the characteristics of your specific dataset and the problem you are solving when selecting hyperparameters, as there is no one-size-fits-all solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea91e3f-5a2e-46ca-8f3b-18fff07ebe67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a342d-1e8d-48dc-a717-d87484a3720d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b65a847-16fd-47fb-932d-44ed807be51c",
   "metadata": {},
   "source": [
    "#### Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c9fd3-fdfa-4698-8d9b-b89fe8f20b30",
   "metadata": {},
   "source": [
    "The size of the training set can significantly affect the performance of a K-Nearest Neighbors (KNN) classifier or regressor. The impact of training set size varies depending on the dataset, problem complexity, and other factors. Here's how the training set size influences KNN performance and techniques to optimize it:\n",
    "\n",
    "**Effect of Training Set Size:**\n",
    "\n",
    "1. **Small Training Set:**\n",
    "   - **Advantages:**\n",
    "     - Faster model training as there are fewer data points to consider.\n",
    "     - Potentially less prone to overfitting when dealing with noisy or complex data.\n",
    "   - **Disadvantages:**\n",
    "     - Limited representation of the underlying data distribution, which may lead to biased or less accurate models.\n",
    "     - Lower model generalization as the model might not capture the full complexity of the problem.\n",
    "\n",
    "2. **Large Training Set:**\n",
    "   - **Advantages:**\n",
    "     - Improved model generalization as the model has more data to learn from, making it less likely to overfit.\n",
    "     - Enhanced accuracy and robustness, especially when the problem is complex or high-dimensional.\n",
    "   - **Disadvantages:**\n",
    "     - Longer training times due to the increased number of data points.\n",
    "     - Potential computational challenges when working with very large datasets.\n",
    "\n",
    "**Techniques to Optimize Training Set Size:**\n",
    "\n",
    "1. **Cross-Validation:** Use cross-validation techniques such as k-fold cross-validation to assess the model's performance across different training set sizes. This helps identify the trade-offs between model performance and training set size.\n",
    "\n",
    "2. **Data Augmentation:** In cases of limited training data, consider data augmentation techniques to artificially increase the effective size of the training set. This can involve generating additional samples through techniques like rotation, flipping, or adding noise, particularly in image and text data.\n",
    "\n",
    "3. **Bootstrapping:** For small datasets, bootstrapping can be used to create multiple random samples (with replacement) from the existing data. This process can generate larger training sets and provide a sense of the model's stability.\n",
    "\n",
    "4. **Active Learning:** Active learning strategies allow the model to select the most informative data points for training. Initially, you can start with a small training set and iteratively select additional data points to improve model performance.\n",
    "\n",
    "5. **Resampling Techniques:** When dealing with imbalanced datasets, oversampling and undersampling techniques can be applied to balance the class distribution, effectively adjusting the training set size for each class.\n",
    "\n",
    "6. **Feature Engineering:** Careful feature engineering can reduce the dimensionality of the data, potentially allowing you to work with a smaller, more informative feature set while maintaining model performance.\n",
    "\n",
    "7. **Feature Selection:** Feature selection techniques can help identify and retain the most relevant features, reducing the dimensionality and computational burden of the model.\n",
    "\n",
    "8. **Parallelization and Distributed Computing:** To handle large datasets, consider parallel computing frameworks or distributed processing systems that can distribute the computational load across multiple machines or cores.\n",
    "\n",
    "9. **Model Selection:** Depending on the available training data, you might consider alternative machine learning algorithms or ensemble methods that are less sensitive to training set size limitations.\n",
    "\n",
    "In practice, the choice of training set size should be guided by the problem's complexity, the availability of data, and computational resources. It often involves a trade-off between model accuracy and training time. Experimentation with different training set sizes and proper evaluation using cross-validation can help determine the optimal training set size for a given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b417ac0-02ac-411b-8f39-d0f1541dc252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2ed0d-af43-4875-b4ec-c9ffab1e0f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb182db7-ed90-4347-9f9e-13fad46fbab1",
   "metadata": {},
   "source": [
    "#### Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e2a434-9551-4ecf-9c0d-9e427025ec2e",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a simple and intuitive algorithm, but it comes with some potential drawbacks that can impact its performance. Understanding these drawbacks and employing strategies to overcome them can lead to more effective KNN models. Here are some common drawbacks of using KNN as a classifier or regressor and ways to mitigate them:\n",
    "\n",
    "**1. Sensitivity to Outliers:**\n",
    "   - **Drawback:** KNN can be sensitive to outliers because it relies on distance metrics. Outliers can distort the decision boundaries or influence predictions.\n",
    "   - **Mitigation:** \n",
    "     - Use robust distance metrics like Manhattan distance, which are less affected by outliers.\n",
    "     - Consider outlier detection techniques to identify and handle outliers separately.\n",
    "     - Reduce the impact of outliers by using weighted voting.\n",
    "\n",
    "**2. Computational Complexity:**\n",
    "   - **Drawback:** KNN can be computationally expensive, especially for large datasets with many features, as it requires calculating distances between all data points during prediction.\n",
    "   - **Mitigation:**\n",
    "     - Implement data structures like KD-trees or Ball trees to speed up nearest neighbor search, particularly for high-dimensional data.\n",
    "     - Use approximate nearest neighbor search algorithms for large datasets.\n",
    "     - Preprocess data to reduce dimensionality and noise.\n",
    "\n",
    "**3. Curse of Dimensionality:**\n",
    "   - **Drawback:** In high-dimensional feature spaces, KNN performance can deteriorate due to the curse of dimensionality. Data points become more sparse, and distances between points lose their meaning.\n",
    "   - **Mitigation:**\n",
    "     - Apply dimensionality reduction techniques like Principal Component Analysis (PCA) or feature selection to reduce the number of irrelevant or redundant features.\n",
    "     - Consider using dimensionality reduction combined with KNN to handle high-dimensional data more effectively.\n",
    "\n",
    "**4. Imbalanced Data:**\n",
    "   - **Drawback:** KNN may struggle with imbalanced datasets, where one class significantly outnumbers others, as it can result in biased predictions.\n",
    "   - **Mitigation:**\n",
    "     - Use appropriate resampling techniques like oversampling the minority class or undersampling the majority class to balance the dataset.\n",
    "     - Adjust the class weights during model training to give more importance to the minority class.\n",
    "\n",
    "**5. Optimal K Selection:**\n",
    "   - **Drawback:** Choosing the optimal value of K can be challenging. A poor choice of K can lead to overfitting or underfitting.\n",
    "   - **Mitigation:**\n",
    "     - Use cross-validation to evaluate model performance for different K values.\n",
    "     - Apply techniques like grid search or the elbow method to find the optimal K.\n",
    "     - Consider using an ensemble of KNN models with different K values.\n",
    "\n",
    "**6. Memory Usage:**\n",
    "   - **Drawback:** KNN requires storing the entire training dataset in memory, which can be impractical for very large datasets.\n",
    "   - **Mitigation:**\n",
    "     - Use data sampling or data reduction techniques to work with a smaller representative subset of the data.\n",
    "     - Implement approximate nearest neighbor algorithms to reduce memory requirements.\n",
    "\n",
    "**7. Interpretability:**\n",
    "   - **Drawback:** KNN models are less interpretable compared to some other algorithms that provide feature importance scores or decision rules.\n",
    "   - **Mitigation:**\n",
    "     - Use feature importance analysis to gain insights into the importance of individual features.\n",
    "     - Consider combining KNN with techniques that provide model interpretability, such as local interpretable model-agnostic explanations (LIME).\n",
    "\n",
    "It's important to note that the suitability of KNN depends on the specific characteristics of the data and the problem at hand. Careful preprocessing, parameter tuning, and the use of appropriate distance metrics can help address many of the drawbacks associated with KNN and improve its performance. Additionally, considering hybrid or ensemble approaches that combine KNN with other algorithms can lead to more robust models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
