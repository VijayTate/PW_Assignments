{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ecb538-a67b-45a2-9911-7fa6a667d721",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18e861-e62f-4cc1-a3b2-7137a47064de",
   "metadata": {},
   "source": [
    "**Overfitting** and **underfitting** are common challenges in machine learning that relate to how well a model generalizes to new, unseen data:\n",
    "\n",
    "1. **Overfitting**:\n",
    "Overfitting occurs when a model learns the training data too closely, capturing noise and random fluctuations rather than the underlying patterns. As a result, the model performs well on the training data but poorly on new, unseen data.\n",
    "\n",
    "Consequences:\n",
    "- High training accuracy but poor test accuracy.\n",
    "- Lack of generalization, leading to incorrect predictions on new data.\n",
    "- Highly complex models with many parameters are prone to overfitting.\n",
    "\n",
    "Mitigation:\n",
    "- Use simpler models with fewer parameters to reduce complexity.\n",
    "- Regularization techniques (e.g., L1 or L2 regularization) add penalty terms to the loss function to discourage overly complex models.\n",
    "- Increase the amount of training data to help the model generalize better.\n",
    "- Apply feature selection or dimensionality reduction to reduce noise in the data.\n",
    "- Cross-validation can help identify and prevent overfitting by evaluating the model's performance on multiple subsets of the data.\n",
    "\n",
    "2. **Underfitting**:\n",
    "Underfitting occurs when a model is too simple to capture the underlying patterns in the data. It fails to learn the relationships and performs poorly on both the training and test data.\n",
    "\n",
    "Consequences:\n",
    "- Low training accuracy and low test accuracy.\n",
    "- Inability to capture relevant patterns, leading to inaccurate predictions.\n",
    "\n",
    "Mitigation:\n",
    "- Use more complex models that can better capture underlying relationships.\n",
    "- Increase the model's capacity by adding more features or increasing the depth of the network.\n",
    "- Ensure that the data is properly preprocessed and features are appropriately engineered.\n",
    "- Experiment with different algorithms to find the best fit for the data.\n",
    "- Consider ensemble methods that combine multiple models to improve predictive power.\n",
    "\n",
    "Balancing between overfitting and underfitting is a critical task in machine learning. Regular monitoring of the model's performance on both training and test data, as well as using techniques like cross-validation, can help in identifying and addressing these issues. The goal is to find a model that achieves a good balance between fitting the training data and generalizing well to new data, ensuring reliable and accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e65130-d848-47e3-9e98-1bd239b7b23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a32c58-671a-498e-8775-8a115f775c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1652ae03-0ee7-4470-b64f-e8b55708593e",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcda5382-bd72-4dda-a3f4-67c7617c9e9a",
   "metadata": {},
   "source": [
    "Reducing overfitting in machine learning involves various techniques and strategies aimed at preventing the model from fitting the noise or random fluctuations in the training data too closely. Here are some common methods to reduce overfitting:\n",
    "\n",
    "1. Cross-Validation:\n",
    "   - Utilize k-fold cross-validation to assess the model's performance on multiple subsets of the data. This helps evaluate how well the model generalizes to different data partitions and can identify overfitting tendencies.\n",
    "\n",
    "2. Regularization:\n",
    "   - Apply L1 (Lasso) or L2 (Ridge) regularization to penalize large parameter values in the model. This encourages the model to focus on the most important features and reduces the risk of fitting noise.\n",
    "\n",
    "3. Reduce Model Complexity:\n",
    "   - Choose simpler models with fewer parameters when possible, as they are less likely to overfit. For example, use linear models instead of complex non-linear models.\n",
    "\n",
    "4. Feature Selection:\n",
    "   - Carefully select relevant features and remove irrelevant or redundant ones. Fewer features can lead to a simpler model and reduce the risk of overfitting.\n",
    "\n",
    "5. Early Stopping:\n",
    "   - Monitor the model's performance on a validation set during training and stop training when performance on the validation set starts to degrade. This prevents the model from continuing to fit noise.\n",
    "\n",
    "6. Data Augmentation:\n",
    "   - Increase the size of the training dataset by applying transformations or perturbations to the existing data. This can help expose the model to more variations in the data.\n",
    "\n",
    "7. Ensemble Methods:\n",
    "   - Combine predictions from multiple models (e.g., bagging, boosting, or stacking) to reduce overfitting. Ensemble methods average out individual model errors, leading to better generalization.\n",
    "\n",
    "8. Dropout (for Neural Networks):\n",
    "   - In neural networks, apply dropout by randomly deactivating a fraction of neurons during each training iteration. This prevents any single neuron from becoming overly specialized.\n",
    "\n",
    "9. Hyperparameter Tuning:\n",
    "   - Experiment with different hyperparameters (learning rate, regularization strength, etc.) to find the settings that produce the best trade-off between bias and variance.\n",
    "\n",
    "10. Cross-Validation with Multiple Models:\n",
    "    - Consider using a variety of algorithms and architectures during cross-validation to identify the model with the best generalization performance.\n",
    "\n",
    "By implementing these techniques and selecting the appropriate ones based on the characteristics of your data and model, you can effectively reduce overfitting and create machine learning models that generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2e2da-2d15-4efe-90ea-d0f25335375f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a44fe9c-e299-4abe-86c9-17c75550ae87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03a64324-108a-4ec9-8464-a3aa3cfa730f",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09faf28-f6aa-42c0-b433-323156baad7d",
   "metadata": {},
   "source": [
    "Underfitting occurs in machine learning when a model is too simplistic to capture the underlying patterns present in the data. It generally arises when the model's complexity is insufficient to represent the relationships between the input features and the target variable. As a result, the model performs poorly not only on the training data but also on new, unseen data.\n",
    "\n",
    "Key Characteristics of Underfitting:\n",
    "\n",
    "- Low training accuracy: The model struggles to fit the training data, resulting in low accuracy during training.\n",
    "- Low test accuracy: The poor performance on the training data extends to the test data, indicating that the model fails to generalize to new data.\n",
    "- Oversimplification: The model may make overly simplistic assumptions and miss important patterns and relationships in the data.\n",
    "\n",
    "Scenarios where underfitting can occur in machine learning include:\n",
    "\n",
    "1. **Insufficient Model Complexity:** If the chosen model is too basic or has too few parameters to adequately represent the data's complexities, it may underfit. For example, attempting to fit a complex non-linear relationship with a linear regression model can lead to underfitting.\n",
    "\n",
    "2. **Limited Features:** When important features are missing from the dataset or not properly utilized, the model may not have enough information to capture the data's underlying patterns.\n",
    "\n",
    "3. **Small Training Dataset:** With a small training dataset, the model may not have enough examples to learn meaningful patterns. This can result in poor generalization to new data.\n",
    "\n",
    "4. **Over-regularization:** Excessive use of regularization techniques (like L1 or L2 regularization) can overly constrain the model, leading to underfitting by discouraging the model from fitting the training data closely.\n",
    "\n",
    "5. **Under-training:** If the model is trained for too few epochs or with insufficient iterations, it may not have converged to the optimal solution, leading to underfitting.\n",
    "\n",
    "6. **Ignoring Outliers:** If outliers in the data are not appropriately handled, the model may fit the majority of data points but perform poorly on outliers.\n",
    "\n",
    "7. **High Bias:** Bias refers to the difference between the predicted values and the true values. High bias implies that the model's predictions consistently deviate from the actual values, indicating underfitting.\n",
    "\n",
    "8. **Ignoring Domain Knowledge:** Failing to incorporate domain-specific insights or prior knowledge about the problem can result in models that are too simplistic to capture relevant patterns.\n",
    "\n",
    "9. **Incorrect Model Choice:** Choosing a model architecture that is inherently incapable of capturing the complexity of the data can lead to underfitting. For instance, using a linear model for data with non-linear relationships.\n",
    "\n",
    "10. **Balancing Trade-offs:** In some cases, model complexity needs to be carefully balanced. While increasing complexity may help mitigate underfitting, it could also increase the risk of overfitting.\n",
    "\n",
    "Addressing underfitting generally involves increasing the model's complexity, either by selecting a more suitable model, adding more relevant features, reducing regularization, or increasing the number of training iterations. It's important to strike a balance between model complexity and the available data to ensure that the model captures the necessary relationships without memorizing noise or overcomplicating the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d44cd47-2fc0-4d95-8558-cc1c8f16fb59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0feac-922f-409a-8840-ba8760f8551b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f84cb7f-67b3-4808-9efc-7bfef794c7d6",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3e89bf-bd81-4338-837f-c329edbbf4e1",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning that deals with the relationship between model complexity, model error, and generalization. It highlights the tradeoff between two sources of error, namely bias and variance, that influence a model's performance on both training data and unseen test data.\n",
    "\n",
    "1. Bias:\n",
    "Bias represents the error introduced by approximating a real-world problem with a simplified model. It occurs when a model is too simplistic or makes assumptions that do not fully capture the underlying patterns in the data. A high bias model tends to underfit the training data, leading to poor performance on both training and test sets.\n",
    "\n",
    "2. Variance:\n",
    "Variance represents the sensitivity of a model's predictions to changes in the training data. It occurs when a model is too complex and captures noise and random fluctuations in the training data, resulting in high variability in the predictions. A high variance model tends to overfit the training data, performing well on the training set but poorly on unseen test data.\n",
    "\n",
    "The Relationship and Impact on Model Performance:\n",
    "- As model complexity increases, bias decreases, but variance increases.\n",
    "- As model complexity decreases, bias increases, but variance decreases.\n",
    "\n",
    "1. High Bias (Underfitting):\n",
    "- Low model complexity.\n",
    "- Fails to capture the underlying patterns in the data.\n",
    "- Results in poor performance on both training and test data.\n",
    "- The model is too simplistic and cannot handle the complexity of the problem.\n",
    "\n",
    "2. High Variance (Overfitting):\n",
    "- High model complexity.\n",
    "- Captures noise and random fluctuations in the training data.\n",
    "- Performs well on the training data but poorly on unseen test data.\n",
    "- The model memorizes the training data and fails to generalize to new data.\n",
    "\n",
    "The Bias-Variance Tradeoff:\n",
    "The goal in machine learning is to strike a balance between bias and variance to achieve the best generalization performance. An ideal model would have low bias to capture the underlying patterns accurately and low variance to avoid overfitting and maintain consistent predictions on new data.\n",
    "\n",
    "Finding this balance is not always straightforward and can be challenging, especially with limited data. Some strategies to manage the bias-variance tradeoff include:\n",
    "\n",
    "1. Cross-Validation: Use cross-validation to evaluate model performance on different subsets of the data and find the best tradeoff between bias and variance.\n",
    "\n",
    "2. Regularization: Apply regularization techniques to penalize complex models, reducing variance and mitigating overfitting.\n",
    "\n",
    "3. Feature Engineering: Improve the quality of input features to help the model capture the underlying patterns more effectively.\n",
    "\n",
    "4. Ensemble Methods: Combine multiple models (e.g., bagging, boosting) to reduce variance and improve overall performance.\n",
    "\n",
    "In conclusion, the bias-variance tradeoff highlights the interplay between model complexity, error sources, and generalization performance. Balancing bias and variance is essential to build accurate and robust machine learning models that can generalize well to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a078dd50-12fe-4d18-8247-8ef97a762f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d835cce-4b5c-4adf-9f37-7c66bd31dd64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbef957c-17a9-4c99-a682-2d26aa2f9034",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80270e25-057d-4d3a-8f59-b54e52ac1a98",
   "metadata": {},
   "source": [
    "Detecting overfitting and underfitting is crucial to building effective machine learning models. Here are some common methods and techniques to determine whether your model is suffering from overfitting or underfitting:\n",
    "\n",
    "1. **Learning Curves:**\n",
    "   - Learning curves show the model's performance (such as accuracy or error) on both the training and validation sets as a function of the training data size.\n",
    "   - In cases of overfitting, the training performance is significantly better than the validation performance, and the gap between the two curves widens as the model sees more data.\n",
    "   - In cases of underfitting, both the training and validation performance are poor, and the curves converge at a low performance level.\n",
    "\n",
    "2. **Validation Curve:**\n",
    "   - A validation curve shows the model's performance (e.g., accuracy) as a function of a hyperparameter (e.g., regularization strength).\n",
    "   - An overfit model may have high training performance but lower validation performance, indicating that increasing the hyperparameter's value is contributing to overfitting.\n",
    "   - An underfit model may exhibit low performance on both training and validation sets, indicating that increasing the hyperparameter's value could help.\n",
    "\n",
    "3. **Cross-Validation:**\n",
    "   - Cross-validation involves splitting the dataset into multiple subsets (folds) and training/validating the model on different combinations of these subsets.\n",
    "   - Overfitting is indicated if the model performs exceptionally well on the training folds but poorly on the validation folds.\n",
    "   - Underfitting is suggested if the model performs poorly on both training and validation folds.\n",
    "\n",
    "4. **Bias-Variance Analysis:**\n",
    "   - Understanding the bias-variance tradeoff can help diagnose overfitting and underfitting.\n",
    "   - High bias suggests underfitting, where the model is too simplistic to capture patterns.\n",
    "   - High variance suggests overfitting, where the model is too complex and captures noise.\n",
    "\n",
    "5. **Feature Importance:**\n",
    "   - Analyzing feature importance can provide insights into overfitting or underfitting.\n",
    "   - If the model is overfitting, it may assign too much importance to noise features.\n",
    "   - If the model is underfitting, it might struggle to capture important features.\n",
    "\n",
    "6. **Visual Inspection:**\n",
    "   - Visualizing the model's predictions and actual outcomes can help identify overfitting or underfitting tendencies.\n",
    "   - Overfitting may show predictions closely following the training data points but performing poorly on new data.\n",
    "   - Underfitting may show consistently inaccurate predictions.\n",
    "\n",
    "7. **Regularization Effect:**\n",
    "   - Gradually increasing the strength of regularization (e.g., L1 or L2) can help detect overfitting.\n",
    "   - As the regularization strength increases, the model's complexity is reduced, and if the validation performance improves, overfitting might have been present.\n",
    "\n",
    "8. **Model Complexity:**\n",
    "   - Experimenting with models of varying complexities can help diagnose overfitting or underfitting.\n",
    "   - A model that is too simple might underfit, while a model that is too complex might overfit.\n",
    "\n",
    "9. **Hyperparameter Tuning:**\n",
    "   - Adjusting hyperparameters can help find the right balance between overfitting and underfitting.\n",
    "   - Regularization strength, learning rate, number of layers, etc., can affect model behavior.\n",
    "\n",
    "By applying these methods and techniques, you can gain insights into whether your model is overfitting or underfitting and make informed decisions to improve its performance and generalization abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5cec3-5fc9-43fd-8cd6-2ceee4f505bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ea7d3-b2c2-4723-8107-60bb6d3d9d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ad4b266-2e87-4523-8ff8-74b8c6327673",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017f3d7-dfe8-457b-ac92-633ff04f828c",
   "metadata": {},
   "source": [
    "Bias and variance are two distinct sources of error in machine learning models that affect the model's ability to generalize to new, unseen data. Let's compare and contrast bias and variance and provide examples of high bias and high variance models:\n",
    "\n",
    "Bias:\n",
    "\n",
    "- Bias represents the error introduced by approximating a real-world problem with a simplified model.\n",
    "- High bias models are overly simplistic and make strong assumptions about the data, leading to poor fit to the training data and new data.\n",
    "- Bias can result in underfitting, where the model fails to capture the underlying patterns in the data.\n",
    "- A model with high bias may consistently make systematic errors in its predictions.\n",
    "\n",
    "Variance:\n",
    "\n",
    "- Variance represents the model's sensitivity to fluctuations or noise in the training data.\n",
    "- High variance models are overly complex and capture noise and random fluctuations in the training data, leading to memorization.\n",
    "- Variance can result in overfitting, where the model fits the training data very well but fails to generalize to new data.\n",
    "- A model with high variance may perform well on the training data but poorly on unseen data.\n",
    "\n",
    "Examples of High Bias and High Variance Models:\n",
    "\n",
    "1. High Bias (Underfitting):\n",
    "- Example: Linear regression applied to a highly non-linear dataset.\n",
    "- Performance: Both training and test errors are high.\n",
    "- Characteristic: The model oversimplifies the relationships in the data, resulting in systematic errors and poor fit.\n",
    "\n",
    "2. High Variance (Overfitting):\n",
    "- Example: A deep neural network with many layers and parameters trained on a small dataset.\n",
    "- Performance: Training error is significantly lower than test error.\n",
    "- Characteristic: The model memorizes the training data and captures noise, leading to poor generalization and high variability in predictions.\n",
    "\n",
    "Comparison:\n",
    "\n",
    "- Bias is related to the model's ability to fit the data and capture its underlying patterns.\n",
    "- Variance is related to the model's sensitivity to changes in the training data and its ability to generalize.\n",
    "- High bias models have limited flexibility and make strong assumptions, leading to systematic errors.\n",
    "- High variance models are too flexible and capture noise, resulting in inconsistent and highly variable predictions.\n",
    "- Bias and variance have an inverse relationship; as one increases, the other decreases.\n",
    "\n",
    "Balancing Bias and Variance:\n",
    "\n",
    "The goal in machine learning is to find the right balance between bias and variance to achieve good generalization performance. This is known as the bias-variance tradeoff. A well-tuned model strikes a balance by avoiding overly simplistic assumptions (high bias) while preventing overfitting (high variance) to the noise in the training data.\n",
    "\n",
    "Regularization techniques, cross-validation, and appropriate model complexity are used to manage the bias-variance tradeoff and create models that generalize well to new data while avoiding underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95964f58-5dc1-4fb2-8bd8-e49a027c38c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072599d-aeff-45a5-8af7-a04da0af14a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ad633e-f600-4b07-805e-b1ce9694b401",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55e662-7419-4e97-8db6-00f362351c36",
   "metadata": {},
   "source": [
    "Regularization is a set of techniques used in machine learning to prevent overfitting, which occurs when a model learns to fit the noise and randomness in the training data rather than the underlying patterns. Regularization methods add constraints or penalties to the model's optimization process, encouraging it to be simpler and less prone to overfitting.\n",
    "\n",
    "Common Regularization Techniques:\n",
    "\n",
    "1. L1 Regularization (Lasso):\n",
    "L1 regularization adds a penalty proportional to the absolute value of the model's coefficients. It encourages some coefficients to become exactly zero, effectively performing feature selection and making the model simpler. L1 regularization is particularly useful when there are many irrelevant or redundant features.\n",
    "\n",
    "Mathematically: Loss + λ * ∑|coefficients|\n",
    "\n",
    "2. L2 Regularization (Ridge):\n",
    "L2 regularization adds a penalty proportional to the square of the model's coefficients. It discourages large coefficients, promoting more balanced and distributed weights. L2 regularization is effective when all features contribute to the model's performance.\n",
    "\n",
    "Mathematically: Loss + λ * ∑(coefficients^2)\n",
    "\n",
    "3. Elastic Net Regularization:\n",
    "Elastic Net combines L1 and L2 regularization, providing a balance between feature selection (L1) and coefficient balance (L2). It is suitable when there are many features and some of them are highly correlated.\n",
    "\n",
    "Mathematically: Loss + λ1 * ∑|coefficients| + λ2 * ∑(coefficients^2)\n",
    "\n",
    "4. Dropout (Neural Networks):\n",
    "Dropout is a regularization technique specific to neural networks. During training, random neurons are \"dropped out\" by setting their outputs to zero with a certain probability. This prevents the network from relying too heavily on specific neurons and encourages it to learn more robust features.\n",
    "\n",
    "5. Early Stopping:\n",
    "Early stopping involves monitoring the model's performance on a validation set during training and stopping when the performance starts to degrade. This prevents the model from overfitting by preventing it from learning noise present in later training iterations.\n",
    "\n",
    "6. Data Augmentation:\n",
    "Data augmentation involves creating new training examples by applying random transformations (rotations, flips, translations, etc.) to the existing data. This increases the diversity of the training set, helping the model generalize better.\n",
    "\n",
    "7. Max Norm Regularization:\n",
    "Max norm regularization constrains the weights of the model by limiting their maximum value. This helps prevent overly large weights that could lead to overfitting.\n",
    "\n",
    "The Mechanism:\n",
    "\n",
    "Regularization methods modify the model's loss function by adding a penalty term that depends on model complexity (i.e., the size of the coefficients or weights). As the optimization process seeks to minimize the combined loss and penalty, the model is encouraged to have smaller coefficients, effectively simplifying its structure. This discourages the model from fitting noise and reduces the risk of overfitting.\n",
    "\n",
    "Regularization techniques should be chosen based on the problem at hand and the characteristics of the data. The regularization strength (λ) is a hyperparameter that controls the amount of regularization applied. Proper hyperparameter tuning and validation are crucial to achieve the right balance between model complexity and generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
