{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79909733-6fe6-4fbc-8a95-0a10df048e1e",
   "metadata": {},
   "source": [
    "Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d71585-bbcd-46a0-880e-2ceeb12d3e67",
   "metadata": {},
   "source": [
    "The Probability Mass Function (PMF) and Probability Density Function (PDF) are both mathematical functions used to describe the probability distribution of a discrete random variable and a continuous random variable, respectively. These functions help us understand the likelihood of different outcomes or values occurring in a random experiment.\n",
    "\n",
    "(A) Probability Mass Function (PMF):\n",
    "The PMF is used for discrete random variables, where the possible outcomes are countable and finite. It assigns a probability to each individual outcome of the random variable.\n",
    "\n",
    "Example of PMF:\n",
    "Let's consider a fair six-sided dice. The random variable X represents the outcome of rolling the dice, and its possible values are 1, 2, 3, 4, 5, and 6. Since the dice is fair, each outcome has an equal probability of occurring, which is 1/6.\n",
    "\n",
    "The PMF for this scenario is given by:\n",
    "\n",
    "PMF(X = 1) = 1/6\n",
    "\n",
    "PMF(X = 2) = 1/6\n",
    "\n",
    "PMF(X = 3) = 1/6\n",
    "\n",
    "PMF(X = 4) = 1/6\n",
    "\n",
    "PMF(X = 5) = 1/6\n",
    "\n",
    "PMF(X = 6) = 1/6\n",
    "\n",
    "(B) Probability Density Function (PDF):\n",
    "The PDF is used for continuous random variables, where the possible outcomes form an uncountably infinite set, usually an interval on the real number line. It provides the relative likelihood of the random variable taking on a specific value within a given range.\n",
    "\n",
    "Example of PDF:\n",
    "Let's consider a continuous random variable Y that follows a standard normal distribution, which is a type of bell-shaped curve centered at 0 with a standard deviation of 1.\n",
    "\n",
    "The PDF of the standard normal distribution is given by the formula:\n",
    "\n",
    "f(y) = (1 / sqrt(2 * π)) * e^((-y^2) / 2)\n",
    "\n",
    "Using this PDF, we can find the probability of Y falling within a certain range, such as P(a < Y < b). For example, the probability of Y being between -1 and 1 is approximately 0.6827.\n",
    "\n",
    "It's important to note that the area under the PDF curve within a specified range represents the probability of the random variable falling within that range.\n",
    "\n",
    "In summary, the PMF is used for discrete random variables, providing the probability of specific outcomes, while the PDF is used for continuous random variables, describing the relative likelihood of values within a given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f89112-f02c-4c6a-be34-8ea0cfafe1ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2fb4c5-ad09-4328-b34f-1e06b06dbe53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0efec3ef-7e69-48c1-a5c7-024f7139a7c4",
   "metadata": {},
   "source": [
    "Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bda2b0-1238-4334-accb-0e55d7b226c8",
   "metadata": {},
   "source": [
    "The Cumulative Density Function (CDF) is a function that represents the cumulative probability of a random variable taking on a value less than or equal to a specific value. It provides a way to describe the probability distribution of a random variable and helps us understand the likelihood of different values occurring up to a certain point.\n",
    "\n",
    "Mathematically, for a random variable X, the CDF is defined as:\n",
    "CDF(x) = P(X ≤ x)\n",
    "\n",
    "Where:\n",
    "- CDF(x) is the cumulative probability that X is less than or equal to x.\n",
    "- P(X ≤ x) is the probability that the random variable X takes on a value less than or equal to x.\n",
    "\n",
    "The CDF is particularly useful when dealing with both discrete and continuous random variables, as it allows us to find probabilities of intervals and ranges of values.\n",
    "\n",
    "Example of CDF:\n",
    "\n",
    "Let's consider a fair six-sided dice again. The random variable X represents the outcome of rolling the dice, and its possible values are 1, 2, 3, 4, 5, and 6.\n",
    "\n",
    "The CDF of the dice can be represented as follows:\n",
    "- CDF(X = 1) = P(X ≤ 1) = 1/6\n",
    "- CDF(X = 2) = P(X ≤ 2) = 2/6 = 1/3\n",
    "- CDF(X = 3) = P(X ≤ 3) = 3/6 = 1/2\n",
    "- CDF(X = 4) = P(X ≤ 4) = 4/6 = 2/3\n",
    "- CDF(X = 5) = P(X ≤ 5) = 5/6\n",
    "- CDF(X = 6) = P(X ≤ 6) = 6/6 = 1\n",
    "\n",
    "The CDF shows the cumulative probability of each value or below, so it represents the probability of rolling a value less than or equal to a specific number on the dice.\n",
    "\n",
    "Why CDF is used?\n",
    "The CDF is used for several reasons:\n",
    "\n",
    "1. Probabilities of Ranges: The CDF allows us to find the probability of a random variable falling within a specified range, such as P(a ≤ X ≤ b), which is essential for various statistical calculations.\n",
    "\n",
    "2. Identifying Percentiles: The CDF can help identify percentiles of the distribution, such as the 25th percentile (P25), 50th percentile (median), or 75th percentile (P75), which provide insights into the spread and central tendency of the data.\n",
    "\n",
    "3. Comparing Distributions: CDFs are useful for comparing different probability distributions and understanding how they differ in terms of likelihoods for different values.\n",
    "\n",
    "4. Hypothesis Testing: The CDF is used in hypothesis testing to calculate critical values and make decisions based on the probability of observing a specific outcome.\n",
    "\n",
    "In summary, the CDF is a powerful tool for understanding the distribution of random variables and is widely used in probability theory, statistics, and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0685f4d-7024-4611-8c25-8202b7116156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b759240-c1d7-4ef1-9c4a-93517190b98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86b3392c-bdcd-4a1f-94e9-01c303c91d0e",
   "metadata": {},
   "source": [
    "Q3: What are some examples of situations where the normal distribution might be used as a model? Explain how the parameters of the normal distribution relate to the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75862378-7b13-4640-90c0-426a7fb45308",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution, is one of the most widely used probability distributions in various fields due to its many useful properties. It is characterized by its bell-shaped curve and is often used as a model in situations where data tends to cluster around a central mean value with symmetrical spread.\n",
    "\n",
    "Examples of situations where the normal distribution might be used as a model include:\n",
    "\n",
    "1. Height of a Population: The heights of a large population of adults often follow a normal distribution. The majority of people will have heights close to the average (mean), and fewer individuals will be much taller or shorter.\n",
    "\n",
    "2. Test Scores: In educational settings, test scores are often normally distributed. For standardized tests like the SAT or IQ tests, the scores tend to cluster around the mean score with a symmetrical spread.\n",
    "\n",
    "3. Measurement Errors: When measuring physical quantities, such as the weight of objects or the length of a process, random errors can often be approximated as normally distributed around the true value.\n",
    "\n",
    "4. Biological Parameters: Many biological parameters, such as blood pressure, heart rate, or enzyme activity, can be modeled with a normal distribution.\n",
    "\n",
    "The normal distribution is characterized by two parameters: the mean (μ) and the standard deviation (σ). The mean determines the central value around which the data clusters, while the standard deviation controls the spread or dispersion of the data points from the mean.\n",
    "\n",
    "1. Mean (μ): The mean represents the central location of the distribution. It is the average value of all data points in the dataset. Shifting the mean to the left or right will change the position of the center of the bell curve.\n",
    "\n",
    "2. Standard Deviation (σ): The standard deviation measures the dispersion or variability of the data points around the mean. A smaller standard deviation means the data is more concentrated around the mean, resulting in a narrower bell curve. Conversely, a larger standard deviation indicates a more spread-out distribution with a wider bell curve.\n",
    "\n",
    "Together, the mean and standard deviation fully define the normal distribution. By adjusting these parameters, we can control the shape of the distribution. A smaller mean and a larger standard deviation will shift the distribution to the left and make it wider, while a larger mean and a smaller standard deviation will shift it to the right and make it narrower. The standard deviation is also used to measure the spread of the distribution and quantify the degree of uncertainty or variability in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01566a0d-d411-4601-89c5-79a996fc9868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc02f1-fb04-4097-bbd0-7c8d5581fed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1815e0b9-7526-4b99-b8f3-cd41098ebf2f",
   "metadata": {},
   "source": [
    "Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5d7b8-909f-4437-a25a-b176254c948f",
   "metadata": {},
   "source": [
    "The normal distribution is of great importance in statistics and data analysis due to its many valuable properties. Some of the key reasons for its significance are:\n",
    "\n",
    "- Central Limit Theorem: The normal distribution plays a fundamental role in the Central Limit Theorem. According to this theorem, the sum of a large number of independent and identically distributed random variables will tend to follow a normal distribution, regardless of the original distribution of the individual variables. This property allows us to make inferences about a population based on a sample, which is crucial in statistical analysis.\n",
    "\n",
    "- Data Modeling: The normal distribution is often used as a model for real-life data that approximately follows a bell-shaped pattern. While many natural phenomena exhibit some degree of randomness and variability, a normal distribution can provide a good approximation for many processes, making it a powerful tool for data modeling.\n",
    "\n",
    "- Statistical Inference: In hypothesis testing and confidence interval estimation, the normal distribution is widely used. When the sample size is large, several statistical tests and estimation methods assume normality, allowing researchers to draw meaningful conclusions about the population.\n",
    "\n",
    "- Real-Life Applications: Many real-life phenomena exhibit a normal distribution, or they can be well approximated by a normal distribution. Some examples of real-life applications of the normal distribution include:\n",
    "\n",
    "    * Heights of a population: Human heights tend to follow a normal distribution in large populations.\n",
    "    * IQ scores: IQ scores are standardized to have a normal distribution with a mean of 100 and a standard deviation of 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05c77f-025e-41b4-af12-48a9c0e3eda5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389bd8ae-9d20-4364-9c03-d234070ba111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2bc0a74a-395c-434e-bc02-7e8dc418b631",
   "metadata": {},
   "source": [
    "Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea82b2-9174-4893-8ed7-e3deb25fb19b",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is a discrete probability distribution that models a single binary event with two possible outcomes: success (usually represented as 1) and failure (usually represented as 0). The distribution is characterized by a single parameter, often denoted as \"p,\" which represents the probability of success.\n",
    "\n",
    "Example of Bernoulli Distribution:\n",
    "\n",
    "A classic example of the Bernoulli distribution is flipping a fair coin. Suppose we define \"success\" as getting heads and \"failure\" as getting tails. In this case, the probability of success (getting heads) is 0.5, and the probability of failure (getting tails) is also 0.5. The outcomes of this experiment can be represented by a Bernoulli distribution, where p = 0.5.\n",
    "\n",
    "Difference between Bernoulli Distribution and Binomial Distribution:\n",
    "\n",
    "1. Number of Trials:\n",
    "   - Bernoulli Distribution: Represents a single trial or experiment with two possible outcomes.\n",
    "   - Binomial Distribution: Represents the number of successes in a fixed number of independent Bernoulli trials (experiments).\n",
    "\n",
    "2. Number of Parameters:\n",
    "   - Bernoulli Distribution: Has only one parameter \"p\" (probability of success).\n",
    "   - Binomial Distribution: Has two parameters - \"n\" (number of trials) and \"p\" (probability of success).\n",
    "\n",
    "3. Probability Mass Function (PMF):\n",
    "   - Bernoulli Distribution: The PMF of the Bernoulli distribution is P(X = x) = p^x * (1-p)^(1-x), where x = 0 or 1.\n",
    "   - Binomial Distribution: The PMF of the Binomial distribution is P(X = k) = C(n, k) * p^k * (1-p)^(n-k), where k is the number of successes in \"n\" trials.\n",
    "\n",
    "4. Range of Values:\n",
    "   - Bernoulli Distribution: Can only take two possible values - 0 or 1.\n",
    "   - Binomial Distribution: Can take values from 0 to \"n,\" where \"n\" is the number of trials.\n",
    "\n",
    "In summary, the Bernoulli distribution is used to model a single binary event, while the Binomial distribution is used to model the number of successes in a fixed number of independent Bernoulli trials. The Binomial distribution generalizes the Bernoulli distribution to multiple trials, allowing us to compute the probability of obtaining a specific number of successes in those trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a04b56b-f5fe-40fd-a3ad-7512b9f0fb77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436834b3-06b4-4c7a-8c8e-85f3b39bc90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f420b816-7421-4a89-86c5-9ce78d61c241",
   "metadata": {},
   "source": [
    "Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset is normally distributed, what is the probability that a randomly selected observation will be greater than 60? Use the appropriate formula and show your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba443b-9295-4ea8-bed8-4304616a4697",
   "metadata": {},
   "source": [
    "To find the probability that a randomly selected observation from a normally distributed dataset with a mean of 50 and a standard deviation of 10 will be greater than 60, we can use the z-score formula and the standard normal distribution table.\n",
    "\n",
    "The z-score formula is given by:\n",
    "\n",
    "z = (X - μ) / σ\n",
    "\n",
    "Where:\n",
    "- X = Value of the observation (in this case, X = 60)\n",
    "- μ = Mean of the dataset (μ = 50)\n",
    "- σ = Standard deviation of the dataset (σ = 10)\n",
    "\n",
    "Calculating the z-score:\n",
    "- z = (60 - 50) / 10\n",
    "- z = 1\n",
    "\n",
    "Now, we look up the probability corresponding to the z-score of 1 in the standard normal distribution table. The standard normal distribution table provides the cumulative probability up to a given z-score. In this case, we want the probability of being greater than the z-score, which is 1 - P(z < 1).\n",
    "\n",
    "Using the table or a calculator, we find that P(z < 1) is approximately 0.8413.\n",
    "\n",
    "So, the probability of a randomly selected observation being greater than 60 is:\n",
    "\n",
    "P(X > 60) = 1 - P(z < 1) = 1 - 0.8413 = 0.1587\n",
    "\n",
    "Therefore, the probability is approximately 0.1587, or 15.87%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edb3cfe-fab2-419a-91c9-a5423e6700f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfe486-6496-423e-9378-8af2dae32749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf4b135a-423e-4be1-9aa6-2bf86565e313",
   "metadata": {},
   "source": [
    "Q7: Explain uniform Distribution with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59321e5f-492e-4646-9b32-9f5661f9a66d",
   "metadata": {},
   "source": [
    "\n",
    "Uniform distribution is a probability distribution where all values in the range of the distribution have an equal probability of occurring. In other words, each value is equally likely to be chosen.\n",
    "\n",
    "An example of a uniform distribution is rolling a fair six-sided die. In this case, each outcome (numbers 1 to 6) has an equal probability of 1/6 or approximately 0.1667 of occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d43e5a-e783-4a56-8769-752752fb807f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84a96d-717e-47a4-b908-f5472f970e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b92e118-b1a8-4a15-8fd1-59c83ada9fcb",
   "metadata": {},
   "source": [
    "Q8: What is the z score? State the importance of the z score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e4968-d917-419f-a0d5-7890722584d4",
   "metadata": {},
   "source": [
    "The z-score, also known as the standard score, is a statistical measurement that describes a value's relationship to the mean of a group of values. It is a dimensionless number that represents how many standard deviations a particular data point is from the mean.\n",
    "\n",
    "The formula for calculating the z-score of a data point x, given a mean (μ) and standard deviation (σ) of the dataset, is:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "The importance of the z-score lies in its ability to standardize data and make comparisons across different datasets with varying scales and distributions. It allows us to determine how unusual or typical a data point is relative to the rest of the data. Here are some key points highlighting the importance of the z-score:\n",
    "\n",
    "1. Standardization: The z-score transforms raw data into a standardized scale, where a positive z-score indicates that a data point is above the mean, and a negative z-score indicates that it is below the mean. This makes it easier to compare values across different datasets.\n",
    "\n",
    "2. Outlier Detection: Z-scores help identify outliers in a dataset. Outliers are data points that deviate significantly from the rest of the data. Typically, z-scores greater than a certain threshold (e.g., ±2 or ±3) are considered outliers.\n",
    "\n",
    "3. Probability Calculation: Z-scores are used in calculating probabilities associated with specific data points in a normal distribution. For example, in a standard normal distribution (mean=0, standard deviation=1), the z-score represents the probability of obtaining a value less than or greater than a specific data point.\n",
    "\n",
    "4. Hypothesis Testing: Z-tests, which rely on z-scores, are commonly used in hypothesis testing. They allow us to determine whether a sample mean significantly differs from a known population mean.\n",
    "\n",
    "5. Data Comparison: When comparing data from different populations or datasets, z-scores enable us to assess relative positions and understand how values in one dataset compare to another.\n",
    "\n",
    "Overall, the z-score is a powerful tool in statistics, aiding in data analysis, hypothesis testing, and understanding the distribution of data points. It provides valuable insights into the characteristics of data and facilitates informed decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7dd26-7012-4c62-b50d-0158011c01cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a533331-db5f-4eb6-aa91-5c4c42c17168",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7363d46-8224-4228-953b-eebb2fcdc9be",
   "metadata": {},
   "source": [
    "Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441b28c-7ae5-44cf-9d0a-075f31d03cae",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in statistics that states that the sampling distribution of the sample mean of any independent, identically distributed random variables will tend to follow a normal distribution as the sample size increases, regardless of the shape of the original population distribution.\n",
    "\n",
    "The significance of the Central Limit Theorem lies in its wide-ranging applications and its usefulness in making statistical inferences. Here are some key points highlighting the importance of the Central Limit Theorem:\n",
    "\n",
    "1. Normal Approximation: The Central Limit Theorem allows us to approximate the sampling distribution of the sample mean, regardless of the underlying population distribution, to a normal distribution. This is particularly important because the normal distribution is well-understood and has many useful properties.\n",
    "\n",
    "2. Sample Mean Properties: The CLT implies that as the sample size increases, the mean of the sample means will approach the population mean, and the standard deviation of the sample means (standard error) will decrease. This provides valuable information about the precision of sample estimates.\n",
    "\n",
    "3. Confidence Intervals: The Central Limit Theorem is the basis for constructing confidence intervals for population parameters, such as the population mean. It helps in determining the range of values within which we can be confident that the true population parameter lies.\n",
    "\n",
    "4. Hypothesis Testing: The CLT underpins many statistical hypothesis tests. For example, when testing hypotheses about population means, we often assume that the sample means follow a normal distribution due to the CLT.\n",
    "\n",
    "5. Large Sample Size Assumption: The Central Limit Theorem allows statisticians to work with large samples from complex populations and make valid inferences, even if the underlying distribution is not known or is not normal.\n",
    "\n",
    "6. Real-World Applications: The CLT is applicable to a wide range of real-world scenarios. It is particularly valuable in areas such as quality control, market research, public health, social sciences, and finance, where data often exhibit complex distributions.\n",
    "\n",
    "In summary, the Central Limit Theorem is a foundational concept in statistics that enables us to make inferences about population parameters based on sample data. It provides a powerful tool for statistical analysis, hypothesis testing, and making reliable estimates in diverse fields of research and practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1a499-0a2e-49c3-8f79-611ae5dd9677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53779580-7f78-4ba2-88b9-c9d6a55418d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96cda55c-581a-4271-b922-b4106cada876",
   "metadata": {},
   "source": [
    "Q10: State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708560a-dfa0-435d-ba70-058015a258da",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) relies on certain assumptions to hold true. These assumptions are essential for the theorem to apply and for the sampling distribution of the sample mean to converge to a normal distribution. The key assumptions of the Central Limit Theorem are as follows:\n",
    "\n",
    "1. Independence: The individual observations in the sample should be independent of each other. This means that the value of one observation should not be influenced by or related to the values of other observations in the sample.\n",
    "\n",
    "2. Identical Distribution: The sampled observations should be drawn from the same probability distribution. In other words, the underlying population from which the samples are taken should have the same distribution for all observations.\n",
    "\n",
    "3. Finite Variance: The population from which the samples are drawn should have a finite variance. If the variance is infinite or does not exist, the CLT may not hold.\n",
    "\n",
    "4. Sample Size: The sample size should be sufficiently large. Although there is no strict rule for what constitutes a \"large\" sample size, as a general guideline, a sample size of at least 30 is often considered adequate for the CLT to apply. However, the larger the sample size, the better the approximation to the normal distribution.\n",
    "\n",
    "It's important to note that while the CLT is robust and holds under many situations, there are cases where the assumptions may not be fully met. For example, when dealing with small samples or heavily skewed populations, the normal approximation may not be as accurate. In such cases, alternative methods or modifications of the CLT may be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
