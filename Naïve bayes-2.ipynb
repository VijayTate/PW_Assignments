{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3393edb-f56b-4542-958b-7aca0cf682c5",
   "metadata": {},
   "source": [
    "#### Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8995e93e-e859-46e2-a621-f643e39c3c18",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that they use the health insurance plan, we can use conditional probability. We are looking for P(Smoker | Uses Insurance Plan).\n",
    "\n",
    "We can use the formula for conditional probability:\n",
    "\n",
    "P(Smoker | Uses Insurance Plan) = P(Smoker and Uses Insurance Plan) / P(Uses Insurance Plan)\n",
    "\n",
    "We are given two pieces of information:\n",
    "\n",
    "1. P(Uses Insurance Plan) = 70% = 0.70 (the probability that an employee uses the insurance plan).\n",
    "2. P(Smoker | Uses Insurance Plan) = 40% = 0.40 (the probability that a person is a smoker given that they use the insurance plan).\n",
    "\n",
    "Now, we can plug these values into the formula:\n",
    "\n",
    "P(Smoker | Uses Insurance Plan) = P(Smoker and Uses Insurance Plan) / P(Uses Insurance Plan)\n",
    "\n",
    "0.40 = P(Smoker and Uses Insurance Plan) / 0.70\n",
    "\n",
    "Now, we can solve for P(Smoker and Uses Insurance Plan) :\n",
    "\n",
    "P(Smoker and Uses Insurance Plan) = 0.40 * 0.70 = 0.28\n",
    "\n",
    "So, the probability that an employee is a smoker given that they use the health insurance plan is 0.28, or 28%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c4845-b884-4893-9cc2-112d4e387267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8719c7f-9dfd-4a05-adea-a1699e909d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a540c1e2-b62c-422e-a7ea-ab4a45cf1503",
   "metadata": {},
   "source": [
    "#### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6befafc4-1608-497b-b8e6-f62eef97f376",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two different variants of the Naive Bayes classifier, each suited to different types of data and applications. Here are the key differences between them:\n",
    "\n",
    "1. **Data Type:**\n",
    "   - **Bernoulli Naive Bayes:** It is typically used for binary or categorical data where features represent the presence (1) or absence (0) of specific attributes or events. It is well-suited for problems involving binary text data or presence/absence features.\n",
    "   - **Multinomial Naive Bayes:** It is used for discrete data, particularly when dealing with count or frequency data. It is commonly applied to text classification problems where features represent word counts, term frequencies, or other discrete numerical values.\n",
    "\n",
    "2. **Feature Representation:**\n",
    "   - **Bernoulli Naive Bayes:** Assumes binary features, and it models the probability of each feature being either 0 or 1. It is useful when you want to capture the presence or absence of specific features.\n",
    "   - **Multinomial Naive Bayes:** Assumes discrete numerical features, typically non-negative integers. It models the probability of observing a specific count or frequency of each feature. It is useful for modeling the distribution of discrete values, such as word frequencies.\n",
    "\n",
    "3. **Use Cases:**\n",
    "   - **Bernoulli Naive Bayes:** It is commonly used for text classification tasks, such as spam detection, sentiment analysis, or document categorization, where the focus is on the presence or absence of specific words or features.\n",
    "   - **Multinomial Naive Bayes:** It is also used for text classification but is more appropriate when you want to consider the frequency of words or terms in documents. It is suitable for tasks like topic modeling or document retrieval.\n",
    "\n",
    "4. **Mathematical Formulation:**\n",
    "   - **Bernoulli Naive Bayes:** It models the probability of observing a binary feature given a class label, typically using Bernoulli distribution probabilities.\n",
    "   - **Multinomial Naive Bayes:** It models the probability of observing a count or frequency of a feature given a class label, typically using multinomial distribution probabilities.\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of your data and the specific problem you are trying to solve. If your data consists of binary or presence/absence features, Bernoulli Naive Bayes may be more appropriate. If your data involves discrete counts or frequencies, particularly in text data, Multinomial Naive Bayes is a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dc6ef4-9d06-4e96-9bee-2bc5a2ab7971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9055f5ca-2090-462c-a82c-c583e24cb939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fa4a233-b873-42c2-9402-5c70c28c4573",
   "metadata": {},
   "source": [
    "#### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af031fb-2fac-49cf-a8e1-b62481135914",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values by considering them as a separate category or class. In other words, missing values are treated as a distinct feature category in Bernoulli Naive Bayes, and their presence or absence is considered when calculating probabilities.\n",
    "\n",
    "Here's how Bernoulli Naive Bayes deals with missing values:\n",
    "\n",
    "1. **Encoding Missing Values:**\n",
    "   - Bernoulli Naive Bayes typically requires binary features, where each feature is either 0 (absence) or 1 (presence). To handle missing values, you can encode them as a separate category, often represented as -1 or another distinct value.\n",
    "\n",
    "2. **Modeling Missing Values:**\n",
    "   - When calculating probabilities for each feature given a class label, the classifier considers the presence (1), absence (0), and the missing value (-1) as separate possibilities.\n",
    "\n",
    "3. **Probability Estimation:**\n",
    "   - In the probability estimation step, the classifier calculates the probability of each feature being 0, 1, or -1 for each class.\n",
    "\n",
    "4. **Predictions:**\n",
    "   - When making predictions, the classifier factors in the probabilities associated with the presence, absence, and missing values of features for each class and selects the class with the highest posterior probability.\n",
    "\n",
    "5. **Handling Missing Values in Test Data:**\n",
    "   - When applying the trained model to new data that may contain missing values, the classifier uses the encoded missing value (-1) to make predictions.\n",
    "\n",
    "It's important to note that the treatment of missing values as a separate category can introduce additional complexity into the model and may require careful preprocessing of the data. The choice of how to encode missing values (e.g., as -1) should be consistent between the training and testing data to ensure accurate predictions.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes accommodates missing values by treating them as a distinct category and factoring them into the probability calculations for each feature. However, handling missing values in Naive Bayes can be a topic of consideration during data preprocessing and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb9ae4-ed85-4650-831b-2605b8958b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a46ab17-4eed-4372-9db6-57a65bd72205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5b633e4-d112-4d80-a87b-a5d6310335f4",
   "metadata": {},
   "source": [
    "#### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1005c-0245-480f-8bd2-bb08bf1a7992",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. While it is often associated with binary classification problems, it can be extended to handle multi-class classification by employing a one-vs-all (also known as one-vs-rest) strategy.\n",
    "\n",
    "Here's how Gaussian Naive Bayes can be adapted for multi-class classification:\n",
    "\n",
    "1. **One-vs-All (OvA) Strategy:**\n",
    "   - In a multi-class classification problem with \\(k\\) classes, you create \\(k\\) separate binary classifiers, one for each class.\n",
    "   - For each binary classifier, you treat one class as the positive class and group the remaining \\(k-1\\) classes as the negative class.\n",
    "\n",
    "2. **Training:**\n",
    "   - For each binary classifier, you train it using the Gaussian Naive Bayes algorithm but with the labels transformed into binary values, where the positive class is labeled as 1, and the negative class is labeled as 0.\n",
    "   - Each binary classifier learns the Gaussian distribution parameters (mean and variance) for its respective positive class.\n",
    "\n",
    "3. **Prediction:**\n",
    "   - To make predictions for a new instance, you apply all \\(k\\) binary classifiers to it.\n",
    "   - The class associated with the binary classifier that produces the highest probability or score is the predicted class for the multi-class problem.\n",
    "\n",
    "4. **Decision Rule:**\n",
    "   - The decision rule is typically based on the posterior probabilities produced by each binary classifier. You can choose the class with the highest posterior probability or apply a threshold to make decisions.\n",
    "\n",
    "This OvA strategy effectively transforms the multi-class problem into a set of binary classification problems, where each binary classifier specializes in distinguishing one class from the rest. Gaussian Naive Bayes is well-suited for this approach when features are continuous and can be modeled using Gaussian (normal) distributions.\n",
    "\n",
    "In summary, while Gaussian Naive Bayes is originally designed for binary classification, it can be extended to handle multi-class classification through the one-vs-all strategy, making it a versatile algorithm for a wide range of classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9be76d3-a8c1-4d88-bbfe-06f2bedc7db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e6319-fe26-4863-b15e-9c314600fb48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44c3d75a-e786-4ba4-925c-65f115236c59",
   "metadata": {},
   "source": [
    "#### Q5. Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7386583-67df-4146-920a-6e9e3572ca4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>is_spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  is_spam  \n",
       "0                       278        1  \n",
       "1                      1028        1  \n",
       "2                      2259        1  \n",
       "3                       191        1  \n",
       "4                       191        1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "destination_file = \"spambase.data\"\n",
    "urllib.request.urlretrieve(url, destination_file)\n",
    "\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "column_names = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n",
    "    \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n",
    "    \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n",
    "    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\",\n",
    "    \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\",\n",
    "    \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\",\n",
    "    \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\",\n",
    "    \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\",\n",
    "    \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\",\n",
    "    \"capital_run_length_longest\", \"capital_run_length_total\", \"is_spam\"\n",
    "]\n",
    "\n",
    "# Read the dataset into a DataFrame\n",
    "df = pd.read_csv(\"spambase.data\", names=column_names, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d71d053-1ee2-41ff-b00a-b29a141b078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and the target (y)\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "846b7c6a-e131-49e2-8102-61425c65b3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy (Bernoulli Naive Bayes): 0.8839380364047911\n",
      "Average Accuracy (Multinomial Naive Bayes): 0.7863496180326323\n",
      "Average Accuracy (Gaussian Naive Bayes): 0.8217730830896915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Create instances of each Naive Bayes classifier\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and evaluate the classifiers\n",
    "scores_bernoulli = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy')\n",
    "scores_multinomial = cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy')\n",
    "scores_gaussian = cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "# Print the average accuracy scores for each classifier\n",
    "print(\"Average Accuracy (Bernoulli Naive Bayes):\", scores_bernoulli.mean())\n",
    "print(\"Average Accuracy (Multinomial Naive Bayes):\", scores_multinomial.mean())\n",
    "print(\"Average Accuracy (Gaussian Naive Bayes):\", scores_gaussian.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dafc1f-50e5-406d-af53-9ccb91e5f198",
   "metadata": {},
   "source": [
    "The results obtained from evaluating different variants of Naive Bayes classifiers on the dataset are as follows:\n",
    "\n",
    "- Average Accuracy (Bernoulli Naive Bayes): 0.8839\n",
    "- Average Accuracy (Multinomial Naive Bayes): 0.7863\n",
    "- Average Accuracy (Gaussian Naive Bayes): 0.8218\n",
    "\n",
    "**Discussion of Results:**\n",
    "\n",
    "1. **Bernoulli Naive Bayes:** The Bernoulli Naive Bayes classifier achieved the highest average accuracy among the three variants, with an accuracy of approximately 88.39%. This variant performed the best in this specific dataset, likely because the dataset or the features are binary or binary-like (presence or absence of specific attributes).\n",
    "\n",
    "2. **Multinomial Naive Bayes:** The Multinomial Naive Bayes classifier had a lower average accuracy of approximately 78.63%. This lower performance might be attributed to the nature of the dataset or features. Multinomial Naive Bayes is more suitable for datasets with discrete counts or frequencies, often used in text classification tasks.\n",
    "\n",
    "3. **Gaussian Naive Bayes:** The Gaussian Naive Bayes classifier achieved an average accuracy of approximately 82.18%. Gaussian Naive Bayes is suitable for datasets with continuous features that can be modeled using Gaussian (normal) distributions. It performed moderately well, but not as well as Bernoulli Naive Bayes.\n",
    "\n",
    "**Limitations of Naive Bayes:**\n",
    "\n",
    "- **Independence Assumption:** Naive Bayes classifiers assume that features are independent of each other given the class label. This assumption may not hold in real-world datasets, and violations of independence can affect model performance.\n",
    "\n",
    "- **Sensitivity to Feature Scaling:** Gaussian Naive Bayes is sensitive to the scale of continuous features. If features have different scales, it can impact the classifier's performance.\n",
    "\n",
    "- **Limited Expressiveness:** Naive Bayes classifiers have limited expressiveness compared to more complex models like decision trees or neural networks. They may not capture intricate relationships in the data.\n",
    "\n",
    "**Suggestions for Future Work:**\n",
    "\n",
    "1. **Feature Engineering:** Explore feature engineering techniques to improve the performance of Naive Bayes classifiers. Feature selection, dimensionality reduction, or transforming features might enhance the models' accuracy.\n",
    "\n",
    "2. **Hyperparameter Tuning:** Experiment with hyperparameter tuning for the Naive Bayes classifiers. Adjusting parameters or using smoothing techniques could lead to better results.\n",
    "\n",
    "3. **Ensemble Methods:** Consider using ensemble methods like Random Forest or Gradient Boosting, which can combine the predictions of multiple classifiers to potentially achieve higher accuracy.\n",
    "\n",
    "4. **Evaluate Other Algorithms:** Compare the performance of Naive Bayes with other classification algorithms (e.g., decision trees, support vector machines, or deep learning models) to determine if a different algorithm is better suited for this dataset.\n",
    "\n",
    "5. **Data Preprocessing:** Carefully preprocess the dataset, addressing issues like missing values, outliers, or class imbalances, which can impact model performance.\n",
    "\n",
    "6. **Cross-Validation Strategies:** Experiment with different cross-validation strategies, such as stratified or k-fold cross-validation, to ensure robust evaluation.\n",
    "\n",
    "In summary, while Naive Bayes classifiers are simple and interpretable, they may not always be the best choice for all datasets. Exploring alternative algorithms and preprocessing techniques can lead to improved model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
