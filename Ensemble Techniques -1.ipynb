{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab43e6a-7180-470d-9f4b-fc200cb41856",
   "metadata": {},
   "source": [
    "#### Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca6ce4-06e1-4118-adbb-891c5f73b16d",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning is a method of combining the predictions from multiple individual machine learning models to improve overall predictive performance. Ensembling leverages the diversity of multiple models to make more accurate and robust predictions compared to using a single model.\n",
    "\n",
    "The idea behind ensemble techniques is based on the concept of \"wisdom of the crowd,\" where a group of individuals can collectively make better decisions than any single individual. Similarly, ensembles aim to harness the collective predictive power of multiple models to enhance the accuracy and generalization of predictions.\n",
    "\n",
    "Ensemble techniques can be broadly categorized into two main types:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating):** In bagging, multiple instances of the same base model are trained on different subsets of the training data using techniques like bootstrapping. The predictions from each model are then aggregated, typically by majority voting (for classification) or averaging (for regression). Random Forest is a popular ensemble algorithm that uses bagging with decision trees as base models.\n",
    "\n",
    "2. **Boosting:** Boosting is an iterative ensemble technique in which multiple weak learners (models that perform slightly better than random chance) are trained sequentially. Each subsequent model focuses on correcting the errors made by the previous ones. AdaBoost and Gradient Boosting are examples of boosting algorithms.\n",
    "\n",
    "Ensemble techniques offer several advantages:\n",
    "\n",
    "- Improved Accuracy: Ensembling often leads to higher predictive accuracy because it combines the strengths of multiple models while mitigating their weaknesses.\n",
    "\n",
    "- Enhanced Robustness: Ensembles are less prone to overfitting and are more robust to noisy data.\n",
    "\n",
    "- Increased Generalization: Ensembles can generalize well to unseen data by reducing the bias and variance of the models.\n",
    "\n",
    "- Versatility: Ensembles can be applied to various types of machine learning algorithms, including decision trees, neural networks, support vector machines, and more.\n",
    "\n",
    "Common ensemble methods include Random Forest, AdaBoost, Gradient Boosting (including XGBoost and LightGBM), and Stacking. The choice of ensemble method depends on the problem and the types of base models being used.\n",
    "\n",
    "Ensemble techniques have become an integral part of machine learning because of their ability to significantly improve model performance and robustness across a wide range of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d5e6d-3289-4254-a2d5-85551517c3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6158c-37d3-404c-af94-8e8345e6e097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e75c7a6e-761f-4c4d-b9f2-4be17c2a8ca1",
   "metadata": {},
   "source": [
    "#### Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d767e-5e1c-4585-b508-e350c908971b",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several important reasons:\n",
    "\n",
    "1. **Improved Predictive Accuracy:** One of the primary motivations for using ensemble techniques is to improve predictive accuracy. By combining the predictions of multiple individual models, ensembles can reduce errors and make more accurate predictions. In many cases, ensembles outperform individual models.\n",
    "\n",
    "2. **Robustness:** Ensembles are more robust to noise and outliers in the data. Because they rely on the consensus of multiple models, they are less likely to be affected by individual model errors or data anomalies, leading to more reliable predictions.\n",
    "\n",
    "3. **Reduced Overfitting:** Ensemble methods, especially bagging, can help reduce overfitting. Overfitting occurs when a model learns the noise in the training data, leading to poor generalization to unseen data. Ensembles, by averaging or combining multiple models, tend to have lower variance and are less prone to overfitting.\n",
    "\n",
    "4. **Model Generalization:** Ensembles often generalize better to unseen data. By combining diverse models, they capture different aspects of the underlying patterns in the data, making them more effective at handling variations and complexities in real-world datasets.\n",
    "\n",
    "5. **Versatility:** Ensemble techniques can be applied to a wide range of machine learning algorithms as base models. Whether you're using decision trees, neural networks, support vector machines, or other models, you can often benefit from ensembling.\n",
    "\n",
    "6. **Handling Complex Relationships:** Ensembles can capture complex relationships in the data that individual models might miss. For example, gradient boosting algorithms can build models that focus on correcting errors made by previous models, effectively learning complex patterns in the data.\n",
    "\n",
    "7. **Model Stability:** Ensembles can make models more stable over time. When trained with different subsets of data or different initializations, ensembles tend to produce consistent and reliable predictions.\n",
    "\n",
    "8. **Scalability:** Ensembles can be distributed and parallelized easily, making them suitable for large-scale machine learning tasks and big data applications.\n",
    "\n",
    "9. **State-of-the-Art Performance:** In many machine learning competitions and real-world applications, ensemble methods have consistently achieved state-of-the-art performance, demonstrating their effectiveness in improving model quality.\n",
    "\n",
    "Common ensemble methods, such as Random Forest, AdaBoost, and Gradient Boosting, have been widely adopted in various domains, including finance, healthcare, natural language processing, computer vision, and more. They are considered essential tools for practitioners and data scientists looking to maximize predictive accuracy and model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5746d7-b89c-48be-9704-ec971b055264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0d07c-76f5-494d-a183-87d56afd4d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0d99aac-371e-4019-8c69-b5e9b9a0c0c8",
   "metadata": {},
   "source": [
    "#### Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94b4da6-e75d-4e86-b405-23db6f0f184f",
   "metadata": {},
   "source": [
    "Bagging, which stands for Bootstrap Aggregating, is an ensemble machine learning technique used to improve the accuracy and robustness of predictive models. Bagging works by training multiple instances of the same base model on different subsets of the training data and then aggregating their predictions. The primary goal of bagging is to reduce variance, enhance model stability, and minimize overfitting.\n",
    "\n",
    "Here's how bagging works:\n",
    "\n",
    "1. **Bootstrap Sampling:** Bagging starts by creating multiple random subsets of the training data, each of the same size as the original dataset. This sampling is performed with replacement, meaning that the same data points can appear in multiple subsets, while others may not appear at all. This process is known as \"bootstrap sampling.\"\n",
    "\n",
    "2. **Parallel Model Training:** For each bootstrap sample, a separate instance of the base model is trained on that sample. This means that you have multiple models trained on different variations of the training data.\n",
    "\n",
    "3. **Prediction Aggregation:** When making predictions, bagging typically uses one of the following aggregation methods:\n",
    "   - **Classification (Voting):** In classification tasks, each base model makes predictions, and the final prediction is determined by majority voting. The class that receives the most votes becomes the predicted class.\n",
    "   - **Regression (Averaging):** In regression tasks, each base model predicts a numerical value, and the final prediction is obtained by averaging the predictions from all base models.\n",
    "\n",
    "Key characteristics and benefits of bagging:\n",
    "\n",
    "- **Variance Reduction:** Bagging reduces the variance of the predictions by averaging or voting across multiple models. This reduction in variance helps to create a more stable and robust model.\n",
    "\n",
    "- **Improved Generalization:** By training on multiple subsets of the data, bagging models tend to generalize better to unseen data, reducing overfitting.\n",
    "\n",
    "- **Model Diversity:** Bagging creates diverse models by exposing them to different data subsets. Diversity among the models helps capture different aspects of the underlying patterns in the data.\n",
    "\n",
    "- **Parallelization:** The training of base models in bagging can be parallelized, making it suitable for distributed computing and large datasets.\n",
    "\n",
    "- **Applicability:** Bagging can be applied to a wide range of base models, including decision trees, neural networks, support vector machines, and more.\n",
    "\n",
    "One of the most well-known bagging algorithms is the Random Forest, which uses bagging with decision trees as base models. Random Forest has been highly successful in various machine learning applications due to its ability to handle complex datasets and reduce overfitting.\n",
    "\n",
    "Overall, bagging is a powerful ensemble technique that helps improve the performance of predictive models and is commonly used in machine learning for classification and regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4710e4-494a-41e6-bbc1-497e1e003a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fef41b-b396-408f-9931-f8ecdecc23f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f062b233-9799-4523-9874-fdb4527583d0",
   "metadata": {},
   "source": [
    "#### Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306958c6-72ae-4815-9b12-5cbdc9d4877a",
   "metadata": {},
   "source": [
    "Boosting is an ensemble machine learning technique used to enhance the performance of predictive models by combining multiple weak learners (models that perform slightly better than random chance) into a strong learner. The fundamental idea behind boosting is to sequentially train a series of models, with each new model focusing on the instances that previous models struggled with. This process allows boosting to correct errors made by earlier models and improve overall predictive accuracy.\n",
    "\n",
    "Here's how boosting typically works:\n",
    "\n",
    "1. **Sequential Model Training:** Boosting starts by training a base model (often referred to as a \"weak learner\") on the original training dataset. This initial model does not need to be very accurate; it can be as simple as a decision stump (a decision tree with a single split).\n",
    "\n",
    "2. **Weighted Training Data:** After the first model is trained, boosting assigns weights to the training instances based on their accuracy. Misclassified instances are assigned higher weights, making them more likely to be selected during the next round of training.\n",
    "\n",
    "3. **Sequential Model Building:** Boosting continues by training additional base models sequentially. Each subsequent model is designed to correct the mistakes made by the previous models. The training data's weights are adjusted to focus on the instances that the current model finds challenging.\n",
    "\n",
    "4. **Weighted Voting:** When making predictions, boosting combines the predictions of all base models, giving more weight to models that perform better on the training data. The final prediction is often determined by weighted voting, where each model's vote is weighted by its performance.\n",
    "\n",
    "Key characteristics and benefits of boosting:\n",
    "\n",
    "- **Sequential Correction:** Boosting focuses on correcting errors made by earlier models, making it particularly effective in cases where individual models may perform poorly.\n",
    "\n",
    "- **Model Diversity:** Each base model in boosting is trained to address specific challenges in the data, leading to diversity among models. This diversity helps capture different aspects of the underlying patterns.\n",
    "\n",
    "- **Iterative Learning:** Boosting is an iterative process that gradually improves model performance over multiple rounds of training.\n",
    "\n",
    "- **Adaptive Learning:** The training data's weights are adaptively adjusted in boosting to emphasize the instances that are more challenging to classify.\n",
    "\n",
    "- **Strong Predictive Performance:** Boosting often results in strong predictive models that achieve high accuracy and generalization to unseen data.\n",
    "\n",
    "Common boosting algorithms include AdaBoost (Adaptive Boosting) and Gradient Boosting. AdaBoost assigns higher weights to misclassified instances, while Gradient Boosting optimizes the loss function by fitting subsequent models to the residuals of the previous ones. Variants of Gradient Boosting, such as XGBoost and LightGBM, have gained popularity for their high performance and efficiency.\n",
    "\n",
    "Boosting has been widely used in machine learning for classification and regression tasks, and it has achieved state-of-the-art results in various applications, including natural language processing, computer vision, and recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3eb57c-982e-44d5-8032-b3be49d847e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870d927-51ff-46fa-9d46-025b667cc21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8653408b-85fa-49ab-8aa1-5bf78cef5dce",
   "metadata": {},
   "source": [
    "#### Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7472b-a099-413f-a76f-6964aab022cc",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several benefits in the field of machine learning and predictive modeling:\n",
    "\n",
    "1. **Improved Predictive Accuracy:** Ensemble methods often lead to better predictive accuracy compared to individual models. By combining the predictions of multiple models, ensembles reduce errors and produce more robust and reliable predictions.\n",
    "\n",
    "2. **Reduced Overfitting:** Ensembles, especially bagging techniques like Random Forest, help reduce overfitting. Overfitting occurs when a model learns to fit the noise in the training data, leading to poor generalization to unseen data. Ensembles combine multiple models, reducing the risk of overfitting.\n",
    "\n",
    "3. **Enhanced Robustness:** Ensembles are more robust to noise and outliers in the data. Because they rely on the consensus of multiple models, they are less likely to be influenced by individual model errors or data anomalies.\n",
    "\n",
    "4. **Improved Generalization:** Ensembles often generalize well to unseen data. By combining diverse models, they capture different aspects of the underlying patterns in the data, making them effective at handling variations and complexities.\n",
    "\n",
    "5. **Model Diversity:** Ensemble methods create diverse models by exposing them to different data subsets or using different learning algorithms. Diversity among the models helps capture different aspects of the data distribution and underlying patterns.\n",
    "\n",
    "6. **Versatility:** Ensemble techniques can be applied to a wide range of machine learning algorithms as base models. Whether you're using decision trees, neural networks, support vector machines, or other models, you can often benefit from ensembling.\n",
    "\n",
    "7. **Model Stability:** Ensembles can make models more stable over time. When trained with different subsets of data or different initializations, ensembles tend to produce consistent and reliable predictions.\n",
    "\n",
    "8. **Parallelization:** The training of base models in ensemble techniques can be parallelized, making them suitable for distributed computing and large datasets.\n",
    "\n",
    "9. **State-of-the-Art Performance:** In many machine learning competitions and real-world applications, ensemble methods have consistently achieved state-of-the-art performance, demonstrating their effectiveness in improving model quality.\n",
    "\n",
    "10. **Interpretability:** Some ensemble methods, such as Random Forest, provide feature importance scores that can be used to interpret the relative importance of input features in making predictions.\n",
    "\n",
    "11. **Reduced Bias:** Ensembles can reduce bias in predictions by incorporating multiple perspectives on the data. This can lead to more accurate and fairer predictions in certain applications.\n",
    "\n",
    "While ensemble techniques offer numerous advantages, they may come with increased computational costs and complexity compared to single models. Additionally, the choice of the appropriate ensemble method and hyperparameter tuning may require careful consideration. Nonetheless, ensembles have become a valuable tool in the machine learning practitioner's toolkit, contributing to improved model performance across various domains and applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6e146f-4388-4a5c-ac5a-bd46c82f0488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd6174f-3ad9-4bde-b14f-aa5d68bc7ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa107b3-3ad6-42ef-93a7-5aa9b86ef016",
   "metadata": {},
   "source": [
    "#### Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093d09e-0765-44fe-a017-2d98ac8ea3db",
   "metadata": {},
   "source": [
    "Ensemble techniques are not always better than individual models, and their effectiveness depends on various factors, including the specific problem, the quality of the base models, and how the ensemble is constructed. Here are some considerations:\n",
    "\n",
    "1. **Quality of Base Models:** Ensembles work best when the base models (individual models) are diverse and perform better than random chance. If the base models are weak or highly correlated (lack diversity), ensembles may not lead to significant improvements.\n",
    "\n",
    "2. **Ensemble Method:** Different ensemble methods have different strengths and weaknesses. For example, bagging methods like Random Forest are effective at reducing overfitting and improving robustness, while boosting methods like AdaBoost and Gradient Boosting focus on correcting errors made by weak learners. The choice of the ensemble method should align with the problem and data characteristics.\n",
    "\n",
    "3. **Data Quality and Quantity:** Ensembles tend to perform better when trained on larger and cleaner datasets. If the dataset is small, noisy, or contains outliers, the benefits of ensembling may be limited.\n",
    "\n",
    "4. **Computational Resources:** Ensembling can be computationally expensive, especially when dealing with a large number of base models. In cases where computational resources are limited, it may not be feasible to train and maintain a complex ensemble.\n",
    "\n",
    "5. **Overfitting:** While ensembles can reduce overfitting in general, there is a risk of overfitting to the training data if the ensemble becomes too complex. Careful hyperparameter tuning and monitoring of model performance on validation data are essential to avoid overfitting.\n",
    "\n",
    "6. **Interpretability:** Ensembles can be more challenging to interpret compared to individual models. In applications where model interpretability is crucial, using simpler models might be preferred.\n",
    "\n",
    "7. **Diminishing Returns:** As more base models are added to an ensemble, the marginal improvement in performance tends to diminish. At some point, the additional complexity and computational cost may not justify the small gains in accuracy.\n",
    "\n",
    "8. **Ensemble Size:** The size of the ensemble (the number of base models) can impact its performance. Too few base models may not harness the full potential of ensembling, while too many may lead to diminishing returns and increased computational overhead.\n",
    "\n",
    "In practice, it's essential to experiment with ensembling and compare its performance to that of individual models on a validation dataset. The decision to use ensembles should be guided by empirical evidence and domain knowledge. In some cases, a well-tuned individual model may perform just as well as or even better than an ensemble, while in others, ensembling can lead to significant improvements. Therefore, the choice of whether to use ensemble techniques should be problem-specific and based on thorough experimentation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3aa59f-3480-4f57-bef8-ded7472600e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ef81d-67d2-4580-812c-276df4cb8f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4c23aef-0cc7-4008-b50b-01e7e2710dc6",
   "metadata": {},
   "source": [
    "#### Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c679e-865a-4649-abda-39fe6b580d53",
   "metadata": {},
   "source": [
    "The confidence interval calculated using bootstrap is a statistical technique that allows you to estimate the uncertainty or variability in a sample statistic, such as the mean, median, or any other parameter of interest, by repeatedly resampling the observed data. The steps to calculate a bootstrap confidence interval are as follows:\n",
    "\n",
    "1. **Data Resampling:** Start with your original dataset, which contains 'n' observations. To create a bootstrap sample, randomly select 'n' observations from the dataset with replacement. This means that some data points may be selected multiple times, while others may not be selected at all. Each bootstrap sample is considered a new, resampled dataset.\n",
    "\n",
    "2. **Statistical Estimation:** Calculate the statistic of interest (e.g., mean, median, standard deviation, etc.) for each bootstrap sample. This gives you a collection of statistics, one for each resampled dataset.\n",
    "\n",
    "3. **Sampling Distribution:** The collection of statistics obtained from the bootstrap samples represents the sampling distribution of the statistic. This distribution provides information about how the statistic varies when different random samples are drawn from the population.\n",
    "\n",
    "4. **Confidence Interval Calculation:** To calculate the confidence interval, you need to determine the range of values that covers a specified proportion of the sampling distribution. The most common confidence levels are 95%, 90%, or 99%, but you can choose any level you prefer.\n",
    "\n",
    "   - For a two-tailed confidence interval (e.g., 95%), you typically use the 2.5th and 97.5th percentiles of the sampling distribution as the lower and upper bounds of the interval.\n",
    "\n",
    "   - For a one-tailed confidence interval (e.g., testing if a parameter is greater or less than a certain value), you use the appropriate percentile (e.g., 5th or 95th percentile) of the sampling distribution.\n",
    "\n",
    "The steps outlined above can be summarized as follows:\n",
    "\n",
    "1. Create many (often thousands) bootstrap samples by randomly drawing data points with replacement.\n",
    "\n",
    "2. Calculate the statistic of interest for each bootstrap sample.\n",
    "\n",
    "3. Determine the desired confidence level (e.g., 95%).\n",
    "\n",
    "4. Find the appropriate percentiles of the sampling distribution to define the lower and upper bounds of the confidence interval.\n",
    "\n",
    "Bootstrap resampling is a powerful technique because it allows you to estimate the uncertainty around a statistic without making strong parametric assumptions about the underlying data distribution. It is particularly useful when dealing with small sample sizes or non-normal data, where traditional parametric methods may not be as reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fcb9ce-1ff0-4795-9fd3-75ce9a5fe245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18dab1-45b4-4851-bc25-0b0ed6440288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cad8c690-2d13-4e5d-bf57-9072d8a661da",
   "metadata": {},
   "source": [
    "#### Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e3aa8-a612-47dc-9113-2f68fc225d65",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique in statistics that allows you to estimate the sampling distribution of a statistic or parameter by repeatedly drawing samples (with replacement) from the observed data. It's a powerful method for making inferences about a population when you have a limited sample. Here are the steps involved in the bootstrap process:\n",
    "\n",
    "1. **Original Data:** Start with your original dataset, which contains 'n' observations.\n",
    "\n",
    "2. **Sampling with Replacement:** The key idea of bootstrap is to create multiple resamples (bootstrap samples) by randomly selecting 'n' observations from the original dataset, with replacement. This means that each time you draw an observation, you record it and then put it back into the dataset so that it can be selected again. As a result, some data points may be selected multiple times, while others may not be selected at all.\n",
    "\n",
    "3. **Sample Size:** Each bootstrap sample also contains 'n' observations, just like the original dataset.\n",
    "\n",
    "4. **Statistic of Interest:** Calculate the statistic of interest (e.g., mean, median, variance, regression coefficient, etc.) on each bootstrap sample. This gives you a collection of statistics, one for each resampled dataset.\n",
    "\n",
    "5. **Sampling Distribution:** The collection of statistics obtained from the bootstrap samples represents the empirical sampling distribution of the statistic. This distribution describes how the statistic varies when different random samples are drawn from the population.\n",
    "\n",
    "6. **Estimate and Confidence Interval:** You can use the bootstrap sampling distribution to estimate the population parameter (e.g., the population mean) by taking the average (or other appropriate measure) of the bootstrap statistics. Additionally, you can calculate a confidence interval for the parameter by finding the appropriate percentiles of the bootstrap sampling distribution. Commonly used confidence levels include 95%, 90%, or 99%.\n",
    "\n",
    "The key advantage of the bootstrap method is that it provides a way to estimate the uncertainty or variability associated with a sample statistic without assuming a specific underlying probability distribution. It works well for a wide range of statistical problems, including hypothesis testing, parameter estimation, and constructing confidence intervals.\n",
    "\n",
    "The more bootstrap samples you generate, the more accurate and stable your estimates and confidence intervals become. Typically, thousands or even tens of thousands of bootstrap samples are used to obtain reliable results. Bootstrap is widely used in various fields of statistics and data analysis for its versatility and robustness, particularly when dealing with small or non-normally distributed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ddb00e-3810-41b7-90f8-8a70ff7ea281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8365fb5f-7461-493d-9ae4-c3beff54216f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59bc9181-ca1b-4b30-ad03-551f449283e3",
   "metadata": {},
   "source": [
    "#### Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d62960-9b32-4ee6-9c49-48e16b16d7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap 95% Confidence Interval for Mean Height: [5.8395 8.2   ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original sample data\n",
    "original_sample_mean = 15\n",
    "original_sample_std = 2\n",
    "sample_size = 50\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_sample_means = []\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Resample with replacement\n",
    "    bootstrap_sample = np.random.choice(original_sample_mean, size=sample_size, replace=True)\n",
    "    bootstrap_sample_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_sample_means.append(bootstrap_sample_mean)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_sample_means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap 95% Confidence Interval for Mean Height:\", confidence_interval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
