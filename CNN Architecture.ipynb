{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee7676d-9517-41e8-b521-fe6eb38b4df0",
   "metadata": {},
   "source": [
    "### TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b734aac-c604-4fa1-9591-d341769ed99b",
   "metadata": {},
   "source": [
    "#### 1. Describe the purpose and brnefits of pooling in CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f081b02-69f8-4653-95c3-74e3894785e3",
   "metadata": {},
   "source": [
    "Pooling, or max-pooling, is a crucial operation in Convolutional Neural Networks (CNNs) that plays a significant role in their architecture. Its primary purpose is to reduce the spatial dimensions of the input data while retaining its essential features. This process offers several benefits in the context of CNNs:\n",
    "\n",
    "1. **Dimensionality Reduction**: Pooling reduces the size of feature maps by taking the maximum (max-pooling) or average (average-pooling) value within a local region. This reduction in dimensionality helps in decreasing the computational complexity of the network and can prevent overfitting by reducing the number of parameters.\n",
    "\n",
    "2. **Translation Invariance**: Pooling helps CNNs achieve translation invariance, meaning that the network can recognize patterns in an image regardless of their exact position. By aggregating information in small local regions, the network becomes less sensitive to small shifts and translations in the input.\n",
    "\n",
    "3. **Information Retention**: Pooling retains the most important information while discarding less relevant details. Max-pooling, in particular, keeps the maximum value from each region, which tends to capture the most salient features, making it useful for feature selection.\n",
    "\n",
    "4. **Reduced Computational Load**: Smaller feature maps resulting from pooling operations require fewer computations, making the CNN more computationally efficient, especially in deep networks.\n",
    "\n",
    "5. **Parameter Sharing**: Pooling involves no learnable parameters, which reduces the risk of overfitting and simplifies the training process compared to convolutional layers that require weight learning.\n",
    "\n",
    "6. **Improved Generalization**: Pooling contributes to better generalization by promoting feature detection. It focuses on local features that are useful for classification and reduces the impact of noise or small variations.\n",
    "\n",
    "7. **Effective Feature Hierarchies**: In CNN architectures, pooling is often interleaved with convolutional layers. This hierarchy of convolutional layers followed by pooling layers allows the network to learn increasingly abstract and complex features as the depth of the network increases.\n",
    "\n",
    "8. **Faster Training**: Smaller feature maps require less memory and computational resources, which can speed up training, particularly in cases where memory is a constraint.\n",
    "\n",
    "However, it's essential to note that while pooling has many advantages, it can also result in some loss of spatial information. In some situations, this may not be desirable, and alternative techniques, such as dilated convolutions or global average pooling, are used to achieve different trade-offs between spatial resolution and feature reduction. The choice of pooling method and parameters depends on the specific task and the architecture of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c6c8e-2691-4de1-b0fa-a013bec261f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8164188-c39d-45f3-bb27-643e01f3c4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fc73d36-50b1-4e83-9fe7-3a749a459731",
   "metadata": {},
   "source": [
    "#### 2. Explain the difference between min pooling and max pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6576874-462c-4e7d-9a55-265ff371a7df",
   "metadata": {},
   "source": [
    "Min pooling and max pooling are two common types of pooling operations in Convolutional Neural Networks (CNNs). They are both used to reduce the spatial dimensions of feature maps while retaining essential features, but they differ in how they select and propagate information from the input.\n",
    "\n",
    "1. **Max Pooling**:\n",
    "\n",
    "   - **Operation**: Max pooling involves selecting the maximum value from a local region of the input. The most prominent feature within the region is retained, while the others are discarded.\n",
    "   - **Advantages**:\n",
    "     - Max pooling is particularly effective at capturing and preserving the most salient features in an image. It is robust to small variations and noise.\n",
    "     - It helps in achieving translation invariance by focusing on the most dominant features.\n",
    "   - **Common Use**: Max pooling is commonly used in CNN architectures, especially in image classification tasks.\n",
    "\n",
    "2. **Min Pooling**:\n",
    "\n",
    "   - **Operation**: Min pooling, on the other hand, selects the minimum value from a local region of the input. The smallest feature within the region is retained, while the others are discarded.\n",
    "   - **Advantages**:\n",
    "     - Min pooling can be useful in specific situations where the smallest features are more informative. For instance, it might be applied in certain edge detection scenarios.\n",
    "     - It can provide a complementary approach to feature selection, emphasizing different aspects of the data compared to max pooling.\n",
    "   - **Less Common**: Min pooling is less common in CNNs compared to max pooling, as max pooling is generally more effective for preserving dominant features.\n",
    "\n",
    "In summary, the main difference between min pooling and max pooling lies in the type of information they retain from the local regions of the input. Max pooling selects the most significant features by keeping the maximum value, while min pooling selects the smallest features by keeping the minimum value. The choice between the two depends on the specific task and the nature of the features you want to emphasize in the CNN's architecture. Max pooling is more commonly used in practice, but min pooling may find application in specialized scenarios where it is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84d8ce-e3ca-40a1-9d5b-9c03fcfe76ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5606103-f925-49c0-ba31-aa3427346c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9109effc-3f40-4d94-a96a-82c5fa970fca",
   "metadata": {},
   "source": [
    "#### 3. Discuss the concept of padding in CNN and its significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a626a-4977-46d2-9df8-6a862dd26b40",
   "metadata": {},
   "source": [
    "Padding in Convolutional Neural Networks (CNNs) is a technique used to control the spatial dimensions of the feature maps produced during the convolutional and pooling operations. It involves adding extra rows and columns of zeros (or other values) around the input data before applying convolutions or pooling. Padding is significant for several reasons:\n",
    "\n",
    "1. **Preserving Spatial Information**:\n",
    "   - Padding allows the network to preserve the spatial dimensions of the feature maps, ensuring that the output has the same spatial dimensions as the input. This can be important for retaining positional information and ensuring that the network captures features near the edges of the input.\n",
    "\n",
    "2. **Avoiding Edge Information Loss**:\n",
    "   - In convolutional operations, without padding, the size of the feature maps decreases as you move deeper into the network layers. This can result in the loss of information at the edges of the input, which may be essential for detecting features near the boundaries of objects in an image.\n",
    "\n",
    "3. **Controlling Output Size**:\n",
    "   - Padding enables control over the size of the output feature maps. By adjusting the amount of padding, you can increase or decrease the spatial resolution of the feature maps. This is particularly important when designing architectures and managing memory requirements.\n",
    "\n",
    "4. **Striding Compatibility**:\n",
    "   - Padding can be used to ensure that the convolutional or pooling operation aligns with the stride. When the stride is applied, it specifies how far the convolutional filter or pooling window moves horizontally and vertically. Padding can help in cases where the stride does not evenly divide the input size.\n",
    "\n",
    "5. **Mitigating Information Loss**:\n",
    "   - Pooling layers, which reduce the spatial dimensions, can lead to a loss of information. Padding can mitigate this loss by extending the input with zeros, making it less likely to lose critical features in the pooling process.\n",
    "\n",
    "6. **Enabling Different Network Architectures**:\n",
    "   - Padding flexibility allows for the design of various network architectures with different spatial resolutions. This flexibility is essential for adapting to different tasks and datasets.\n",
    "\n",
    "There are two common types of padding:\n",
    "\n",
    "1. **Valid (No Padding)**:\n",
    "   - In this mode, no padding is added to the input. As a result, the spatial dimensions of the feature maps decrease with each convolution or pooling operation, which is typical in many deep CNN architectures.\n",
    "\n",
    "2. **Same (Zero Padding)**:\n",
    "   - In this mode, padding is added such that the output feature maps have the same spatial dimensions as the input. Zero padding is often used because it doesn't introduce additional values and maintains the overall scale of the input data.\n",
    "\n",
    "The choice of padding and its amount (i.e., the number of rows and columns of padding added) should be carefully considered when designing a CNN architecture, taking into account the specific requirements of the task and the desired balance between spatial resolution and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d4508-1099-46c1-a4b9-d9af9acb4d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6843d040-2dfd-4d8b-b675-8840151fecc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0ceb48e-69d3-4918-93a8-6d211579d024",
   "metadata": {},
   "source": [
    "#### 4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a56a63-34e2-43fc-87bc-f7792d4006a7",
   "metadata": {},
   "source": [
    "Zero-padding and valid-padding are two common techniques used to control the size of the output feature maps in Convolutional Neural Networks (CNNs). They have contrasting effects on the output feature map size:\n",
    "\n",
    "1. **Zero-padding**:\n",
    "\n",
    "   - **Effect on Output Size**: Zero-padding increases the size of the output feature map compared to the input size.\n",
    "   - **Preservation of Spatial Dimensions**: With zero-padding, the spatial dimensions of the output feature map are typically preserved, meaning that the output has the same height and width as the input.\n",
    "   - **Usage**: Zero-padding is commonly used when you want to maintain the spatial information and ensure that the output feature map has the same spatial dimensions as the input. This is useful in cases where maintaining positional information and spatial resolution is crucial, such as in image segmentation tasks.\n",
    "   - **Padding Values**: Zero-padding adds rows and columns of zeros around the input data.\n",
    "\n",
    "2. **Valid-padding** (No Padding):\n",
    "\n",
    "   - **Effect on Output Size**: Valid-padding reduces the size of the output feature map compared to the input size.\n",
    "   - **Reduction in Spatial Dimensions**: With valid-padding (or no padding), the spatial dimensions of the output feature map decrease as compared to the input. Each convolution operation reduces the size of the feature map.\n",
    "   - **Usage**: Valid-padding is commonly used in deep CNN architectures where the objective is to progressively reduce the spatial dimensions to extract hierarchical and abstract features. This reduction in size can help control computational complexity and memory requirements.\n",
    "   - **Padding Values**: Valid-padding does not add any padding values; it directly applies the convolution operation, leading to a reduction in spatial dimensions.\n",
    "\n",
    "In summary, zero-padding and valid-padding are used to control the size of the output feature maps in CNNs, but they have opposite effects. Zero-padding increases the size of the output, maintaining spatial dimensions, while valid-padding reduces the size, progressively downsizing the feature maps as you go deeper into the network. The choice between these padding techniques depends on the specific requirements of the task, the desired balance between spatial resolution and computational efficiency, and the architectural design of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4f9af-64e9-41ad-b6c9-e33b0a9d503b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9bef4b-5f14-419c-9889-7f85e9bc0450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d359e2b5-f8ae-4909-a88b-c69d868868f3",
   "metadata": {},
   "source": [
    "### TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631cf751-b21e-4696-a23b-4f581bfa9cc3",
   "metadata": {},
   "source": [
    "#### 1. Provide a brief overview oj LeNet-5 architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec6c75-d638-4c5f-972d-9a020cc38f39",
   "metadata": {},
   "source": [
    "LeNet-5 is a classic and pioneering Convolutional Neural Network (CNN) architecture developed by Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner in the 1990s. It is one of the earliest CNNs and played a crucial role in advancing the field of deep learning and computer vision. LeNet-5 was primarily designed for handwritten digit recognition, specifically for recognizing digits in postal codes on mail envelopes. Here is a brief overview of the LeNet-5 architecture:\n",
    "\n",
    "1. **Input Layer**:\n",
    "   - LeNet-5 accepts grayscale images as input.\n",
    "   - The original architecture was designed for 32x32 pixel input images, which was a common size for hand-written digits at the time.\n",
    "\n",
    "2. **Convolutional Layers**:\n",
    "   - LeNet-5 consists of two convolutional layers followed by max-pooling layers.\n",
    "   - The first convolutional layer applies six 5x5 filters, producing six feature maps.\n",
    "   - The second convolutional layer applies sixteen 5x5 filters, producing sixteen feature maps.\n",
    "   - The convolutional layers use the tanh activation function.\n",
    "\n",
    "3. **Max-Pooling Layers**:\n",
    "   - After each convolutional layer, LeNet-5 employs max-pooling layers.\n",
    "   - The first max-pooling layer uses 2x2 windows with a stride of 2.\n",
    "   - The second max-pooling layer uses 2x2 windows with a stride of 2.\n",
    "\n",
    "4. **Fully Connected Layers**:\n",
    "   - LeNet-5 has three fully connected layers.\n",
    "   - The first fully connected layer has 120 neurons.\n",
    "   - The second fully connected layer has 84 neurons.\n",
    "   - The final output layer has 10 neurons, corresponding to the 10 possible digits (0-9).\n",
    "   - The fully connected layers use the tanh activation function.\n",
    "\n",
    "5. **Output Layer**:\n",
    "   - The output layer uses the softmax activation function to produce class probabilities.\n",
    "\n",
    "6. **Training**:\n",
    "   - LeNet-5 was trained using the gradient-based optimization technique known as stochastic gradient descent (SGD).\n",
    "\n",
    "LeNet-5's architecture reflects some of the fundamental concepts in CNN design, such as the use of convolutional and pooling layers for feature extraction and spatial hierarchy, followed by fully connected layers for classification. While it was originally designed for digit recognition, it laid the foundation for more advanced CNN architectures used in a wide range of computer vision tasks.\n",
    "\n",
    "It's important to note that over the years, CNN architectures have evolved significantly, with deeper networks, improved activation functions, and more sophisticated techniques, but LeNet-5 remains a historically important and influential model in the field of deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f6a73-2862-4d47-90f9-a876c7b6906d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fce6ad-ab37-490d-b196-52a42e7a5de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caf307af-05b4-4c88-bc05-d6b3241b852f",
   "metadata": {},
   "source": [
    "#### 2. Describe the key components of LeNet-5 and their respective purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e946e969-2dcd-42cf-8310-0efcd8e7068a",
   "metadata": {},
   "source": [
    "LeNet-5 is a pioneering Convolutional Neural Network (CNN) architecture developed by Yann LeCun and his colleagues in the 1990s. It was designed for handwritten digit recognition and played a crucial role in the development of deep learning and computer vision. Here are the key components of LeNet-5 and their respective purposes:\n",
    "\n",
    "1. **Input Layer**:\n",
    "   - **Purpose**: The input layer of LeNet-5 accepts grayscale images as input.\n",
    "   - **Details**: The original architecture was designed for 32x32 pixel input images. It serves as the entry point for the image data.\n",
    "\n",
    "2. **Convolutional Layers**:\n",
    "   - **Purpose**: Convolutional layers are responsible for feature extraction. They apply learnable filters to the input image to detect features.\n",
    "   - **Details**:\n",
    "     - The first convolutional layer applies six 5x5 filters, producing six feature maps. These filters capture low-level features like edges and simple textures.\n",
    "     - The second convolutional layer applies sixteen 5x5 filters, producing sixteen feature maps. These filters capture more complex and abstract features.\n",
    "\n",
    "3. **Max-Pooling Layers**:\n",
    "   - **Purpose**: Max-pooling layers downsample the feature maps, reducing their spatial dimensions while retaining the most salient information. This aids in translation invariance and reduces computational complexity.\n",
    "   - **Details**:\n",
    "     - The first max-pooling layer uses 2x2 windows with a stride of 2. It reduces the size of the feature maps by half.\n",
    "     - The second max-pooling layer uses 2x2 windows with a stride of 2, similar to the first layer.\n",
    "\n",
    "4. **Fully Connected Layers**:\n",
    "   - **Purpose**: Fully connected layers are responsible for classification and decision-making. They take the high-level features extracted by the convolutional and pooling layers and map them to class labels.\n",
    "   - **Details**:\n",
    "     - The first fully connected layer has 120 neurons. It further abstracts the features.\n",
    "     - The second fully connected layer has 84 neurons, capturing more complex patterns.\n",
    "     - The final output layer has 10 neurons, corresponding to the 10 possible digits (0-9). It uses the softmax activation function to produce class probabilities.\n",
    "\n",
    "5. **Activation Functions**:\n",
    "   - **Purpose**: Activation functions introduce non-linearity to the network, allowing it to model complex relationships in the data.\n",
    "   - **Details**: LeNet-5 uses the hyperbolic tangent (tanh) activation function in the convolutional and fully connected layers. The output layer uses softmax to produce class probabilities.\n",
    "\n",
    "6. **Training**:\n",
    "   - **Purpose**: Training is the process of updating the model's weights to minimize the prediction error. It involves an optimization algorithm like stochastic gradient descent (SGD).\n",
    "   - **Details**: LeNet-5 was trained using SGD and backpropagation to adjust the weights of the network's parameters.\n",
    "\n",
    "LeNet-5's architecture represents some of the foundational principles of CNN design, such as the use of convolutional and pooling layers for feature extraction, followed by fully connected layers for classification. It demonstrated the effectiveness of deep learning in computer vision tasks, laying the groundwork for the development of more advanced CNN architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9980e1-69c7-44ae-9d4f-7d8f8822282f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05771db4-772d-4c5b-b445-4cee1102443a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b621dccb-0d83-4c4a-9b23-908ba997e972",
   "metadata": {},
   "source": [
    "#### 3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1e3e8-d2a0-4aac-8164-d6fc3e104280",
   "metadata": {},
   "source": [
    "LeNet-5, one of the earliest Convolutional Neural Network (CNN) architectures, has been influential in the field of deep learning and computer vision. However, it has both advantages and limitations when used in the context of image classification tasks:\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "1. **Pioneering Architecture**: LeNet-5 was a pioneering architecture that demonstrated the effectiveness of CNNs for image classification. It laid the foundation for subsequent, more advanced CNN architectures.\n",
    "\n",
    "2. **Feature Extraction**: LeNet-5 effectively captures features through its convolutional layers, which are designed to recognize edges and textures. This makes it suitable for tasks where these lower-level features are essential.\n",
    "\n",
    "3. **Translation Invariance**: The use of max-pooling layers helps LeNet-5 achieve translation invariance. It can recognize features regardless of their exact position in the input image.\n",
    "\n",
    "4. **Simple and Elegant**: LeNet-5 is a relatively simple and easy-to-understand architecture. This simplicity can be advantageous for educational purposes and as a starting point for understanding CNNs.\n",
    "\n",
    "5. **Low Memory and Computational Requirements**: The architecture is computationally efficient compared to more modern deep CNNs. It requires less memory and computational power, which can be advantageous in resource-constrained environments.\n",
    "\n",
    "**Limitations**:\n",
    "\n",
    "1. **Limited Depth**: LeNet-5 is relatively shallow compared to modern CNNs. With only two convolutional layers and a simple architecture, it may struggle with more complex and deep feature hierarchies needed for intricate image classification tasks.\n",
    "\n",
    "2. **Small Input Size**: The original LeNet-5 was designed for small 32x32 pixel input images. This restricts its applicability to modern high-resolution images, where larger networks are often necessary.\n",
    "\n",
    "3. **Activation Functions**: LeNet-5 uses the hyperbolic tangent (tanh) activation function, which has vanishing gradient problems. Modern architectures often use more advanced activation functions like ReLU to mitigate this issue.\n",
    "\n",
    "4. **Lack of Regularization**: LeNet-5 does not incorporate modern regularization techniques such as dropout or batch normalization, which can help prevent overfitting.\n",
    "\n",
    "5. **Limited Applicability**: While it's suitable for digit recognition, LeNet-5 may not perform as well on more complex and diverse image classification tasks such as object detection or scene recognition.\n",
    "\n",
    "6. **Training Data**: LeNet-5 was originally designed for handwritten digit recognition, and its performance can vary when applied to different types of data. Modern CNN architectures are typically more versatile and transferable across various tasks.\n",
    "\n",
    "In summary, LeNet-5 was groundbreaking for its time and served as a critical step in the evolution of deep learning for image classification. However, its limitations, especially its shallowness, limited input size, and outdated architectural choices, make it less suitable for contemporary image classification tasks, which often demand deeper, more complex, and adaptable CNN architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591f9be-f573-4645-ab4a-7c6339b61a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba46afd-b1ac-4921-ba49-4423b937f91e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82d56a8c-f8a4-4548-bf4a-5aaa79333f39",
   "metadata": {},
   "source": [
    "#### 4. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79abbd30-1ee4-4f14-b332-b83049b4a189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 14:55:18.637152: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-18 14:55:18.703397: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-18 14:55:18.703470: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-18 14:55:18.703525: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-18 14:55:18.716254: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-18 14:55:18.717582: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-18 14:55:20.594030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 3s 0us/step\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 15s 7ms/step - loss: 0.1710 - accuracy: 0.9492 - val_loss: 0.0861 - val_accuracy: 0.9729\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0646 - accuracy: 0.9802 - val_loss: 0.0499 - val_accuracy: 0.9854\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0449 - accuracy: 0.9859 - val_loss: 0.0499 - val_accuracy: 0.9844\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0337 - accuracy: 0.9891 - val_loss: 0.0434 - val_accuracy: 0.9864\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0275 - accuracy: 0.9914 - val_loss: 0.0623 - val_accuracy: 0.9807\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0209 - accuracy: 0.9932 - val_loss: 0.0416 - val_accuracy: 0.9882\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.0426 - val_accuracy: 0.9878\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0135 - accuracy: 0.9957 - val_loss: 0.0537 - val_accuracy: 0.9850\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.0606 - val_accuracy: 0.9853\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0131 - accuracy: 0.9955 - val_loss: 0.0454 - val_accuracy: 0.9875\n",
      "313/313 - 1s - loss: 0.0454 - accuracy: 0.9875 - 987ms/epoch - 3ms/step\n",
      "Test accuracy: 0.987500011920929\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(6, (5, 5), activation='tanh', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (5, 5), activation='tanh'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(120, activation='tanh'))\n",
    "model.add(layers.Dense(84, activation='tanh'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf9501-47b3-47d0-ba57-16eb01b239f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca04ab-247e-49f7-a1d2-c2ac8bc8e62a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "327d9a8a-f612-4981-b9c6-13d709943483",
   "metadata": {},
   "source": [
    "### TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca85803-a826-46d0-961e-b0633b750c36",
   "metadata": {},
   "source": [
    "#### 1. Present an overview of the AlexNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b189b77c-7ead-4efe-9d01-85596b6fec10",
   "metadata": {},
   "source": [
    "AlexNet is a groundbreaking convolutional neural network (CNN) architecture that played a pivotal role in advancing the field of deep learning, particularly in image recognition. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, it won the ImageNet Large Scale Visual Recognition Challenge in 2012, significantly reducing the error rate and demonstrating the power of deep neural networks. Here's an overview of the AlexNet architecture:\n",
    "\n",
    "1. **Input Layer**:\n",
    "   - AlexNet takes color images as input. The original architecture was designed for images of size 224x224 pixels with three color channels (RGB).\n",
    "\n",
    "2. **Convolutional Layers**:\n",
    "   - AlexNet consists of five convolutional layers.\n",
    "   - The first convolutional layer applies 96 filters of size 11x11 with a stride of 4. It captures low-level features like edges and simple textures.\n",
    "   - The subsequent convolutional layers apply a variety of filter sizes, including 5x5 and 3x3.\n",
    "   - These convolutional layers use the ReLU (Rectified Linear Unit) activation function, which introduced non-linearity into the network.\n",
    "\n",
    "3. **Max-Pooling Layers**:\n",
    "   - After some of the convolutional layers, max-pooling layers are applied to downsample the feature maps and reduce their spatial dimensions.\n",
    "\n",
    "4. **Normalization Layers**:\n",
    "   - Local Response Normalization (LRN) layers are used to normalize the responses of neurons within the same feature map. This helps enhance contrast and response to small features.\n",
    "\n",
    "5. **Fully Connected Layers**:\n",
    "   - AlexNet has three fully connected layers.\n",
    "   - The first fully connected layer has 4096 neurons.\n",
    "   - The second fully connected layer also has 4096 neurons.\n",
    "   - The final output layer has 1000 neurons, corresponding to the 1000 classes in the ImageNet dataset.\n",
    "\n",
    "6. **Dropout**:\n",
    "   - Dropout is applied before the fully connected layers to prevent overfitting. It randomly drops out a portion of neurons during training.\n",
    "\n",
    "7. **Output Layer**:\n",
    "   - The output layer uses the softmax activation function to produce class probabilities for image classification.\n",
    "\n",
    "8. **Training**:\n",
    "   - AlexNet was trained using stochastic gradient descent (SGD) with a relatively large learning rate. Data augmentation techniques were also employed during training to improve generalization.\n",
    "\n",
    "9. **Overlapping Pooling**:\n",
    "   - In some cases, AlexNet uses overlapping pooling, where the pooling regions overlap, instead of non-overlapping max-pooling.\n",
    "\n",
    "10. **Parallel Processing**:\n",
    "    - AlexNet was designed to take advantage of parallel processing by using two GPUs, which significantly accelerated training.\n",
    "\n",
    "AlexNet was a significant leap forward in the field of deep learning. It demonstrated that deep neural networks, with the right architecture and training techniques, could achieve remarkable performance in image classification tasks. While it's been surpassed by more recent architectures in terms of accuracy and efficiency, it remains a landmark model that paved the way for the development of deeper and more complex CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb15ad2-27fa-46ac-8825-e9d5950e64be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd9fded-731a-483f-9c96-9e8b73b0a203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7122f962-3f75-422e-95e7-0320304f4238",
   "metadata": {},
   "source": [
    "#### 2. Explain the architectural innovations introduced in AlexNet that contributed to its brceakthrough performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9978ab-f689-41b3-993a-2287b3e9f971",
   "metadata": {},
   "source": [
    "AlexNet, introduced by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, was a groundbreaking Convolutional Neural Network (CNN) architecture that significantly contributed to the resurgence of deep learning in computer vision. Several architectural innovations in AlexNet played a crucial role in its breakthrough performance:\n",
    "\n",
    "1. **Deep Architecture**:\n",
    "   - AlexNet was one of the first CNN architectures to have a deep structure. It consisted of eight layers: five convolutional layers and three fully connected layers. This depth allowed the network to capture hierarchical features and abstract representations of the input data.\n",
    "\n",
    "2. **Rectified Linear Units (ReLU)**:\n",
    "   - AlexNet used Rectified Linear Units (ReLU) as the activation function, which replaced traditional sigmoid or hyperbolic tangent functions. ReLU is computationally efficient and mitigates the vanishing gradient problem, allowing for faster training and improved convergence.\n",
    "\n",
    "3. **Local Response Normalization (LRN)**:\n",
    "   - AlexNet introduced a local response normalization (LRN) layer after some of the convolutional layers. This layer helps neurons to respond to a broader range of inputs and enhances generalization.\n",
    "\n",
    "4. **Overlapping Max-Pooling**:\n",
    "   - In the pooling layers, AlexNet used overlapping max-pooling, which means the pooling windows had an overlap. This allowed the network to capture spatial hierarchies and retain more spatial information.\n",
    "\n",
    "5. **Data Augmentation**:\n",
    "   - Data augmentation techniques, such as random cropping and horizontal flipping, were used during training. These techniques increased the diversity of the training data and improved the model's ability to handle variations in the input.\n",
    "\n",
    "6. **Dropout Regularization**:\n",
    "   - AlexNet employed dropout in the fully connected layers. Dropout randomly deactivates a fraction of neurons during training, which acts as a form of regularization, preventing overfitting and improving generalization.\n",
    "\n",
    "7. **Large Training Dataset**:\n",
    "   - AlexNet was trained on a massive dataset, ImageNet, which contained over a million images with 1,000 object categories. The large and diverse training dataset played a significant role in the model's ability to generalize well to a wide range of images.\n",
    "\n",
    "8. **Parallelization**:\n",
    "   - AlexNet was one of the first models to effectively utilize GPU hardware for deep learning. It employed a dual-GPU setup to distribute the computational load, reducing training time.\n",
    "\n",
    "9. **Softmax Cross-Entropy Loss**:\n",
    "   - AlexNet used the softmax activation function followed by a cross-entropy loss function for classification tasks. This combination provided more discriminative class probabilities and improved the model's classification performance.\n",
    "\n",
    "The combination of these architectural innovations allowed AlexNet to achieve state-of-the-art performance in the ImageNet Large Scale Visual Recognition Challenge in 2012, significantly reducing the error rate compared to previous methods. This success paved the way for the development of deeper and more sophisticated CNN architectures, such as VGG, GoogLeNet, and ResNet, and played a vital role in the resurgence of deep learning and its widespread adoption in computer vision and beyond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38573d5e-8e54-4b5b-be36-417dd8d95d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdebc08-58e3-495b-9042-f5ebae0b4cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d3a5cdc-380e-4ec2-975a-d0ddadb9242a",
   "metadata": {},
   "source": [
    "#### 3. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d2bd9e-2154-4e13-ba8d-6eb05299cb6c",
   "metadata": {},
   "source": [
    "In AlexNet, convolutional layers, pooling layers, and fully connected layers play distinct and complementary roles in the architecture, contributing to its success in image classification tasks. Here's an overview of the roles of each layer type in AlexNet:\n",
    "\n",
    "1. **Convolutional Layers**:\n",
    "   - **Role**: Convolutional layers are responsible for feature extraction. They apply convolution operations to the input image, which involve learning and applying filters (kernels) to detect local features like edges, textures, and more complex patterns.\n",
    "   - **Details in AlexNet**:\n",
    "     - AlexNet consists of five convolutional layers, each followed by an activation function (Rectified Linear Unit - ReLU) and local response normalization (LRN) in some layers.\n",
    "     - These layers learn a hierarchy of features, starting with simpler features like edges and gradually moving to more complex features, which are crucial for image understanding.\n",
    "   - **Convolutional Filter Sizes**: In AlexNet, the convolutional filter sizes vary from 11x11 to 3x3, with varying depths (number of filters) in each layer.\n",
    "\n",
    "2. **Pooling Layers**:\n",
    "   - **Role**: Pooling layers, specifically max-pooling in AlexNet, serve the purpose of reducing the spatial dimensions of the feature maps. This downsampling operation helps in decreasing the computational load and enhancing the network's translation invariance.\n",
    "   - **Details in AlexNet**:\n",
    "     - AlexNet employs max-pooling layers after the convolutional layers, reducing the feature map size by selecting the maximum value from local regions.\n",
    "     - Overlapping max-pooling with a stride of 2 was used to capture spatial hierarchies.\n",
    "   - **Pooling Windows**: The pooling windows in AlexNet were typically 3x3 with overlapping regions.\n",
    "\n",
    "3. **Fully Connected Layers**:\n",
    "   - **Role**: Fully connected layers in AlexNet are responsible for classification and decision-making. They take the high-level features extracted by the convolutional and pooling layers and map them to class labels.\n",
    "   - **Details in AlexNet**:\n",
    "     - AlexNet has three fully connected layers. The first fully connected layer consists of 120 neurons, followed by a layer with 84 neurons, and the final output layer with 10 neurons (corresponding to the 10 possible object categories in the ImageNet dataset).\n",
    "     - These layers perform a series of linear transformations and nonlinear activations, mapping the features to class probabilities via the softmax activation function in the output layer.\n",
    "   - **High-Level Features**: The fully connected layers aggregate high-level and abstract representations of the input image, making them suitable for classification.\n",
    "\n",
    "In summary, convolutional layers in AlexNet capture hierarchical image features, including edges, textures, and more complex patterns. Pooling layers reduce the spatial dimensions of the feature maps and help the network maintain translation invariance. Fully connected layers make classification decisions based on the high-level features extracted by the previous layers. The combined use of these layer types enables AlexNet to effectively extract and learn discriminative features from images and classify them into different object categories, which was a key factor in its breakthrough performance in image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea82689-d9ad-43ba-9456-6a6492f58a22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3923413-cc3a-4b3e-a6e0-edd42994c728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0699e1ba-54ca-4bf1-b98e-c36a3abacefc",
   "metadata": {},
   "source": [
    "#### 4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08039a49-e36f-45d2-a38e-856d08eca1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.13.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2022.11.0)\n",
      "Collecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "Successfully installed filelock-3.13.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.52 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 torchvision-0.16.0 triton-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7bd308d-849d-4d53-ab11-7bf372093c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18d60cee-8df0-466b-9510-b54b9b12258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def get_train_valid_loader(data_dir,\n",
    "                           batch_size,\n",
    "                           augment,\n",
    "                           random_seed,\n",
    "                           valid_size=0.1,\n",
    "                           shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2023, 0.1994, 0.2010],\n",
    "    )\n",
    "\n",
    "    # define transforms\n",
    "    valid_transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "    ])\n",
    "    if augment:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((227,227)),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    # load the dataset\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=train_transform,\n",
    "    )\n",
    "\n",
    "    valid_dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=True,\n",
    "        download=True, transform=valid_transform,\n",
    "    )\n",
    "\n",
    "    num_train = len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    " \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    return (train_loader, valid_loader)\n",
    "\n",
    "\n",
    "def get_test_loader(data_dir,\n",
    "                    batch_size,\n",
    "                    shuffle=True):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "\n",
    "    # define transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((227,227)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.CIFAR10(\n",
    "        root=data_dir, train=False,\n",
    "        download=True, transform=transform,\n",
    "    )\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "# CIFAR10 dataset \n",
    "train_loader, valid_loader = get_train_valid_loader(data_dir = './data',                                      batch_size = 64,\n",
    "                       augment = False,                             \t\t     random_seed = 1)\n",
    "\n",
    "test_loader = get_test_loader(data_dir = './data',\n",
    "                              batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f00355a3-a650-4011-95d1-7e57b2a58f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5c5cd4e-2cea-4e53-a59c-a3911e818046",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "\n",
    "model = AlexNet(num_classes).to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad90e3-93f6-43e9-a9be-9bbb2d48f542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [704/704], Loss: 0.4558\n",
      "Accuracy of the network on the 5000 validation images: 60.92 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "            \n",
    "    # Validation\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
