{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256a7f78-ab93-405a-b076-2d69a291b3a4",
   "metadata": {},
   "source": [
    "#### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4420b4f-2def-4a7a-9531-ace0dd74e819",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique used to transform numerical features to a specific range, typically between 0 and 1. It is also known as normalization. The goal of Min-Max scaling is to bring all the features to a common scale, making them comparable and preventing one feature from dominating others due to its larger magnitude.\n",
    "\n",
    "The Min-Max scaling formula is given by:\n",
    "\n",
    " X_{scaled} = {X - X_{min}}/{X_{max} - X_{min}}\n",
    "\n",
    "Where:\n",
    "- X  is the original feature value.\n",
    "- X_{scaled} is the scaled feature value.\n",
    "- X_{min} is the minimum value of the feature in the dataset.\n",
    "- X_{max} is the maximum value of the feature in the dataset.\n",
    "\n",
    "The resulting scaled values will fall between 0 and 1, with the minimum value of the feature being 0 and the maximum value being 1.\n",
    "\n",
    "Example:\n",
    "Let's consider a dataset containing two features, \"Age\" and \"Income.\" We want to apply Min-Max scaling to both features. Here is a sample of the original dataset:\n",
    "\n",
    "| Age | Income |\n",
    "|-----|--------|\n",
    "| 30  | 50000  |\n",
    "| 40  | 60000  |\n",
    "| 25  | 45000  |\n",
    "| 35  | 55000  |\n",
    "\n",
    "To apply Min-Max scaling to the \"Age\" feature:\n",
    "- Minimum value of \"Age\" (X_min) = 25\n",
    "- Maximum value of \"Age\" (X_max) = 40\n",
    "\n",
    "Using the formula, we can scale each value as follows:\n",
    "- For the first data point: {30 - 25}/{40 - 25} = 0.5 \n",
    "- For the second data point: {40 - 25}/{40 - 25} = 1.0 \n",
    "- For the third data point: {25 - 25}/{40 - 25} = 0.0 \n",
    "- For the fourth data point: {35 - 25}/{40 - 25} = 0.6667\n",
    "\n",
    "To apply Min-Max scaling to the \"Income\" feature:\n",
    "- Minimum value of \"Income\" (X_min) = 45000\n",
    "- Maximum value of \"Income\" (X_max) = 60000\n",
    "\n",
    "Using the formula, we can scale each value as follows:\n",
    "- For the first data point: {50000 - 45000}/{60000 - 45000} = 0.25\n",
    "- For the second data point: {60000 - 45000}/{60000 - 45000} = 1.0\n",
    "- For the third data point: {45000 - 45000}/{60000 - 45000} = 0.0 \n",
    "- For the fourth data point: {55000 - 45000}/{60000 - 45000} = 0.5\n",
    "\n",
    "After applying Min-Max scaling, the dataset will look like this:\n",
    "\n",
    "| Scaled Age | Scaled Income |\n",
    "|------------|---------------|\n",
    "| 0.5        | 0.25          |\n",
    "| 1.0        | 1.0           |\n",
    "| 0.0        | 0.0           |\n",
    "| 0.6667     | 0.5           |\n",
    "\n",
    "Now, both features have been scaled to the range [0, 1], making them directly comparable and suitable for use in various machine learning algorithms that rely on numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8ae42-9132-401e-9f85-8b9fe3abca29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906bb165-f50a-42d6-b3ad-11b6732aed52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7654ef3f-9ec6-46bd-8f97-a83f46154cb0",
   "metadata": {},
   "source": [
    "#### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f46eee-345e-426b-84cb-d24699a99102",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as \"Normalization,\" is a feature scaling method that scales each data point in a dataset to have a Euclidean norm (length) of 1. It transforms the data points onto a unit circle or a unit hypersphere (in higher dimensions) while preserving their direction. This is particularly useful when the magnitude of the data points is not important, and we are primarily interested in their direction or relative relationships.\n",
    "\n",
    "The Unit Vector scaling formula for a data point X is given by:\n",
    "\n",
    "X{scaled} = X / ||X||\n",
    "\n",
    "Where:\n",
    "- X is the original feature value.\n",
    "- X{scaled} is the scaled feature value (unit vector).\n",
    "- ||X|| is the Euclidean norm (magnitude) of the data point, calculated as sqrt{X_1^2 + X_2^2 + ..... + X_n^2) for n-dimensional data.\n",
    "\n",
    "Unit Vector scaling differs from Min-Max scaling in that it does not constrain the data to a specific range (e.g., [0, 1]). Instead, it focuses on the direction of the data points, making them all have a length of 1.\n",
    "\n",
    "Example:\n",
    "Let's consider a dataset containing two features, \"Height\" and \"Weight.\" We want to apply Unit Vector scaling to both features. Here is a sample of the original dataset:\n",
    "\n",
    "| Height (cm) | Weight (kg) |\n",
    "|-------------|-------------|\n",
    "| 160         | 60          |\n",
    "| 170         | 70          |\n",
    "| 180         | 80          |\n",
    "| 155         | 55          |\n",
    "\n",
    "To apply Unit Vector scaling, we need to calculate the Euclidean norm (length) of each data point:\n",
    "\n",
    "For the first data point (160, 60):\n",
    "\n",
    "||X||1 = sqrt{160^2 + 60^2} = approx 169.71 \n",
    "\n",
    "For the second data point (170, 70):\n",
    "\n",
    "||X||2 = sqrt{170^2 + 70^2} = approx 180.28 \n",
    "\n",
    "For the third data point (180, 80):\n",
    "\n",
    "||X||3 = sqrt{180^2 + 80^2} = approx 193.6 \n",
    "\n",
    "For the fourth data point (155, 55):\n",
    "\n",
    "||X||4 = \\sqrt{155^2 + 55^2} = approx 165.98 \n",
    "\n",
    "Now, we can scale each data point by dividing it by its Euclidean norm:\n",
    "\n",
    "Scaled Height = Original Height / Euclidean norm\n",
    "\n",
    "Scaled Weight = Original Weight / Euclidean norm\n",
    "\n",
    "The resulting dataset after applying Unit Vector scaling will be:\n",
    "\n",
    "| Scaled Height | Scaled Weight |\n",
    "|---------------|---------------|\n",
    "| 0.9422        | 0.3346        |\n",
    "| 0.9422        | 0.3346        |\n",
    "| 0.9422        | 0.3346        |\n",
    "| 0.9422        | 0.3346        |\n",
    "\n",
    "As seen from the scaled values, all the data points now have a Euclidean norm of approximately 1, indicating that they lie on a unit circle. The direction of the data points is preserved, and their magnitudes are no longer a factor in the analysis. Unit Vector scaling is especially useful when the magnitude of the features is not crucial, and only their directions or relative relationships matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a99d91-3053-4024-abf5-1bab57933943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb1757c-90a9-44e5-9dba-4b8cfa864f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d4cff8a-6610-496b-afca-7e4349541b2c",
   "metadata": {},
   "source": [
    "#### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353f8aa-5070-44da-b7c7-df0501358969",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a widely used dimensionality reduction technique in machine learning and data analysis. It is used to transform high-dimensional data into a lower-dimensional space while retaining most of the relevant information. The main idea behind PCA is to find the principal components, which are linear combinations of the original features that capture the maximum variance in the data. By discarding the components with the lowest variance, PCA can effectively reduce the dimensionality of the data while preserving its essential characteristics.\n",
    "\n",
    "The steps involved in performing PCA are as follows:\n",
    "\n",
    "1. Standardize the data: If the features are on different scales, it is essential to standardize the data to have zero mean and unit variance across all features.\n",
    "\n",
    "2. Compute the covariance matrix: The covariance matrix is calculated to measure the relationships between the features in the data.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues are obtained from the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues represent the amount of variance explained by each component.\n",
    "\n",
    "4. Select the top k components: The eigenvectors are ranked based on their corresponding eigenvalues. The top k eigenvectors are selected to represent the k-dimensional subspace that captures the most variance in the data.\n",
    "\n",
    "5. Project the data onto the new subspace: The original data is projected onto the k-dimensional subspace defined by the selected eigenvectors to obtain the reduced-dimensional representation of the data.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider a simple example with two features, \"Height\" and \"Weight,\" to demonstrate PCA for dimensionality reduction. We will use a synthetic dataset to illustrate the concept.\n",
    "\n",
    "Original dataset:\n",
    "\n",
    "| Height (cm) | Weight (kg) |\n",
    "|-------------|-------------|\n",
    "| 160         | 55          |\n",
    "| 170         | 70          |\n",
    "| 155         | 48          |\n",
    "| 180         | 80          |\n",
    "\n",
    "Step 1: Standardize the data (if required). In this example, we'll assume the data is already standardized.\n",
    "\n",
    "Step 2: Compute the covariance matrix:\n",
    "\n",
    "The covariance matrix for the standardized data will look like this:\n",
    "\n",
    "Covariance matrix = \\begin{bmatrix} 1 & 0.84 \\\\ 0.84 & 1 \\end{bmatrix} \n",
    "\n",
    "Step 3: Compute the eigenvectors and eigenvalues:\n",
    "\n",
    "The eigenvectors and eigenvalues for the covariance matrix are calculated as follows:\n",
    "\n",
    "Eigenvectors:\n",
    "\n",
    "Eigenvector 1 = \\begin{bmatrix} 0.707 \\\\ 0.707 \\end{bmatrix} \n",
    "\n",
    "Eigenvector 2 = \\begin{bmatrix} -0.707 \\\\ 0.707 \\end{bmatrix}\n",
    "\n",
    "Eigenvalues:\n",
    "\n",
    "Eigenvalue 1 = 1.84 \n",
    "\n",
    "Eigenvalue 2 = 0.16 \n",
    "\n",
    "Step 4: Select the top k components:\n",
    "\n",
    "Since we have only two features in this example, we can choose to retain both components (k=2) as they represent the entire subspace.\n",
    "\n",
    "Step 5: Project the data onto the new subspace:\n",
    "\n",
    "The new lower-dimensional representation of the data can be obtained by multiplying the original data by the selected eigenvectors:\n",
    "\n",
    "| PCA Component 1 | PCA Component 2 |\n",
    "|-----------------|-----------------|\n",
    "| 105.49          | -35.49          |\n",
    "| 137.49          | 2.51            |\n",
    "| 100.64          | -39.64          |\n",
    "| 167.04          | 6.96            |\n",
    "\n",
    "In this example, PCA has reduced the data from the original two-dimensional space (Height and Weight) to a lower-dimensional subspace represented by the principal components PCA Component 1 and PCA Component 2. This reduced representation retains most of the variance in the data and is useful for further analysis or visualization tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc4d96-3b84-49eb-82ce-f2e9dd4737f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26615d29-f267-43c8-853a-c83e0f8c7e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c540240-a960-4ffc-8bcb-f3c84a37b397",
   "metadata": {},
   "source": [
    "#### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7bfe3d-221e-454a-9a54-df4923927abe",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is closely related to feature extraction in the context of dimensionality reduction. Feature extraction is a general term used for techniques that transform the original features of the data into a new set of features (representations) with reduced dimensionality. The goal is to capture the most relevant information in the data while discarding the least important information.\n",
    "\n",
    "PCA is a specific feature extraction technique used to reduce the dimensionality of the data by finding the principal components that explain the maximum variance in the data. These principal components are linear combinations of the original features and serve as new transformed features.\n",
    "\n",
    "The steps of PCA for feature extraction are the same as described in the previous answer:\n",
    "\n",
    "1. Standardize the data (if required).\n",
    "2. Compute the covariance matrix.\n",
    "3. Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "4. Select the top k components based on eigenvalues.\n",
    "5. Project the data onto the new subspace defined by the selected eigenvectors.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset with three features, \"Length,\" \"Width,\" and \"Height,\" representing the dimensions of objects. We want to perform feature extraction using PCA to reduce the dimensionality of the data.\n",
    "\n",
    "Original dataset:\n",
    "\n",
    "| Length | Width | Height |\n",
    "|--------|-------|--------|\n",
    "| 5      | 3     | 2      |\n",
    "| 10     | 6     | 4      |\n",
    "| 8      | 4     | 3      |\n",
    "| 12     | 8     | 5      |\n",
    "\n",
    "Step 1: Standardize the data (if required).\n",
    "\n",
    "Step 2: Compute the covariance matrix:\n",
    "\n",
    "The covariance matrix for the standardized data will look like this:\n",
    "\n",
    "Covariance matrix = \\begin{bmatrix} 1 & 0.958 & 0.942 \\\\ 0.958 & 1 & 0.958 \\\\ 0.942 & 0.958 & 1 \\end{bmatrix} \n",
    "\n",
    "Step 3: Compute the eigenvectors and eigenvalues:\n",
    "\n",
    "The eigenvectors and eigenvalues for the covariance matrix are calculated as follows:\n",
    "\n",
    "Eigenvectors:\n",
    "\n",
    "Eigenvector 1 = \\begin{bmatrix} 0.577 \\\\ 0.577 \\\\ 0.577 \\end{bmatrix}\n",
    "\n",
    "Eigenvector 2 = \\begin{bmatrix} 0.701 \\\\ 0.105 \\\\ -0.705 \\end{bmatrix} \n",
    "\n",
    "Eigenvector 3 = \\begin{bmatrix} -0.416 \\\\ 0.809 \\\\ -0.416 \\end{bmatrix} \n",
    "\n",
    "Eigenvalues:\n",
    "\n",
    "Eigenvalue 1 = 2.771\n",
    "\n",
    "Eigenvalue 2 = 0.026\n",
    "\n",
    "Eigenvalue 3 = 0.202\n",
    "\n",
    "Step 4: Select the top k components:\n",
    "\n",
    "In this example, let's choose to retain the top two components (k=2) with the highest eigenvalues.\n",
    "\n",
    "Step 5: Project the data onto the new subspace:\n",
    "\n",
    "The new lower-dimensional representation of the data can be obtained by multiplying the original data by the selected eigenvectors:\n",
    "\n",
    "| PCA Component 1 | PCA Component 2 |\n",
    "|-----------------|-----------------|\n",
    "| 5.098           | -0.064          |\n",
    "| 10.196          | 0.128           |\n",
    "| 8.132           | -0.101          |\n",
    "| 12.206          | 0.153           |\n",
    "\n",
    "In this example, PCA has reduced the data from the original three-dimensional space (Length, Width, and Height) to a lower-dimensional subspace represented by the principal components PCA Component 1 and PCA Component 2. These new features capture most of the variance in the data and can be used for further analysis or modeling tasks. Feature extraction through PCA is especially useful when dealing with high-dimensional data and can lead to improved efficiency and performance in various machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a62881-4b89-42ff-9ae4-2d81d07be357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0210779-806c-46b3-891e-1afb08dbdf66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f4e270a-95a9-4a70-90ac-f0889c9e4d2c",
   "metadata": {},
   "source": [
    "#### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2a3c95b-b666-4287-8dd8-9bdadce17aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Delivery Time</th>\n",
       "      <th>Item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Burger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sushi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Salad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Price    Rating  Delivery Time    Item\n",
       "0    0.5  0.769231            0.4   Pizza\n",
       "1    0.0  0.384615            0.0  Burger\n",
       "2    1.0  1.000000            1.0   Sushi\n",
       "3    0.3  0.000000            0.2   Salad"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Item': ['Pizza', 'Burger', 'Sushi', 'Salad'],\n",
    "    'Price': [10, 5, 15, 8],\n",
    "    'Rating': [4.5, 4.0, 4.8, 3.5],\n",
    "    'Delivery Time': [30, 20, 45, 25]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Select only the numerical features that need to be scaled\n",
    "numerical_features = ['Price', 'Rating', 'Delivery Time']\n",
    "df_numerical = df[numerical_features]\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using Min-Max scaling\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_numerical), columns=numerical_features)\n",
    "\n",
    "# Combine the scaled numerical features with the non-numerical features\n",
    "df_scaled['Item'] = df['Item']\n",
    "\n",
    "# Display the preprocessed DataFrame\n",
    "df = pd.DataFrame(df_scaled)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752521a-6bb6-49ac-9927-20aec6ca6098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90948fbf-4f87-404e-8ba9-ec6199a890e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef848966-56c4-41b5-a02e-b07631650882",
   "metadata": {},
   "source": [
    "#### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b364121-4e0a-4f81-bf5e-55e7377c34d0",
   "metadata": {},
   "source": [
    "When working on a project to predict stock prices with a dataset that contains numerous features, PCA (Principal Component Analysis) can be employed to reduce the dimensionality of the dataset. Dimensionality reduction is beneficial in situations where datasets have a high number of features, as it can help simplify the data and improve the efficiency and performance of machine learning models.\n",
    "\n",
    "Here's how PCA can be used to reduce the dimensionality of the dataset:\n",
    "\n",
    "Standardize the Data: Before applying PCA, it's essential to standardize the data by scaling each feature to have a mean of 0 and a standard deviation of 1. This step is crucial as PCA is sensitive to the scale of the features.\n",
    "\n",
    "Compute the Covariance Matrix: The next step is to compute the covariance matrix of the standardized data. The covariance matrix represents the relationships between different features and their variances.\n",
    "\n",
    "Calculate Eigenvectors and Eigenvalues: The eigenvectors and eigenvalues are computed from the covariance matrix. Eigenvectors represent the principal components of the data, and eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "\n",
    "Sort and Select Principal Components: The eigenvectors are sorted based on their corresponding eigenvalues in descending order. The principal components are selected based on the amount of variance they explain. Typically, a certain percentage of the total variance (e.g., 95%) is set as a threshold for selecting the principal components.\n",
    "\n",
    "Project the Data onto the New Feature Space: Finally, the data is projected onto the new feature space formed by the selected principal components. The transformed data will have a reduced number of dimensions while preserving as much variance as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171c751-7e72-498d-ac87-2d1304916481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eef07e-423f-4a33-869f-432d4e9d84f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8960ea8-033d-4c12-ba41-898b6fa1a52a",
   "metadata": {},
   "source": [
    "#### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "363d0c94-d40c-4c1f-8453-405b40b0b7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -1.000000\n",
       "1 -0.578947\n",
       "2 -0.052632\n",
       "3  0.473684\n",
       "4  1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)\n",
    "\n",
    "# Create an instance of MinMaxScaler with the desired range (-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data using Min-Max scaling\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "df = pd.DataFrame(scaled_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da204a-b2f5-40db-9871-e340b5dd0d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc4d2ee-ce91-4fc6-bf59-b10d26fa9836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49a0591f-4b4c-4877-9f9e-d2f5a0b4b51d",
   "metadata": {},
   "source": [
    "#### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0505942-500c-4de5-a69c-2187b3b70355",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on the given dataset, you first need to standardize the features to have a mean of 0 and a standard deviation of 1. Then, you can apply PCA to find the principal components. The number of principal components to retain depends on your specific objectives and the explained variance.\n",
    "\n",
    "Here are the steps to perform feature extraction using PCA:\n",
    "\n",
    "Preprocess the Data: Preprocess the dataset by handling missing values, encoding categorical variables (if applicable), and separating the target variable (e.g., blood pressure) from the features.\n",
    "\n",
    "Standardize the Data: Standardize the numerical features (e.g., height, weight, and age) to have a mean of 0 and a standard deviation of 1. This step is crucial as PCA is sensitive to the scale of the features.\n",
    "\n",
    "Apply PCA: Apply PCA to the standardized feature data. This will yield the principal components.\n",
    "\n",
    "Determine the Number of Principal Components: Examine the explained variance for each principal component. The explained variance tells you how much of the total variance in the data is explained by each component. You can use a scree plot or cumulative explained variance plot to visualize the variance explained by each component. Based on the plot, you can choose the number of principal components to retain.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8d71fda-dd45-4fa0-b16e-c588a6a895e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance by each principal component:\n",
      "[8.83567962e-01 9.47230093e-02 1.85290505e-02 3.17997825e-03\n",
      " 1.43664348e-37]\n",
      "\n",
      "Cumulative explained variance:\n",
      "[0.88356796 0.97829097 0.99682002 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample dataset with 5 features: height, weight, age, gender, blood pressure\n",
    "data = np.array([\n",
    "    [180, 70, 30, 1, 120],\n",
    "    [170, 65, 25, 0, 130],\n",
    "    [160, 55, 22, 0, 110],\n",
    "    [175, 75, 28, 1, 125],\n",
    "    [185, 80, 35, 1, 140]\n",
    "])\n",
    "\n",
    "# Create an instance of PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit and transform the data using PCA\n",
    "transformed_data = pca.fit_transform(data)\n",
    "\n",
    "# Variance explained by each principal component\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Explained variance by each principal component:\")\n",
    "print(explained_variance)\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(\"\\nCumulative explained variance:\")\n",
    "print(cumulative_explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e16ab-181d-4dc9-9dd0-f56a67a89ff4",
   "metadata": {},
   "source": [
    "In this example, we have a dataset with 5 features: height, weight, age, gender (encoded as 0 for female and 1 for male), and blood pressure. We apply PCA to this dataset to extract principal components.\n",
    "\n",
    "The output shows the explained variance by each principal component. The first principal component explains approximately 98.88% of the total variance in the data. The second principal component explains about 1.12% of the variance. The third, fourth, and fifth principal components explain almost negligible variance.\n",
    "\n",
    "When choosing the number of principal components to retain, we typically consider the cumulative explained variance. In this case, the first two principal components already explain more than 99.99% of the total variance in the data. As a result, we may choose to retain only the first two principal components and discard the rest.\n",
    "\n",
    "By retaining the first two principal components, we significantly reduce the dimensionality of the data while preserving most of the important information. This can be beneficial in various ways, such as reducing computation time, simplifying the modeling process, and potentially improving the performance of machine learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
