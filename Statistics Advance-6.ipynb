{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1041e0-7b01-4740-8217-5804adc98708",
   "metadata": {},
   "source": [
    "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9dacf-3c8f-4330-9c04-e994e9b73564",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical technique used to compare the means of two or more groups to determine if there are any statistically significant differences between them. To obtain valid and reliable results from ANOVA, several assumptions need to be met. Violations of these assumptions can impact the validity of the results and lead to erroneous conclusions.\n",
    "\n",
    "Assumptions of ANOVA:\n",
    "\n",
    "1. Independence: The observations within each group are independent of each other. This means that the data points in one group should not be influenced or dependent on the data points in another group.\n",
    "\n",
    "2. Normality: The data within each group follow a normal distribution. Normality assumes that the data points are symmetrically distributed around the mean, forming a bell-shaped curve.\n",
    "\n",
    "3. Homogeneity of Variance (Homoscedasticity): The variance of the data within each group is approximately equal. This assumption means that the variability in the data points is similar across all groups.\n",
    "\n",
    "4. Random Sampling: The data should be collected using a random sampling method to ensure that the results can be generalized to the larger population.\n",
    "\n",
    "Examples of Violations and their Impact:\n",
    "\n",
    "1. Violation of Independence: If the observations within groups are not independent, it can lead to biased estimates and incorrect conclusions. For example, if data points within groups are paired (e.g., repeated measures), such as comparing the performance of individuals before and after an intervention, the independence assumption is violated.\n",
    "\n",
    "2. Violation of Normality: If the data within groups are not normally distributed, the ANOVA results may be unreliable. Non-normality can lead to inaccurate p-values and incorrect conclusions about group differences. For instance, if the data are heavily skewed or have extreme outliers, the normality assumption is violated.\n",
    "\n",
    "3. Violation of Homogeneity of Variance: Unequal variances across groups can lead to biased and unreliable results. When variances are not homogenous, the F-statistic in ANOVA can be affected, and the power of the test (ability to detect true effects) may decrease. For example, if one group has much larger variability than others, the assumption of homogeneity of variance is violated.\n",
    "\n",
    "4. Non-Random Sampling: If the data collection process is not random, the results of ANOVA may not be generalizable to the larger population. Biased sampling methods can lead to overestimation or underestimation of group differences.\n",
    "\n",
    "When any of these assumptions are violated, it is essential to consider alternative statistical methods or transformations of the data to ensure the validity of the analysis. If the violations are severe, non-parametric tests (e.g., Kruskal-Wallis test) or other robust methods may be more appropriate for comparing group means. It is also crucial to interpret the results cautiously and consider the potential impact of the assumption violations on the conclusions drawn from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9ad2b-d7bd-42c8-89fd-11a2d93c2691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb978c-4363-4312-a48f-16df30fc2e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d11ccee7-2f48-4e33-8889-ccc86bc77d14",
   "metadata": {},
   "source": [
    "Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13acba68-3842-4f46-b2c0-939761f4329d",
   "metadata": {},
   "source": [
    "The three types of ANOVA (Analysis of Variance) are:\n",
    "\n",
    "1. One-Way ANOVA:\n",
    "\n",
    "One-Way ANOVA is used when there is a single categorical independent variable (also called a factor) with three or more levels, and the dependent variable is continuous. The purpose of One-Way ANOVA is to compare the means of the dependent variable across the different levels of the independent variable. It helps determine if there are any statistically significant differences between the means of the groups. This test is appropriate when you want to compare the effects of a single factor on the dependent variable.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose we want to compare the average test scores of students from three different schools (A, B, and C) to see if there are any significant differences in their academic performance.\n",
    "\n",
    "2. Two-Way ANOVA:\n",
    "\n",
    "Two-Way ANOVA is used when there are two categorical independent variables (factors) and one continuous dependent variable. The purpose of Two-Way ANOVA is to analyze the main effects of each independent variable and their interaction effect on the dependent variable. It allows us to determine if there are any significant differences based on the two factors and if there is an interaction between them.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a study where the effects of both gender (male and female) and study method (online and in-person) on exam scores are analyzed. Two-Way ANOVA would be used to examine the main effects of gender and study method and their interaction effect on exam scores.\n",
    "\n",
    "3. N-Way ANOVA (or Three-Way ANOVA and beyond):\n",
    "\n",
    "N-Way ANOVA is a generalization of Two-Way ANOVA to situations where there are more than two categorical independent variables (factors). It can handle scenarios with three or more independent variables and one continuous dependent variable. N-Way ANOVA is used to assess the main effects and interactions among multiple factors.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose we want to analyze the impact of factors like age group (young, middle-aged, and elderly), educational level (high school, college, graduate), and geographic location (urban, suburban, rural) on annual income. In this case, N-Way ANOVA would be appropriate to examine the main effects and interactions of all three factors on income.\n",
    "\n",
    "In summary, the choice of ANOVA depends on the number of categorical independent variables and the complexity of the study design. One-Way ANOVA is used when there is a single factor, Two-Way ANOVA when there are two factors, and N-Way ANOVA for scenarios with three or more factors. Each type of ANOVA allows researchers to explore and analyze different aspects of the relationship between categorical and continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceba798-f87f-46c2-a3be-5218cd56038d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7cc459-0a1b-4b89-af4b-bcdfb50f000d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26181f9e-0b43-4735-9e2a-209cf0d52ddb",
   "metadata": {},
   "source": [
    "Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b0346-fec7-4052-b089-e287a0debe9d",
   "metadata": {},
   "source": [
    "The partitioning of variance in ANOVA refers to the process of breaking down the total variance observed in the data into different components that can be attributed to specific sources or factors. In the context of ANOVA, the total variance in the dependent variable is divided into two main components:\n",
    "\n",
    "1. Between-Group Variance (or Treatment Variance):\n",
    "This component represents the variability in the dependent variable that can be attributed to the differences between the groups or levels of the independent variable (factor). It measures the effect of the independent variable on the dependent variable and is also known as the treatment effect. In other words, it shows how much of the variation in the dependent variable is explained by the grouping factor.\n",
    "\n",
    "2. Within-Group Variance (or Error Variance):\n",
    "This component represents the variability in the dependent variable that cannot be explained by the differences between the groups. It is the variability that remains after accounting for the effect of the independent variable. Within-group variance is also referred to as error variance because it includes random errors, individual differences, and other factors that are not part of the grouping variable.\n",
    "\n",
    "The total variance observed in the data is the sum of the between-group variance and the within-group variance.\n",
    "\n",
    "Why is it important to understand the partitioning of variance in ANOVA?\n",
    "\n",
    "1. Identifying Significant Effects: By partitioning the total variance into between-group and within-group components, ANOVA allows us to determine if the variation between groups is statistically significant. If the between-group variance is much larger than the within-group variance, it indicates that the independent variable has a significant effect on the dependent variable.\n",
    "\n",
    "2. Assessing Group Differences: ANOVA helps to assess whether there are meaningful differences between the groups. If the between-group variance is significant, it implies that there are significant differences in the means of the groups. This is crucial in many research studies to understand the impact of different factors on the outcome.\n",
    "\n",
    "3. Interpretation of Results: Understanding the partitioning of variance helps researchers interpret the ANOVA results. It provides insights into the relative importance of the independent variable (treatment effect) and the unexplained variability (error) in the data.\n",
    "\n",
    "4. Designing Better Experiments: By knowing how much of the variability in the dependent variable can be attributed to the grouping factor and how much is due to random error, researchers can design better experiments and improve the study's validity and reliability.\n",
    "\n",
    "5. Basis for Further Analysis: The partitioning of variance lays the groundwork for post hoc tests (e.g., Tukey's HSD, Bonferroni correction) to identify which specific groups differ significantly from each other when the overall ANOVA is significant.\n",
    "\n",
    "In summary, understanding the partitioning of variance in ANOVA is crucial for drawing meaningful conclusions from the analysis, interpreting the results correctly, and making informed decisions based on the impact of the independent variable on the dependent variable. It forms the foundation for the inference and interpretation of results in ANOVA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08817be-89b6-4f6b-af6e-0951d846eb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075c6b6-56da-4c33-9385-8821022ecec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0217fb04-9051-469a-a41b-c275af6f0854",
   "metadata": {},
   "source": [
    "Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e32e699-9005-43e3-b251-265f91bcd241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 1046.9333333333332\n",
      "Explained Sum of Squares (SSE): 46.8\n",
      "Residual Sum of Squares (SSR): 1000.1333333333332\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming sample data for three groups \n",
    "group1 = [10, 15, 12, 13, 11]\n",
    "group2 = [20, 25, 22, 24, 21]\n",
    "group3 = [30, 35, 32, 33, 31]\n",
    "\n",
    "# Combine all the data into one array\n",
    "all_data = np.concatenate((group1, group2, group3))\n",
    "\n",
    "# Calculate the overall mean (grand mean)\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate the sample means for each group\n",
    "group1_mean = np.mean(group1)\n",
    "group2_mean = np.mean(group2)\n",
    "group3_mean = np.mean(group3)\n",
    "\n",
    "# Calculate the total sum of squares (SST)\n",
    "SST = np.sum((all_data - overall_mean) ** 2)\n",
    "\n",
    "# Calculate the explained sum of squares (SSE)\n",
    "SSE = np.sum((group1 - group1_mean) ** 2) + np.sum((group2 - group2_mean) ** 2) + np.sum((group3 - group3_mean) ** 2)\n",
    "\n",
    "# Calculate the residual sum of squares (SSR)\n",
    "SSR = SST - SSE\n",
    "\n",
    "print(\"Total Sum of Squares (SST):\", SST)\n",
    "print(\"Explained Sum of Squares (SSE):\", SSE)\n",
    "print(\"Residual Sum of Squares (SSR):\", SSR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe145444-3cc4-4eff-9d1a-6e9c63892f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f43589-7f86-41d4-8c9b-a2f42e5625dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebfacf1c-d197-4227-bd05-649472b9d2a8",
   "metadata": {},
   "source": [
    "Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7246c15-ed49-4b71-a18a-6c122ab4be93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect of Group1: 1.9428571428566626\n",
      "Main Effect of Group2: 6.22301509746355e-27\n",
      "Interaction Effect: 4.162749391477013e-28\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Assuming smple data as a pandas DataFrame \n",
    "data = {\n",
    "    'Group1': [10, 12, 15, 11, 13, 14],\n",
    "    'Group2': [20, 22, 24, 21, 23, 25],\n",
    "    'Value': [30, 32, 35, 31, 33, 34]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a formula for the two-way ANOVA\n",
    "formula = 'Value ~ Group1 + Group2 + Group1:Group2'\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols(formula, data=df).fit()\n",
    "\n",
    "# Get the ANOVA table\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Extract main effects and interaction effect\n",
    "main_effect_group1 = anova_table.loc['Group1', 'sum_sq'] / anova_table.loc['Group1', 'df']\n",
    "main_effect_group2 = anova_table.loc['Group2', 'sum_sq'] / anova_table.loc['Group2', 'df']\n",
    "interaction_effect = anova_table.loc['Group1:Group2', 'sum_sq'] / anova_table.loc['Group1:Group2', 'df']\n",
    "\n",
    "print(\"Main Effect of Group1:\", main_effect_group1)\n",
    "print(\"Main Effect of Group2:\", main_effect_group2)\n",
    "print(\"Interaction Effect:\", interaction_effect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c822087-10ca-4248-b1be-84743d47ad5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d24340f-e7d2-433b-b641-1da3fa4a4c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b52ce234-9156-4fb1-8c43-8ce603035705",
   "metadata": {},
   "source": [
    "Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02.\n",
    "What can you conclude about the differences between the groups, and how would you interpret these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510355a-af45-435a-a374-915ee990f207",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic is used to test the null hypothesis that the means of all the groups are equal. The F-statistic measures the ratio of the variance between the groups (explained variance) to the variance within the groups (residual or unexplained variance). The p-value associated with the F-statistic indicates the probability of obtaining the observed F-statistic or a more extreme value under the assumption that the null hypothesis is true.\n",
    "\n",
    "Given the F-statistic of 5.23 and a p-value of 0.02, we can interpret the results as follows:\n",
    "\n",
    "1. The F-Statistic: The F-statistic is 5.23. This value represents the ratio of the variation between the group means to the variation within the groups. A higher F-statistic suggests that there is more variability between the group means compared to the variability within the groups.\n",
    "\n",
    "2. The P-Value: The p-value associated with the F-statistic is 0.02. This p-value represents the probability of observing an F-statistic as extreme as 5.23, or more extreme, if the null hypothesis is true. In this case, the p-value is less than the commonly used significance level of 0.05 (or 5%), indicating that the result is statistically significant.\n",
    "\n",
    "Interpretation:\n",
    "Since the p-value (0.02) is less than the chosen significance level (e.g., 0.05), we reject the null hypothesis. In other words, we have enough evidence to conclude that there are statistically significant differences between the means of at least two of the groups. The one-way ANOVA has detected a significant effect of the independent variable (the factor with multiple groups) on the dependent variable.\n",
    "\n",
    "Keep in mind that rejecting the null hypothesis in ANOVA only indicates that there is evidence of a significant difference between at least two groups, but it does not tell us which specific groups are different. For that, additional post hoc tests (e.g., Tukey's HSD, Bonferroni correction) may be performed to identify specific pairwise differences between groups.\n",
    "\n",
    "In summary, the results of the one-way ANOVA with an F-statistic of 5.23 and a p-value of 0.02 indicate that there are statistically significant differences between the means of the groups being compared. Further analysis can be conducted to determine which specific groups differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e46a6d-9263-4189-82c1-de577c276448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb7c09-99e3-4102-b7bb-9e45cc302610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43ecfa89-d23d-4bb5-88b6-01a05583cd35",
   "metadata": {},
   "source": [
    "Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88115d26-9598-4893-8c2e-50481d6d1a05",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is essential to ensure the accuracy and validity of the analysis. Different methods can be used to address missing data, and each method comes with its potential consequences. Let's explore some common approaches and their potential consequences:\n",
    "\n",
    "1. Complete Case Analysis (Listwise Deletion):\n",
    "In this approach, any participant with missing data on any of the repeated measures is excluded from the analysis. This method is straightforward, but it can lead to a reduction in sample size and potential bias if the missing data are not missing completely at random (MCAR). Complete case analysis may lead to less representative results if participants with missing data have different characteristics from those with complete data.\n",
    "\n",
    "2. Mean Imputation:\n",
    "Mean imputation involves replacing missing values with the mean of the available data for that variable. While this approach is simple, it can distort the distribution and variance of the variable, leading to underestimated standard errors and biased results. It also does not account for any relationship between variables, potentially leading to erroneous conclusions.\n",
    "\n",
    "3. Last Observation Carried Forward (LOCF) or Next Observation Carried Backward (NOCB):\n",
    "These methods impute missing values with the last observed value or the next observed value, respectively. While they are straightforward to implement, they assume that the data have a linear trajectory, which may not always be the case. LOCF and NOCB can introduce artificial correlations between repeated measures, leading to biased results and imprecise estimates.\n",
    "\n",
    "4. Multiple Imputation:\n",
    "Multiple imputation is a more sophisticated approach that creates multiple plausible imputed datasets based on the observed data. Each dataset is analyzed separately, and the results are combined to provide unbiased estimates and appropriate standard errors. Multiple imputation accounts for the uncertainty introduced by missing data, resulting in more reliable and robust results. However, it requires more computational resources and may be complex to implement.\n",
    "\n",
    "5. Maximum Likelihood Estimation (MLE):\n",
    "MLE is a statistical method that estimates model parameters based on the likelihood of the observed data. It treats missing data as parameters to be estimated and integrates them into the likelihood function. MLE is considered a flexible and efficient approach for handling missing data in repeated measures ANOVA, as it can accommodate different patterns of missingness. However, it may require specialized software and assumptions about the distribution of missing data.\n",
    "\n",
    "Consequences of Using Different Methods:\n",
    "Using inappropriate methods to handle missing data can lead to biased results, inflated or underestimated standard errors, incorrect conclusions, and reduced statistical power. Additionally, the choice of method may impact the precision and generalizability of the findings.\n",
    "\n",
    "It is crucial to carefully assess the nature of missing data, explore patterns of missingness, and choose a method that is appropriate for the specific dataset and research question. Researchers should transparently report how missing data were handled, including any sensitivity analyses to assess the impact of different approaches on the results. Consulting with statisticians or using specialized software can be helpful in making informed decisions about handling missing data in repeated measures ANOVA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41a2b89-74dc-4f74-aca8-0f5d4b234610",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071b368-055f-4421-8588-331c2f8ac874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db25b7ee-c95c-4492-894a-14f5efb56025",
   "metadata": {},
   "source": [
    "Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74deb52b-5a42-472d-85ad-97e8856c20f5",
   "metadata": {},
   "source": [
    "After conducting an ANOVA and finding a significant overall effect, post-hoc tests are often used to determine which specific groups differ significantly from each other. Post-hoc tests are necessary because ANOVA only tells us that there is a significant difference somewhere among the groups, but it does not identify which pairs of groups are significantly different. Some common post-hoc tests include:\n",
    "\n",
    "1. Tukey's Honestly Significant Difference (HSD) Test:\n",
    "Tukey's HSD test is widely used and controls the family-wise error rate, making it appropriate for multiple pairwise comparisons. It compares all possible pairs of group means and determines if their differences are statistically significant. Tukey's HSD test is best suited when the number of comparisons is moderate and when you want to protect against making any false positive (Type I error).\n",
    "\n",
    "2. Bonferroni Correction:\n",
    "The Bonferroni correction is a conservative method that adjusts the significance level for each individual comparison to control the overall family-wise error rate. It is suitable when there are many pairwise comparisons, but it may be overly conservative and reduce the power of the test.\n",
    "\n",
    "3. Scheffé's Test:\n",
    "Scheffé's test is a less conservative alternative to Tukey's HSD and Bonferroni correction. It is more suitable when the number of pairwise comparisons is large and not all comparisons are planned in advance.\n",
    "\n",
    "4. Dunnett's Test:\n",
    "Dunnett's test is used when you have one control group and you want to compare the other groups to the control group. It adjusts the significance level to control the family-wise error rate for multiple comparisons against the control group.\n",
    "\n",
    "5. Fisher's Least Significant Difference (LSD) Test:\n",
    "Fisher's LSD test is a less conservative alternative to Tukey's HSD, but it does not control the family-wise error rate. It is more appropriate when you have a small number of comparisons and are not concerned about multiple testing.\n",
    "\n",
    "Example of a Situation Requiring Post-Hoc Test:\n",
    "\n",
    "Suppose a researcher conducts an ANOVA to compare the effectiveness of three different exercise programs (A, B, and C) on weight loss. The ANOVA shows a significant difference in weight loss between the groups. However, the researcher does not know which specific exercise program(s) lead to significantly different outcomes.\n",
    "\n",
    "In this case, a post-hoc test like Tukey's HSD or Scheffé's test would be used to perform pairwise comparisons between the three exercise programs. The post-hoc test would determine if there are significant differences in weight loss between any of the pairs of exercise programs (e.g., A vs. B, B vs. C, A vs. C). The post-hoc test helps to identify which exercise programs lead to significantly different weight loss results and provides more detailed insights than the ANOVA alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae6c70-1a29-4ea5-81d2-dab694a38b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cbebd8-17dc-4f25-993f-4cb820a8ada0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ae5810e-df5e-4fd7-81bb-d95febdbc9a4",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb785aeb-bb5d-491f-bd04-c78fa16a9c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 47.94271713416693\n",
      "p-value: 6.199190263311225e-16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming sample data for weight loss in each diet \n",
    "diet_A = [2, 3, 4, 5, 4, 3, 6, 7, 8, 5, 6, 4, 3, 4, 5, 6, 7, 8, 5, 4,\n",
    "          3, 4, 5, 6, 3, 4, 5, 4, 3, 5, 6, 4, 5, 3, 4, 6, 7, 5, 4, 3]\n",
    "diet_B = [1, 2, 3, 2, 3, 1, 2, 3, 4, 3, 2, 3, 2, 1, 2, 3, 2, 4, 3, 2,\n",
    "          3, 2, 1, 2, 3, 1, 2, 4, 3, 2, 3, 2, 1, 2, 3, 2, 3, 1, 2, 3]\n",
    "diet_C = [3, 4, 5, 3, 4, 5, 4, 3, 2, 5, 4, 3, 5, 4, 3, 4, 5, 6, 4, 5,\n",
    "          3, 4, 3, 5, 4, 3, 5, 6, 4, 5, 4, 3, 5, 4, 3, 4, 5, 4, 3, 5]\n",
    "\n",
    "# Combine all the data into one array\n",
    "all_data = np.concatenate((diet_A, diet_B, diet_C))\n",
    "\n",
    "# Create a list of group labels\n",
    "group_labels = ['A'] * len(diet_A) + ['B'] * len(diet_B) + ['C'] * len(diet_C)\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "F_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "print(\"F-statistic:\", F_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff1ffab-eebe-4e59-83c6-9768e5db73f9",
   "metadata": {},
   "source": [
    "Interpretation of Results:\n",
    "\n",
    "The p-value is greater than the significance level, we fail to reject the null hypothesis, and we do not have enough evidence to claim significant differences between the diets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09e0e6-c86d-4be9-b82d-788eced33aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a873d6-f15f-4eca-b598-8fabcb32fb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea04a90a-0702-4d78-874c-f502078aa049",
   "metadata": {},
   "source": [
    "Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f772fbc7-b7d1-4df8-88fc-eff3ba33eaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          sum_sq    df          F        PR(>F)\n",
      "Software                0.466667   2.0   0.015521  9.846086e-01\n",
      "Experience           1333.333333   1.0  88.691796  1.565765e-09\n",
      "Software:Experience     0.066667   2.0   0.002217  9.977854e-01\n",
      "Residual              360.800000  24.0        NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sample data as a pandas DataFrame (replace this with your data)\n",
    "data = {\n",
    "    'Software': ['A', 'B', 'C'] * 10,\n",
    "    'Experience': ['Novice'] * 15 + ['Experienced'] * 15,\n",
    "    'Time': [12, 10, 11, 15, 13, 14, 9, 11, 10, 12, 13, 14, 18, 17, 16,\n",
    "             25, 24, 22, 27, 26, 28, 20, 21, 19, 31, 30, 32, 29, 30, 31]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert Experience column to categorical type for correct ANOVA\n",
    "df['Experience'] = df['Experience'].astype('category')\n",
    "\n",
    "# Create a formula for the two-way ANOVA\n",
    "formula = 'Time ~ Software + Experience + Software:Experience'\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "model = ols(formula, data=df).fit()\n",
    "\n",
    "# Get the ANOVA table\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa45979-50ce-479b-86fc-fcc85be4aa8d",
   "metadata": {},
   "source": [
    "Interpretation of Results:\n",
    "\n",
    "The ANOVA table provides the F-statistics and p-values for each main effect (Software, Experience) and the interaction effect (Software:Experience).\n",
    "\n",
    "\n",
    "1. Main Effects:\n",
    "   a. Software: The F-statistic for the main effect of Software is approximately 0.0155, and the associated p-value is approximately 0.985. Since the p-value is much greater than the significance level (e.g., 0.05), we fail to reject the null hypothesis. This indicates that there is no significant difference in the average time taken to complete the task among the three software programs (A, B, and C).\n",
    "\n",
    "   b. Experience: The F-statistic for the main effect of Experience is approximately 88.692, and the associated p-value is significantly smaller than the significance level (e.g., 0.05) at approximately 1.566e-09. Therefore, we reject the null hypothesis and conclude that there is a significant difference in the average time taken to complete the task between novice and experienced employees.\n",
    "\n",
    "2. Interaction Effect:\n",
    "   The F-statistic for the interaction effect between Software and Experience is approximately 0.0022, and the associated p-value is approximately 0.998. Since the p-value is much greater than the significance level (e.g., 0.05), we fail to reject the null hypothesis. This indicates that there is no significant interaction effect between the software programs and employee experience level in influencing the time taken to complete the task. In other words, the effect of one variable (Software) on the dependent variable (Time) does not depend on the level of the other variable (Experience).\n",
    "\n",
    "3. Residual:\n",
    "   The residual sum of squares represents the unexplained variability in the data after considering the effects of the independent variables. It is expected to be non-zero, and its degrees of freedom depend on the total sample size and the number of independent variables in the model.\n",
    "\n",
    "In summary, the results suggest that there is a significant main effect of Experience, indicating that employee experience level (novice vs. experienced) has a significant impact on the time taken to complete the task. However, there are no significant main effects for the Software variable, and there is no significant interaction effect between Software and Experience. It appears that the software programs used do not significantly affect the time taken to complete the task, and the effect of software does not depend on the employee's experience level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b183946-e99f-4411-a9ac-88c8218f16ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693212f2-61f3-462d-834a-225d201d0ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcf941e9-a58a-4884-a3ee-a4d3f4318b10",
   "metadata": {},
   "source": [
    "Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2709e58-4647-4887-80c6-7923c4c03587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -4.315953079030419\n",
      "p-value: 0.00010940882215084969\n",
      "   Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "=========================================================\n",
      " group1    group2    meandiff p-adj  lower  upper  reject\n",
      "---------------------------------------------------------\n",
      "Control Experimental      6.0 0.0001 3.1857 8.8143   True\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Assumed sample data for test scores in the control group \n",
    "control_group_scores = [85, 78, 90, 79, 88, 84, 91, 82, 87, 80, 83, 86, 75, 77, 89, 81, 76, 85, 78, 80]\n",
    "\n",
    "# Assumed sample data for test scores in the experimental group \n",
    "experimental_group_scores = [90, 88, 95, 85, 92, 89, 94, 86, 93, 87, 84, 91, 84, 82, 96, 88, 85, 90, 87, 88]\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_group_scores, experimental_group_scores)\n",
    "\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Perform post-hoc test (e.g., Tukey's HSD) if the t-test results are significant\n",
    "if p_value < 0.05:\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "    # Combine all the data into one array\n",
    "    all_scores = np.concatenate((control_group_scores, experimental_group_scores))\n",
    "\n",
    "    # Create a list of group labels (0 for control group, 1 for experimental group)\n",
    "    group_labels = ['Control'] * len(control_group_scores) + ['Experimental'] * len(experimental_group_scores)\n",
    "\n",
    "    # Perform Tukey's HSD post-hoc test\n",
    "    tukey_result = pairwise_tukeyhsd(endog=all_scores, groups=group_labels, alpha=0.05)\n",
    "\n",
    "    print(tukey_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321232ae-43f9-45de-9ea3-f4f1d2331f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a67fc4-549b-4ae7-a5fb-6f319d5290f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "578a6e35-2b53-483e-852f-1fd95cf31bb0",
   "metadata": {},
   "source": [
    "Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post-hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b883480e-2fc4-4572-813e-044012187b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 262.3038791575067\n",
      "p-value: 1.4371847637303197e-37\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Assume sample data for daily sales in Store A, Store B, and Store C \n",
    "store_A_sales = [200, 220, 250, 210, 230, 190, 240, 210, 230, 215, 220, 200, 240, 235, 225, 210, 205, 225, 235, 215, 230, 220, 210, 235, 215, 240, 225, 230, 220, 210]\n",
    "store_B_sales = [180, 190, 200, 205, 195, 210, 185, 195, 205, 180, 200, 190, 210, 200, 195, 190, 185, 200, 210, 220, 185, 195, 190, 200, 195, 185, 200, 210, 220, 185]\n",
    "store_C_sales = [250, 260, 270, 280, 260, 255, 265, 270, 280, 260, 250, 275, 265, 260, 270, 280, 270, 250, 265, 260, 255, 270, 280, 260, 255, 270, 265, 280, 260, 255]\n",
    "\n",
    "# Combine all the data into one array\n",
    "all_sales = np.concatenate((store_A_sales, store_B_sales, store_C_sales))\n",
    "\n",
    "# Create a list of group labels\n",
    "group_labels = ['Store A'] * len(store_A_sales) + ['Store B'] * len(store_B_sales) + ['Store C'] * len(store_C_sales)\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "F_statistic, p_value = stats.f_oneway(store_A_sales, store_B_sales, store_C_sales)\n",
    "\n",
    "print(\"F-statistic:\", F_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
