{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253329af-6548-4378-9146-0c2c71f1099e",
   "metadata": {},
   "source": [
    "### Q1. Theory and Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdf1a63-e280-45c0-93c8-cb8a28fac887",
   "metadata": {},
   "source": [
    "#### 1. Explain the concept of batch normalization in the context of Artificial Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ce2ad0-1688-439e-8b99-6605104fa84e",
   "metadata": {},
   "source": [
    "Batch Normalization, often abbreviated as \"BatchNorm\" or \"BN,\" is a technique used in artificial neural networks to normalize the activations of each layer within a mini-batch of training examples. It was introduced to address several issues related to training deep neural networks, including the vanishing gradient problem and the training instability that can occur in very deep networks. Here's an explanation of the concept of Batch Normalization:\n",
    "\n",
    "**Key Ideas and Concepts:**\n",
    "\n",
    "1. **Normalization:** The main idea behind Batch Normalization is to normalize the input to each layer within a mini-batch. Normalization typically involves subtracting the mean and dividing by the standard deviation of the activations within the mini-batch. This process transforms the activations to have a mean of zero and a standard deviation of one.\n",
    "\n",
    "2. **Learnable Parameters:** In addition to normalizing the activations, BatchNorm introduces learnable parameters: scaling and shifting parameters (denoted as γ and β), which allow the network to learn the optimal scaling and shifting of the normalized activations. These parameters are learned during training.\n",
    "\n",
    "3. **Applicability:** Batch Normalization is primarily applied to fully connected layers and convolutional layers in deep neural networks. It can be used in feedforward networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs).\n",
    "\n",
    "**Advantages and Benefits:**\n",
    "\n",
    "Batch Normalization offers several advantages and benefits:\n",
    "\n",
    "1. **Stabilizes Training:** BatchNorm helps stabilize the training of deep neural networks by reducing internal covariate shift. It ensures that the activations within each layer have similar statistics (mean and variance) during training, which can lead to faster convergence and more stable gradient updates.\n",
    "\n",
    "2. **Mitigates Vanishing Gradient:** It helps mitigate the vanishing gradient problem by ensuring that gradients can flow more consistently through the network. This can enable training of very deep networks without suffering from issues related to gradient vanishing or exploding.\n",
    "\n",
    "3. **Regularization Effect:** BatchNorm provides a form of regularization because it adds noise to the activations during training. This noise can act as a form of implicit regularization, reducing overfitting and improving generalization.\n",
    "\n",
    "4. **Allows Larger Learning Rates:** With BatchNorm, it's often possible to use larger learning rates during training, which can speed up convergence and lead to better results.\n",
    "\n",
    "5. **Reduces Dependency on Weight Initialization:** BatchNorm makes neural networks less sensitive to the choice of weight initialization. This means that initializing weights randomly (e.g., using Gaussian or Xavier initialization) can work well in conjunction with BatchNorm.\n",
    "\n",
    "**Training and Inference:**\n",
    "\n",
    "During training, BatchNorm computes the mean and standard deviation of activations within each mini-batch. However, during inference (when making predictions), the mean and standard deviation are typically computed using moving averages over multiple mini-batches from the training data. This allows BatchNorm to adapt to the statistics of the entire training dataset.\n",
    "\n",
    "In summary, Batch Normalization is a technique used to normalize the activations of neural network layers within mini-batches during training. It helps stabilize training, mitigate the vanishing gradient problem, act as implicit regularization, and improve the overall convergence of deep neural networks. BatchNorm is a widely used technique and has become a standard component in the training of deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d76536-b1eb-443e-8739-56f02a3fb306",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f47058-9925-448b-9c8f-fff0589a153c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6d7146d-fa39-43c9-aef0-1b312160fe88",
   "metadata": {},
   "source": [
    "#### 2. Describe the benefits of using batch normalization during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e366ac-4d71-43a2-ba2e-fa9e68988597",
   "metadata": {},
   "source": [
    "Batch Normalization (BatchNorm) provides several benefits during the training of artificial neural networks. These benefits contribute to more stable and efficient training processes and lead to improved model performance. Here are the key benefits of using Batch Normalization during training:\n",
    "\n",
    "1. **Stabilizes Training:** BatchNorm helps stabilize the training of deep neural networks by reducing internal covariate shift. Internal covariate shift refers to the change in the distribution of activations within a layer as the network's parameters are updated during training. By normalizing the activations within each mini-batch, BatchNorm ensures that the activations have similar statistics (mean and variance), making the optimization process more stable.\n",
    "\n",
    "2. **Faster Convergence:** Because BatchNorm reduces internal covariate shift and maintains consistent activation statistics, it often leads to faster convergence during training. Neural networks with BatchNorm layers can achieve the desired performance with fewer training epochs, reducing training time.\n",
    "\n",
    "3. **Mitigates Vanishing Gradient:** BatchNorm helps mitigate the vanishing gradient problem, which can occur in deep networks. When activations are well-conditioned (i.e., have similar scales), gradients can flow more consistently through the network during backpropagation. This enables the training of very deep networks without suffering from vanishing or exploding gradients.\n",
    "\n",
    "4. **Enables Larger Learning Rates:** With BatchNorm, it's often possible to use larger learning rates during training. Larger learning rates can speed up convergence and help escape local minima in the loss landscape. BatchNorm's normalization process reduces the sensitivity to the choice of learning rate.\n",
    "\n",
    "5. **Reduces Overfitting:** BatchNorm provides a form of regularization. By adding noise to the activations during training, it acts as a regularizing effect. This reduces the risk of overfitting and leads to models that generalize better to unseen data.\n",
    "\n",
    "6. **Smoother Loss Landscape:** The normalization introduced by BatchNorm results in a smoother loss landscape, which can make optimization easier. This can reduce the likelihood of getting stuck in poor local minima during training.\n",
    "\n",
    "7. **Independence from Weight Initialization:** BatchNorm makes neural networks less sensitive to the choice of weight initialization. This means that initializing weights randomly (e.g., using Gaussian or Xavier initialization) can work well in conjunction with BatchNorm, reducing the need for careful weight initialization strategies.\n",
    "\n",
    "8. **Improved Generalization:** Models trained with BatchNorm often generalize better to unseen data, as they are less prone to overfitting and exhibit more stable convergence behaviors.\n",
    "\n",
    "9. **Compatibility with Different Architectures:** BatchNorm can be applied to various neural network architectures, including feedforward networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). Its versatility makes it a widely applicable technique.\n",
    "\n",
    "10. **Consistent Activation Statistics:** BatchNorm ensures that the mean and variance of activations are consistent within each layer during training, which can lead to more reliable and predictable model behavior.\n",
    "\n",
    "In summary, Batch Normalization is a powerful technique that offers numerous benefits during the training of neural networks. It enhances stability, convergence speed, and generalization while mitigating common training issues like internal covariate shift and the vanishing gradient problem. These advantages have made BatchNorm a standard component in the training of deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85241cb5-5016-4a15-884c-3a418f9da25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c3ec8-6a1e-4d39-9fd1-bc36ba5a944a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "417ffde9-3df5-400c-84d8-83daab5e2ac8",
   "metadata": {},
   "source": [
    "#### 3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0692a8-6fd3-4941-981d-e54e7b0900bb",
   "metadata": {},
   "source": [
    "Batch Normalization (BatchNorm) works by normalizing the activations of each layer within a mini-batch during the training of a neural network. It introduces learnable parameters to fine-tune the normalization process. Here's a detailed explanation of the working principle of BatchNorm, including the normalization step and the learnable parameters:\n",
    "\n",
    "**Normalization Step:**\n",
    "\n",
    "The primary goal of BatchNorm is to normalize the activations within each layer of a neural network. Normalization typically involves two steps: mean and variance normalization.\n",
    "\n",
    "1. **Mean Normalization:** For each feature (i.e., neuron or channel), BatchNorm computes the mean of the activations within a mini-batch. This mean is calculated independently for each feature across the entire mini-batch.\n",
    "\n",
    "2. **Variance Normalization:** BatchNorm also computes the variance of the activations within the mini-batch, again independently for each feature.\n",
    "\n",
    "Once the mean and variance are calculated for each feature, the activations for that feature are normalized using the following formula:\n",
    "\n",
    "\n",
    "x^_i = x_i - μ / sqrt(σ^2 + ϵ) \n",
    "\n",
    "\n",
    "Where:\n",
    "- (x^_i) is the normalized activation for the i-th feature.\n",
    "- (x_i) is the original activation for the i-th feature.\n",
    "- (μ) is the mean of the activations for the i-th feature within the mini-batch.\n",
    "- (σ^2) is the variance of the activations for the i-th feature within the mini-batch.\n",
    "- (ϵ) is a small constant (typically added for numerical stability to prevent division by zero).\n",
    "\n",
    "After normalization, the activations have a mean of zero and a standard deviation of one. This normalization step ensures that the activations within a mini-batch have similar statistical properties, which contributes to more stable training.\n",
    "\n",
    "**Learnable Parameters:**\n",
    "\n",
    "While the normalization step ensures consistency within a mini-batch, BatchNorm introduces learnable parameters to allow the network to adjust and fine-tune the normalized activations. These learnable parameters are:\n",
    "\n",
    "1. **Scaling Parameter (γ):** This parameter is used to scale the normalized activations. It allows the network to learn how much to amplify or attenuate the activations for each feature. If (γ) is large, it amplifies the activations; if it's small, it attenuates them.\n",
    "\n",
    "2. **Shifting Parameter (β):** This parameter is used to shift the normalized activations. It allows the network to learn a bias term for each feature. If (β) is nonzero, it shifts the activations away from a mean of zero.\n",
    "\n",
    "The scaled and shifted normalized activations are computed as follows:\n",
    "\n",
    "y_i = γx^_i + β\n",
    "\n",
    "Where:\n",
    "- (y_i) is the final output activation for the i-th feature.\n",
    "- (γ) is the scaling parameter for the i-th feature.\n",
    "- (β) is the shifting parameter for the i-th feature.\n",
    "- (x^_i) is the normalized activation for the i-th feature.\n",
    "\n",
    "During training, both (γ) and (β) are learned through backpropagation. These parameters allow the network to adapt the normalization process to the specific characteristics of the data and the task, which can lead to improved model performance.\n",
    "\n",
    "In summary, Batch Normalization works by normalizing activations within mini-batches using mean and variance statistics. It introduces learnable scaling and shifting parameters that allow the network to fine-tune the normalization process for each feature. This normalization and parameterization contribute to more stable and efficient training of deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591783a2-46a2-4e19-a5a1-c18cdbff4430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdda8da-113e-4be5-97a3-7475d1166b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ade89c4-f096-438c-a028-a26948de39ce",
   "metadata": {},
   "source": [
    "### Q2. Implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c150eca-8398-447f-a9e1-a18195a6db3a",
   "metadata": {},
   "source": [
    "#### 1. Choose a dataset of your choice (e.g., MNIST, CIFAR-10) and preprocess it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de323cf7-e54c-44b3-968e-560182c3a332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting keras<2.15,>=2.14.0\n",
      "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting tensorboard<2.15,>=2.14\n",
      "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting ml-dtypes==0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-2.3.7-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.23.1-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.9/181.9 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting urllib3>=2.0.5\n",
      "  Downloading urllib3-2.0.5-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, ml-dtypes, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests, pyasn1-modules, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.13\n",
      "    Uninstalling urllib3-1.26.13:\n",
      "      Successfully uninstalled urllib3-1.26.13\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "Successfully installed absl-py-2.0.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.23.1 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.58.0 keras-2.14.0 libclang-16.0.6 markdown-3.4.4 ml-dtypes-0.2.0 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.14.1 tensorboard-data-server-0.7.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-io-gcs-filesystem-0.34.0 termcolor-2.3.0 urllib3-2.0.5 werkzeug-2.3.7 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f65e9827-5e80-4529-a342-9d5b624818b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "689213e2-2a94-4dc4-a6ef-7066d279b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#Data Preprocessing\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cb44dd-c4a4-4069-a679-2635d3192b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9251e55-7244-4f15-bf71-543ba796f34c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e2c466d-657e-4713-9c3c-160f6a6544ac",
   "metadata": {},
   "source": [
    "#### 2. Implement a simple feedforward neural network using any deep learning framework/library (e.g., Tensorflow, PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d51f3a6-913a-4006-a660-18c74b5608bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple feedforward neural network without batch normalization\n",
    "def create_model_without_bn():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(28 * 28,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define a simple feedforward neural network with batch normalization\n",
    "def create_model_with_bn():\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(28 * 28,)),\n",
    "        layers.Dense(128),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(64),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile models\n",
    "model_without_bn = create_model_without_bn()\n",
    "model_with_bn = create_model_with_bn()\n",
    "\n",
    "model_without_bn.compile(optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "model_with_bn.compile(optimizer='adam',\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03afc578-7949-4152-a08c-77b30ab10e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6ac5f-7ed6-4b17-83d6-2481b7772399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72343234-6975-4b51-a06a-c1d887c109c1",
   "metadata": {},
   "source": [
    "#### 3. Train the neural network on the chosen dataset without using batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "daa283c0-689d-45ec-8e1b-fb2ceada9592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 - 4s - loss: 0.2769 - accuracy: 0.9205 - val_loss: 0.1382 - val_accuracy: 0.9573 - 4s/epoch - 4ms/step\n",
      "Epoch 2/10\n",
      "938/938 - 3s - loss: 0.1134 - accuracy: 0.9659 - val_loss: 0.0987 - val_accuracy: 0.9705 - 3s/epoch - 3ms/step\n",
      "Epoch 3/10\n",
      "938/938 - 3s - loss: 0.0756 - accuracy: 0.9766 - val_loss: 0.0821 - val_accuracy: 0.9728 - 3s/epoch - 3ms/step\n",
      "Epoch 4/10\n",
      "938/938 - 3s - loss: 0.0593 - accuracy: 0.9812 - val_loss: 0.0817 - val_accuracy: 0.9737 - 3s/epoch - 3ms/step\n",
      "Epoch 5/10\n",
      "938/938 - 3s - loss: 0.0444 - accuracy: 0.9861 - val_loss: 0.0884 - val_accuracy: 0.9734 - 3s/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "938/938 - 3s - loss: 0.0358 - accuracy: 0.9890 - val_loss: 0.0954 - val_accuracy: 0.9738 - 3s/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "938/938 - 3s - loss: 0.0302 - accuracy: 0.9905 - val_loss: 0.0858 - val_accuracy: 0.9763 - 3s/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "938/938 - 3s - loss: 0.0244 - accuracy: 0.9919 - val_loss: 0.0795 - val_accuracy: 0.9756 - 3s/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "938/938 - 3s - loss: 0.0209 - accuracy: 0.9930 - val_loss: 0.0920 - val_accuracy: 0.9771 - 3s/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "938/938 - 3s - loss: 0.0198 - accuracy: 0.9936 - val_loss: 0.0887 - val_accuracy: 0.9770 - 3s/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train the model without batch normalization\n",
    "history_without_bn = model_without_bn.fit(x_train, y_train, epochs=10, batch_size=64,\n",
    "                                          validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_without_bn, test_accuracy_without_bn = model_without_bn.evaluate(x_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1954619a-967c-4433-9175-ca438fcec8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92cbbb6-e2a9-488a-beda-ceaf37bbfa8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b178e353-2bf6-4c9a-b833-bff582175916",
   "metadata": {},
   "source": [
    "#### 4. Implement batch normalization layers in the neural network and train the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56f590ae-a515-400a-bb4e-d62bc2c25d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 - 6s - loss: 0.2540 - accuracy: 0.9306 - val_loss: 0.1039 - val_accuracy: 0.9680 - 6s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "938/938 - 4s - loss: 0.0992 - accuracy: 0.9701 - val_loss: 0.0892 - val_accuracy: 0.9733 - 4s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "938/938 - 4s - loss: 0.0700 - accuracy: 0.9787 - val_loss: 0.0756 - val_accuracy: 0.9760 - 4s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "938/938 - 4s - loss: 0.0549 - accuracy: 0.9823 - val_loss: 0.0666 - val_accuracy: 0.9794 - 4s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "938/938 - 4s - loss: 0.0450 - accuracy: 0.9854 - val_loss: 0.0682 - val_accuracy: 0.9787 - 4s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "938/938 - 4s - loss: 0.0365 - accuracy: 0.9882 - val_loss: 0.0708 - val_accuracy: 0.9784 - 4s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "938/938 - 4s - loss: 0.0315 - accuracy: 0.9897 - val_loss: 0.0719 - val_accuracy: 0.9783 - 4s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "938/938 - 4s - loss: 0.0267 - accuracy: 0.9913 - val_loss: 0.0732 - val_accuracy: 0.9797 - 4s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "938/938 - 4s - loss: 0.0253 - accuracy: 0.9916 - val_loss: 0.0738 - val_accuracy: 0.9789 - 4s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "938/938 - 4s - loss: 0.0223 - accuracy: 0.9929 - val_loss: 0.0879 - val_accuracy: 0.9764 - 4s/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
    "# Train the model with batch normalization\n",
    "history_with_bn = model_with_bn.fit(x_train, y_train, epochs=10, batch_size=64,\n",
    "                                    validation_data=(x_test, y_test), verbose=2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss_with_bn, test_accuracy_with_bn = model_with_bn.evaluate(x_test, y_test, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d34a5-18b6-4bbc-bab5-e8c8fc0ef8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e3ed7d-1e1f-49a6-baa4-cf88eeca9f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e01b6e5-3e8e-4b75-af27-d88e3cff5a28",
   "metadata": {},
   "source": [
    "#### 5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25470426-8f9d-45b9-9bb9-e86fb1d0d951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model without Batch Normalization:\n",
      "Test Loss: 0.0887, Test Accuracy: 0.9770\n",
      "Model with Batch Normalization:\n",
      "Test Loss: 0.0879, Test Accuracy: 0.9764\n"
     ]
    }
   ],
   "source": [
    "print(\"Model without Batch Normalization:\")\n",
    "print(f\"Test Loss: {test_loss_without_bn:.4f}, Test Accuracy: {test_accuracy_without_bn:.4f}\")\n",
    "\n",
    "print(\"Model with Batch Normalization:\")\n",
    "print(f\"Test Loss: {test_loss_with_bn:.4f}, Test Accuracy: {test_accuracy_with_bn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f77523-f27d-42a4-a5f0-03b86d00f0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c931bbac-7c85-42fd-b56b-be8bbbf98ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e44d93df-d123-4f17-a056-e2e2a9ee7679",
   "metadata": {},
   "source": [
    "#### 6. Discuss the impact of batch normalization on the training process and the performance of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b1e2b-50b8-47db-b277-4fe93b62ee89",
   "metadata": {},
   "source": [
    "Batch normalization typically has lead to reduced loss with almost the same accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530783a-aac6-403d-a110-954c0802e2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec517582-6fcf-4af8-8002-689dc52caaae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bbf8caa-ad14-4248-b25b-b3f604d4e4f1",
   "metadata": {},
   "source": [
    "### Q3. Experimentation and Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6f98f-8839-4771-a8d2-1920a3b58b0e",
   "metadata": {},
   "source": [
    "#### 1. Experiment with different batch sizes and observe the effect on the training dynamics and model performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "668bc4ce-bbab-4d52-8add-9564d9368ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-28 08:25:05.175459: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-28 08:25:05.709419: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-09-28 08:25:05.709479: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-09-28 08:25:05.712791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-28 08:25:06.006700: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-28 08:25:06.009146: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-28 08:25:07.979927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch size 32:\n",
      "Epoch 1/10\n",
      "1500/1500 - 7s - loss: 0.2754 - accuracy: 0.9222 - val_loss: 0.1289 - val_accuracy: 0.9614 - 7s/epoch - 5ms/step\n",
      "Epoch 2/10\n",
      "1500/1500 - 6s - loss: 0.1216 - accuracy: 0.9630 - val_loss: 0.0984 - val_accuracy: 0.9701 - 6s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "1500/1500 - 6s - loss: 0.0928 - accuracy: 0.9710 - val_loss: 0.0880 - val_accuracy: 0.9736 - 6s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "1500/1500 - 6s - loss: 0.0734 - accuracy: 0.9767 - val_loss: 0.0861 - val_accuracy: 0.9736 - 6s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "1500/1500 - 6s - loss: 0.0616 - accuracy: 0.9803 - val_loss: 0.0847 - val_accuracy: 0.9757 - 6s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "1500/1500 - 6s - loss: 0.0503 - accuracy: 0.9830 - val_loss: 0.0845 - val_accuracy: 0.9761 - 6s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "1500/1500 - 5s - loss: 0.0482 - accuracy: 0.9843 - val_loss: 0.0821 - val_accuracy: 0.9771 - 5s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "1500/1500 - 6s - loss: 0.0431 - accuracy: 0.9855 - val_loss: 0.0861 - val_accuracy: 0.9760 - 6s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "1500/1500 - 6s - loss: 0.0380 - accuracy: 0.9876 - val_loss: 0.0860 - val_accuracy: 0.9764 - 6s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "1500/1500 - 6s - loss: 0.0362 - accuracy: 0.9880 - val_loss: 0.0830 - val_accuracy: 0.9791 - 6s/epoch - 4ms/step\n",
      "Test accuracy with batch size 32: 97.93%\n",
      "\n",
      "Training with batch size 64:\n",
      "Epoch 1/10\n",
      "750/750 - 5s - loss: 0.2797 - accuracy: 0.9221 - val_loss: 0.1213 - val_accuracy: 0.9634 - 5s/epoch - 6ms/step\n",
      "Epoch 2/10\n",
      "750/750 - 3s - loss: 0.1079 - accuracy: 0.9677 - val_loss: 0.0968 - val_accuracy: 0.9718 - 3s/epoch - 4ms/step\n",
      "Epoch 3/10\n",
      "750/750 - 3s - loss: 0.0734 - accuracy: 0.9769 - val_loss: 0.1000 - val_accuracy: 0.9694 - 3s/epoch - 4ms/step\n",
      "Epoch 4/10\n",
      "750/750 - 3s - loss: 0.0572 - accuracy: 0.9825 - val_loss: 0.0856 - val_accuracy: 0.9746 - 3s/epoch - 4ms/step\n",
      "Epoch 5/10\n",
      "750/750 - 3s - loss: 0.0451 - accuracy: 0.9855 - val_loss: 0.0885 - val_accuracy: 0.9737 - 3s/epoch - 4ms/step\n",
      "Epoch 6/10\n",
      "750/750 - 3s - loss: 0.0372 - accuracy: 0.9881 - val_loss: 0.1021 - val_accuracy: 0.9723 - 3s/epoch - 4ms/step\n",
      "Epoch 7/10\n",
      "750/750 - 3s - loss: 0.0308 - accuracy: 0.9904 - val_loss: 0.0849 - val_accuracy: 0.9757 - 3s/epoch - 4ms/step\n",
      "Epoch 8/10\n",
      "750/750 - 3s - loss: 0.0273 - accuracy: 0.9915 - val_loss: 0.0982 - val_accuracy: 0.9728 - 3s/epoch - 4ms/step\n",
      "Epoch 9/10\n",
      "750/750 - 3s - loss: 0.0256 - accuracy: 0.9912 - val_loss: 0.0901 - val_accuracy: 0.9753 - 3s/epoch - 4ms/step\n",
      "Epoch 10/10\n",
      "750/750 - 3s - loss: 0.0231 - accuracy: 0.9923 - val_loss: 0.0961 - val_accuracy: 0.9757 - 3s/epoch - 4ms/step\n",
      "Test accuracy with batch size 64: 97.77%\n",
      "\n",
      "Training with batch size 128:\n",
      "Epoch 1/10\n",
      "375/375 - 4s - loss: 0.3338 - accuracy: 0.9104 - val_loss: 0.1581 - val_accuracy: 0.9581 - 4s/epoch - 10ms/step\n",
      "Epoch 2/10\n",
      "375/375 - 2s - loss: 0.1131 - accuracy: 0.9681 - val_loss: 0.1092 - val_accuracy: 0.9663 - 2s/epoch - 5ms/step\n",
      "Epoch 3/10\n",
      "375/375 - 2s - loss: 0.0739 - accuracy: 0.9784 - val_loss: 0.0995 - val_accuracy: 0.9699 - 2s/epoch - 5ms/step\n",
      "Epoch 4/10\n",
      "375/375 - 2s - loss: 0.0507 - accuracy: 0.9855 - val_loss: 0.0964 - val_accuracy: 0.9713 - 2s/epoch - 5ms/step\n",
      "Epoch 5/10\n",
      "375/375 - 2s - loss: 0.0396 - accuracy: 0.9887 - val_loss: 0.0829 - val_accuracy: 0.9752 - 2s/epoch - 5ms/step\n",
      "Epoch 6/10\n",
      "375/375 - 2s - loss: 0.0301 - accuracy: 0.9912 - val_loss: 0.0906 - val_accuracy: 0.9737 - 2s/epoch - 5ms/step\n",
      "Epoch 7/10\n",
      "375/375 - 2s - loss: 0.0241 - accuracy: 0.9928 - val_loss: 0.0881 - val_accuracy: 0.9747 - 2s/epoch - 5ms/step\n",
      "Epoch 8/10\n",
      "375/375 - 2s - loss: 0.0222 - accuracy: 0.9930 - val_loss: 0.0888 - val_accuracy: 0.9768 - 2s/epoch - 5ms/step\n",
      "Epoch 9/10\n",
      "375/375 - 2s - loss: 0.0162 - accuracy: 0.9955 - val_loss: 0.0922 - val_accuracy: 0.9748 - 2s/epoch - 5ms/step\n",
      "Epoch 10/10\n",
      "375/375 - 2s - loss: 0.0131 - accuracy: 0.9963 - val_loss: 0.0977 - val_accuracy: 0.9745 - 2s/epoch - 5ms/step\n",
      "Test accuracy with batch size 128: 97.62%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Data preprocessing\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Experiment with different batch sizes\n",
    "batch_sizes = [32, 64, 128]\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Training with batch size {batch_size}:\")\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28, 1)),\n",
    "        Dense(128),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        Dense(64),\n",
    "        BatchNormalization(),\n",
    "        tf.keras.layers.ReLU(),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=10, validation_split=0.2, verbose=2)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test accuracy with batch size {batch_size}: {test_accuracy * 100:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9af253-c131-404a-a5cd-a1ca18f1faab",
   "metadata": {},
   "source": [
    "#### As the batch size increases, the test accuracy decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7716eb4-6134-4125-9083-8967b72f37c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbe192-0607-40e2-9f81-02cc57ae9fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1cc624b-fa8b-463c-9b49-b00d3f69b121",
   "metadata": {},
   "source": [
    "#### 2. Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a082d-0659-4a0b-9d2b-dae0b8ee71f5",
   "metadata": {},
   "source": [
    "Batch Normalization (BatchNorm) is a powerful technique for improving the training of neural networks. It offers several advantages but also comes with certain potential limitations. Let's discuss both:\n",
    "\n",
    "**Advantages of Batch Normalization:**\n",
    "\n",
    "1. **Stabilizes Training:** BatchNorm reduces internal covariate shift by normalizing activations within each mini-batch. This leads to more stable training dynamics and faster convergence. It enables the use of higher learning rates, which can speed up training.\n",
    "\n",
    "2. **Mitigates Vanishing Gradient:** BatchNorm helps mitigate the vanishing gradient problem, making it easier to train deep networks. By normalizing activations, it ensures that gradients are neither too small nor too large, allowing for more consistent and efficient backpropagation.\n",
    "\n",
    "3. **Regularization Effect:** BatchNorm acts as a form of regularization by adding noise to activations during training. This noise helps prevent overfitting, leading to models that generalize better to unseen data.\n",
    "\n",
    "4. **Reduces Sensitivity to Weight Initialization:** Neural networks with BatchNorm layers are less sensitive to the choice of weight initialization. This makes it easier to train deep networks without needing careful weight initialization strategies.\n",
    "\n",
    "5. **Improved Generalization:** Models trained with BatchNorm often generalize better because they are trained with more stable and well-conditioned activations.\n",
    "\n",
    "6. **Compatibility:** BatchNorm can be applied to various neural network architectures, including feedforward networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs). Its versatility makes it widely applicable.\n",
    "\n",
    "**Potential Limitations of Batch Normalization:**\n",
    "\n",
    "1. **Increased Memory Usage:** BatchNorm maintains running statistics (mean and variance) during training and inference, which increases memory usage. For very deep networks, this memory overhead can be significant.\n",
    "\n",
    "2. **Batch Size Dependency:** The effectiveness of BatchNorm depends on the choice of batch size. Smaller batch sizes may lead to less stable statistics, reducing its benefits. Therefore, selecting an appropriate batch size is crucial.\n",
    "\n",
    "3. **Slower Inference:** During inference, BatchNorm requires computing activations' statistics, which can slow down inference time compared to models without BatchNorm layers.\n",
    "\n",
    "4. **Normalization Effects:** BatchNorm introduces normalization effects that can lead to over-smoothing of the loss landscape. In some cases, it might hinder convergence to sharp minima, which can affect the model's generalization performance.\n",
    "\n",
    "5. **Doesn't Address All Issues:** While BatchNorm addresses certain training challenges, it doesn't solve all problems. For instance, it may not completely eliminate the need for proper weight initialization or prevent the vanishing gradient problem in extremely deep networks.\n",
    "\n",
    "6. **Not Always Suitable for Recurrent Networks:** BatchNorm can be tricky to apply to recurrent neural networks (RNNs) due to the temporal nature of sequences. Variants like Layer Normalization or Group Normalization may be more suitable for RNNs.\n",
    "\n",
    "In practice, BatchNorm is a valuable tool for improving neural network training in many scenarios. However, its usage should be considered carefully, and hyperparameters like batch size and the presence of BatchNorm layers in the architecture should be tuned to optimize model performance. Additionally, newer normalization techniques like Layer Normalization and Group Normalization have been introduced to address some of BatchNorm's limitations in specific contexts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
